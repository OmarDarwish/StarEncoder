pn 
ls
eq
delim 
en
ev
ps
vs
ev

sp 
ps
ce
arithmetic coding for data compression
ps
sp
ce
ian h witten radford m neal and john g cleary
sp
ce
department of computer science
the university of calgary
 university drive nw
calgary canada tn n
sp
ce
august  revised january 
sp 
ini
lli
the state of the art in data compression is arithmetic coding not
the betterknown huffman method
arithmetic coding gives greater compression is faster for adaptive models
and clearly separates the model from the channel encoding
this paper presents a practical implementation of the technique
sp 
in i
ti i
ficr categories and subject descriptorsfr
br
e data coding and information theory em data compaction and compression
br
h models and principles systems and information theory em information theory
sp
ti i
figeneral termsfr  algorithms performance
sp
ti i
fiadditional key words and phrasesfr  arithmetic coding huffman coding adaptive modeling
lli
in 
bp
sh introduction
pp
arithmetic coding is superior in most respects to the betterknown huffman
 method

huffman  method construction minimumredundancy codes

it represents information at least as compactly sometimes considerably more
so
its performance is optimal without the need for blocking of input data
it encourages a clear separation between the model for representing data and
the encoding of information with respect to that model
it accommodates adaptive models easily
it is computationally efficient
yet many authors and practitioners seem unaware of the technique
indeed there is a widespread belief that huffman coding cannot be improved
upon
pp
this paper aims to rectify the situation by presenting an accessible
implementation of arithmetic coding and detailing its performance
characteristics
the next section briefly reviews basic concepts of data compression and
introduces the modelbased approach which underlies most modern techniques
we then outline the idea of arithmetic coding using a simple example
following that programs are presented for both encoding and decoding in which
the model occupies a separate module so that different ones can easily be
used
next we discuss the construction of fixed and adaptive models
the subsequent section details the compression efficiency and execution time
of the programs including the effect of different arithmetic word lengths on
compression efficiency
finally we outline a few applications where arithmetic coding is appropriate
sh data compression
pp
to many data compression conjures up an assortment of fiad hocfr
techniques such as converting spaces in text to tabs creating special codes
for common words or runlength coding of picture data eg see held 

held  data compression techniques applications

this contrasts with the more modern modelbased paradigm for
coding where from an fiinput stringfr of symbols and a fimodelfr an
fiencoded stringfr is produced which is usually a compressed version of
the input
the decoder which must have access to the same model
regenerates the exact input string from the encoded string
input symbols are drawn from some welldefined set such as the ascii or
binary alphabets
the encoded string is a plain sequence of bits
the model is a way of calculating in any given context the distribution of
probabilities for the next input symbol
it must be possible for the decoder to produce exactly the same probability
distribution in the same context
compression is achieved by transmitting the more probable symbols in fewer
bits than the less probable ones
pp
for example the model may assign a predetermined probability to each symbol
in the ascii alphabet
no context is involved
these probabilities may be determined by counting frequencies in
representative samples of text to be transmitted
such a fifixedfr model is communicated in advance to both encoder and
decoder after which it is used for many messages
pp
alternatively the probabilities the model assigns may change as each symbol
is transmitted based on the symbol frequencies seen fiso farfr in this
message
this is an fiadaptivefr model
there is no need for a representative sample of text because each message
is treated as an independent unit starting from scratch
the encoders model changes with each symbol transmitted and the decoders
changes with each symbol received in sympathy
pp
more complex models can provide more accurate probabilistic predictions and
hence achieve greater compression
for example several characters of previous context could condition the
nextsymbol probability
such methods have enabled mixedcase english text to be encoded in around
 bitchar with two quite different kinds of model
cleary  witten b cormack  horspool 

cleary witten  data compression
d b


cormack horspool  dynamic markov
o april

techniques which do not separate modeling from coding so distinctly like
that of ziv  lempel  do not seem to show such great potential for
compression although they may be appropriate when the aim is raw speed rather
than compression performance welch 

ziv lempel  compression of individual sequences


welch  data compression

pp
the effectiveness of any model can be measured by the fientropyfr of the
message with respect to it usually expressed in bitssymbol
shannons fundamental theorem of coding states that given messages randomly
generated from a model it is impossible to encode them into less bits
on average than the entropy of that model shannon  weaver 

shannon weaver 

pp
a message can be coded with respect to a model using either huffman or
arithmetic coding
the former method is frequently advocated as the best possible technique for
reducing the encoded data rate
but it is not
given that each symbol in the alphabet must translate into an integral number
of bits in the encoding huffman coding indeed achieves minimum
redundancy
in other words it performs optimally if all symbol probabilities are
integral powers of 
but this is not normally the case in practice
indeed huffman coding can take up to one extra bit per symbol
the worst case is realized by a source in which one symbol has probability
approaching unity
symbols emanating from such a source convey negligible information on average
but require at least one bit to transmit gallagher 

gallagher  variations on a theme by huffman

arithmetic coding dispenses with the restriction that each symbol translates
into an integral number of bits thereby coding more efficiently
it actually achieves the theoretical entropy bound to compression efficiency
for any source including the one just mentioned
pp
in general sophisticated models expose the deficiencies of huffman coding
more starkly than simple ones
this is because they more often predict symbols with probabilities close to
one the worst case for huffman coding
for example the techniques mentioned above which code english text in
 bitchar both use arithmetic coding as the final step and performance
would be impacted severely if huffman coding were substituted
nevertheless since our topic is coding and not modeling the illustrations in
this paper all employ simple models
even then as we shall see huffman coding is inferior to arithmetic coding
pp
the basic concept of arithmetic coding can be traced back to elias in the
early s see abramson  pp 

abramson 

practical techniques were first introduced by rissanen  and
pasco  and developed further in rissanen 

rissanen  generalized kraft inequality


pasco 


rissanen  number representations


langdon  tutorial arithmetic coding

details of the implementation presented here have not appeared in the
literature before rubin  is closest to our approach

rubin  arithmetic stream coding

the reader interested in the broader class of arithmetic codes is referred
to rissanen  langdon 

rissanen langdon  arithmetic coding

a tutorial is available in langdon 

langdon  tutorial arithmetic coding

despite these publications the method is not widely known
a number of recent books and papers on data compression mention it only in
passing or not at all
sh the idea of arithmetic coding
pp
in arithmetic coding a message is represented by an
interval of real numbers between  and 
as the message becomes longer the interval needed to represent it becomes
smaller and the number of bits needed to specify that interval grows
successive symbols of the message reduce the size of the
interval in accordance with the symbol probabilities generated by the
model
the more likely symbols reduce the range by less than the unlikely symbols
and hence add fewer bits to the message
pp
before anything is transmitted the range for the message is the entire
interval  dg
fn
dg   denotes the halfopen interval fixfr
ef
as each symbol is processed the range is narrowed to that portion of it
allocated to the symbol
for example suppose the alphabet is fia e i o u fr and a
fixed model is used with probabilities shown in table 
imagine transmitting the message fieaiifr
initially both encoder and decoder know that the range is  
after seeing the first symbol fiefr the encoder narrows it to
  the range the model allocates to this symbol
the second symbol fiafr will narrow this new range to the first  of it
since fiafr has been allocated  
this produces   since the previous range was  units long and
 of that is 
the next symbol fiifr is allocated   which when applied to
  gives the smaller range  
proceeding in this way the encoded message builds up as follows
lb
nf
ta wafter seeing   u i w u
initially
after seeingfiefr
fiafr
fiifr
fiifr
fifr
fi
le
figure  shows another representation of the encoding process
the vertical bars with ticks represent the symbol probabilities stipulated
by the model
after the first symbol fiefr has been processed the model is scaled
into the range   as shown in part a
the second symbol fiafr scales it again into the range  
but the picture cannot be continued in this way without a magnifying glass
consequently figure b shows the ranges expanded to full height at every
stage and marked with a scale which gives the endpoints as numbers
pp
suppose all the decoder knows about the message is the final range
 
it can immediately deduce that the first character was fiefr since the
range lies entirely within the space the model of table  allocates for
fiefr
now it can simulate the operation of the fienfrcoder
lb
nf
ta wafter seeing   u i w u
initially
after seeingfiefr
fi
le
this makes it clear that the second character of the message is fiafr
since this will produce the range
lb
nf
ta wafter seeing   u i w u
after seeingfiafr
fi
le
which entirely encloses the given range  
proceeding like this the decoder can identify the whole message
pp
it is not really necessary for the decoder to know both ends of the range
produced by the encoder
instead a single number within the range em for example  em will
suffice
other numbers like   or even  would do just as
well  c
however the decoder will face the problem of detecting the end of the
message to determine when to stop decoding
after all the single number  could represent any of fiafr fiaafr
fiaaafr fiaaaafr  
to resolve the ambiguity we ensure that each message ends with a special
eof symbol known to both encoder and decoder
for the alphabet of table  fifr will be used to terminate messages
and only to terminate messages
when the decoder sees this symbol it stops decoding
pp
relative to the fixed model of table  the entropy of the symbol message
fieaiifr is
lb
  log      log      log      log      log      log    approx  
le
using base  since the above encoding was performed in decimal
this explains why it takes  decimal digits to encode the message
in fact the size of the final range is     
and the entropy is the negative logarithm of this figure
of course we normally work in binary transmitting binary digits and
measuring entropy in bits
pp
five decimal digits seems a lot to encode a message comprising four vowels
it is perhaps unfortunate that our example ended up by expanding rather than
compressing
needless to say however different models will give different entropies
the best singlecharacter model of the message fieaiifr is the set of
symbol frequencies
fiefr   fiafr   fiifr   fifr 
which gives an entropy for fieaiifr of  decimal digits
using this model the encoding would be only  digits long
moreover as noted earlier more sophisticated models give much better
performance in general
sh a program for arithmetic coding
pp
figure  shows a pseudocode fragment which summarizes the encoding and
decoding procedures developed in the last section
symbols are numbered    
the frequency range for the ith symbol is from cumfreqi to
cumfreqi
cumfreqi increases as i decreases and cumfreq  
the reason for this backwards convention is that later cumfreq
will contain a normalizing factor and it will be convenient to have it
begin the array  c
the current interval is low high and for both encoding and
decoding this should be initialized to  
pp
unfortunately figure  is overly simplistic
in practice there are several factors which complicate both encoding and
decoding
lb
np
incremental transmission and reception
br
the encode algorithm as described does not transmit anything until the entire
message has been encoded neither does the decode algorithm begin decoding
until it has received the complete transmission
in most applications an incremental mode of operation is necessary
sp
np
the desire to use integer arithmetic
br
the precision required to represent the low high interval grows with
the length of the message
incremental operation will help overcome this but the potential for overflow
and underflow must still be examined carefully
sp
np
representing the model so that it can be consulted efficiently
br
the representation used for the model should minimize the time required for
the decode algorithm to identify the next symbol
moreover an adaptive model should be organized to minimize the
timeconsuming task of maintaining cumulative frequencies
le
pp
figure  shows working code in c for arithmetic encoding and decoding
it is considerably more detailed than the barebones sketch of figure 
implementations of two different models are given in figure 
the figure  code can use either one
pp
the remainder of this section examines the code of figure  more closely
including a proof that decoding is still correct in the integer
implementation and a review of constraints on word lengths in the program
rh representing the model
implementations of models are discussed in the next section here we
are concerned only with the interface to the model lines 
in c a byte is represented as an integer between  and  call this a
char
internally we represent a byte as an integer between  and  inclusive
call this an index eof being treated as a th symbol
it is advantageous to sort the model into frequency order to minimize the
number of executions of the decoding loop line 
to permit such reordering the charindex translation is implemented as
a pair of tables indextochar   and chartoindex  
in one of our models these tables simply form the index by adding  to the
char but another implements a more complex translation which assigns small
indexes to frequentlyused symbols
pp
the probabilities in the model are represented as integer frequency counts
and cumulative counts are stored in the array cumfreq  
as previously this array is backwards and the total frequency count em
which is used to normalize all frequencies em appears in cumfreq
cumulative counts must not exceed a predetermined maximum maxfrequency
and the model implementation must prevent overflow by scaling appropriately
it must also ensure that neighboring values in the cumfreq   array
differ by at least  otherwise the affected symbol could not be transmitted
rh incremental transmission and reception
unlike figure  the program of figure  represents low and high as
integers
a special data type codevalue is defined for these quantities
together with some useful constants  c
topvalue representing the largest possible codevalue and
firstqtr half and thirdqtr representing parts of the range
lines 
whereas in figure  the current interval is represented by
low high in figure  it is low high that is the range now
includes the value of high
actually it is more accurate though more confusing to say that in the
program of figure  the interval represented is
low high   
this is because when the bounds are scaled up to increase the precision s
are shifted into the loworder bits of low but s are shifted into high
while it is possible to write the program to use a different convention
this one has some advantages in simplifying the code
pp
as the code range narrows the top bits of low and high will become the
same
any bits which are the same can be transmitted immediately since they cannot
be affected by future narrowing
for encoding since we know that low    high this requires code like
lb nnnn
nf
ta wnnnnu wif high  u whalf  u woutputbit low  lowhalf high  highhalf u
ne 
for  
if high half outputbit low  low high  high
if low half outputbit low  lowhalf high  highhalf

fi
le nnnn
which ensures that upon completion low    half    high
this can be found in lines  of encodesymbol  
although there are some extra complications caused by underflow possibilities
see next subsection
care is taken care to shift s in at the bottom when high is scaled as
noted above
pp
incremental reception is done using a number called value as in figure 
in which processed bits flow out the top highsignificance end and
newlyreceived ones flow in the bottom
startdecoding   lines  fills value with received bits initially
once decodesymbol   has identified the next input symbol it shifts out
nowuseless highorder bits which are the same in low and high shifting
value by the same amount and replacing lost bits by fresh input bits at the
bottom end
lb nnnn
nf
ta wnnnnu wif high  u whalf  u wvalue  valuehalfinputbit low  lowhalf high  highhalf u
ne 
for  
if high half value  valueinputbit low  low high  high
if low half value  valuehalfinputbit low  lowhalf high  highhalf

fi
le nnnn
see lines  again complicated by precautions against underflow
discussed below
rh proof of decoding correctness
at this point it is worth checking that the identification of the next
symbol by decodesymbol   works properly
recall from figure  that decodesymbol   must use value to find that
symbol which when encoded reduces the range to one that still includes
value
lines  in decodesymbol   identify the symbol for which
lb
cumfreqsymbol   
left f valuelowcumfreq   over highlow right f
   cumfreqsymbol
le
where left f  right f denotes the integer part of function that
comes from integer division with truncation
it is shown in the appendix that this implies
lb nnnn
low  left f highlowcumfreqsymbol over cumfreq right f
   v    
low  left f highlowcumfreqsymbol over cumfreq right f    
le nnnn
so value lies within the new interval that decodesymbol  
calculates in lines 
this is sufficient to guarantee that the decoding operation identifies each
symbol correctly
rh underflow
as figure  shows arithmetic coding works by scaling the cumulative
probabilities given by the model into the interval low high for
each character transmitted
suppose low and high are very close together so close that this scaling
operation maps some different symbols of the model on to the same integer in
the low high interval
this would be disastrous because if such a symbol actually occurred it would
not be possible to continue encoding
consequently the encoder must guarantee that the interval low high is
always large enough to prevent this
the simplest way is to ensure that this interval is at least as large as
maxfrequency the maximum allowed cumulative frequency count line 
pp
how could this condition be violated
the bitshifting operation explained above ensures that low and high can
only become close together when they straddle half
suppose in fact they become as close as
lb
firstqtr    low  half    high  thirdqtr
le
then the next two bits sent will have opposite polarity either  or 
for example if the next bit turns out to be  ie high descends below
half and  half is expanded to the full interval the bit after
that will be  since the range has to be above the midpoint of the expanded
interval
conversely if the next bit happens to be  the one after that will be 
therefore the interval can safely be expanded right now if only we remember
that whatever bit actually comes next its opposite must be transmitted
afterwards as well
thus lines  expand firstqtr thirdqtr into the whole interval
remembering in bitstofollow that the bit that is output next must be
followed by an opposite bit
this explains why all output is done via bitplusfollow  
lines  instead of directly with outputbit  
pp
but what if after this operation it is fistillfr true that
lb
firstqtr    low  half    high  thirdqtr
le
figure  illustrates this situation where the current low high
range shown as a thick line has been expanded a total of three times
suppose the next bit will turn out to be  as indicated by the arrow in
figure a being below the halfway point
then the next fithreefr bits will be s since not only is the arrow in the
top half of the bottom half of the original range it is in the top quarter
and moreover the top eighth of that half em that is why the expansion
can occur three times
similarly as figure b shows if the next bit turns out to be a  it will
be followed by three s
consequently we need only count the number of expansions and follow the next
bit by that number of opposites lines  and 
pp
using this technique the encoder can guarantee that after the shifting
operations either
lb
ta nluniur
low  firstqtr  half    higha
le
or
lb
ta nluniur
low  half  thirdqtr    highb
le
therefore as long as the integer range spanned by the cumulative frequencies
fits into a quarter of that provided by codevalues the underflow problem
cannot occur
this corresponds to the condition
lb
maxfrequency    topvalue over     
le
which is satisfied by figure  since maxfrequency   sup    and
topvalue   sup    lines  
more than  bits cannot be used to represent cumulative frequency
counts without increasing the number of bits allocated to codevalues
pp
we have discussed underflow in the encoder only
since the decoders job once each symbol has been decoded is to track the
operation of the encoder underflow will be avoided if it performs the same
expansion operation under the same conditions
rh overflow
now consider the possibility of overflow in the integer multiplications
corresponding to those of figure  which occur in lines  and 
of figure 
overflow cannot occur provided the product
lb
range  maxfrequency
le
fits within the integer word length available since cumulative frequencies
cannot exceed maxfrequency
range might be as large as topvalue  so the largest possible product
in figure  is  sup    sup     which is less than  sup 
long declarations are used for codevalue line  and range
lines   to ensure that arithmetic is done to bit precision
rh constraints on the implementation
the constraints on word length imposed by underflow and overflow can
be simplified by assuming frequency counts are represented in f bits and
codevalues in c bits
the implementation will work correctly provided
lb
f    c   
br
f  c    p the precision to which arithmetic is performed
le
in most c implementations p if long integers are used and p
if they are unsigned  long
in figure  f and c
with appropriately modified declarations unsigned  long arithmetic with
f c could be used
in assembly language c is a natural choice because it expedites some
comparisons and bit manipulations eg those of lines  and 
pp
if p is restricted to  bits the best values possible are c and
f making it impossible to encode a full alphabet of  symbols as each
symbol must have a count of at least 
a smaller alphabet eg the letters or bit nibbles could still be
handled
rh termination
to finish the transmission it is necessary to send a unique terminating
symbol eofsymbol line  and then follow it by enough bits to ensure
that the encoded string falls within the final range
since doneencoding   lines  can be sure that
low and high are constrained by either a or b above it need only
transmit  in the first case or  in the second to remove the remaining
ambiguity
it is convenient to do this using the bitplusfollow   procedure
discussed earlier
the inputbit   procedure will actually read a few more bits than were
sent by outputbit   as it needs to keep the low end of the buffer full
it does not matter what value these bits have as the eof is uniquely
determined by the last two bits actually transmitted
sh models for arithmetic coding
pp
the program of figure  must be used with a model which provides
a pair of translation tables indextochar   and chartoindex  
and a cumulative frequency array cumfreq  
the requirements on the latter are that
lb
np
cumfreq i     cumfreq i 
np
an attempt is never made to encode a symbol i for which
cumfreqi  cumfreqi
np
cumfreq    maxfrequency
le
provided these conditions are satisfied the values in the array need bear
no relationship to the actual cumulative symbol frequencies in messages
encoding and decoding will still work correctly although encodings will
occupy less space if the frequencies are accurate
recall our successfully encoding fieaiifr according to the model of
table  which does not actually reflect the frequencies in the message  c
rh fixed models
the simplest kind of model is one in which symbol frequencies are fixed
the first model in figure  has symbol frequencies which approximate those
of english taken from a part of the brown corpus kucera  francis 

a kucera h
a francis wn
d 
t computational analysis of presentday american english
i brown university press
c providence ri

however bytes which did not occur in that sample have been given frequency
counts of  in case they do occur in messages to be encoded
so for example this model will still work for binary files in which all
 bytes occur
frequencies have been normalized to total 
the initialization procedure startmodel   simply computes a cumulative
version of these frequencies lines  having first initialized the
translation tables lines 
execution speed would be improved if these tables were used to reorder
symbols and frequencies so that the most frequent came first in the
cumfreq   array
since the model is fixed the procedure updatemodel   which is
called from both encodec and decodec is null
pp
an fiexactfr model is one where the symbol frequencies in the message are
exactly as prescribed by the model
for example the fixed model of figure  is close to an exact model
for the particular excerpt of the brown corpus from which it was taken
to be truly exact however symbols that did not occur in the excerpt would
be assigned counts of  not  sacrificing the capability of
transmitting messages containing those symbols
moreover the frequency counts would not be scaled to a predetermined
cumulative frequency as they have been in figure 
the exact model may be calculated and transmitted before sending the message
it is shown in cleary  witten a that under quite general conditions
this will finotfr give better overall compression than adaptive coding
described next

cleary witten  enumerative adaptive codes
d a

rh adaptive models
an adaptive model represents the changing symbol frequencies seen fiso farfr
in the message
initially all counts might be the same reflecting no initial information
but they are updated as each symbol is seen to approximate the observed
frequencies
provided both encoder and decoder use the same initial values eg equal
counts and the same updating algorithm their models will remain in step
the encoder receives the next symbol encodes it and updates its model
the decoder identifies it according to its current model and then updates its
model
pp
the second half of figure  shows such an adaptive model
this is the type of model recommended for use with figure  for in practice
it will outperform a fixed model in terms of compression efficiency
initialization is the same as for the fixed model except that all frequencies
are set to 
the procedure updatemodelsymbol is called by both encodesymbol  
and decodesymbol   figure  lines  and  after each symbol is
processed
pp
updating the model is quite expensive because of the need to maintain
cumulative totals
in the code of figure  frequency counts which must be maintained anyway
are used to optimize access by keeping the array in frequency order em an
effective kind of selforganizing linear search hester  hirschberg 

hester hirschberg 

updatemodel   first checks to see if the new model will exceed
the cumulativefrequency limit and if so scales all frequencies down by a
factor of  taking care to ensure that no count scales to zero and
recomputes cumulative values figure  lines 
then if necessary updatemodel   reorders the symbols to place the
current one in its correct rank in the frequency ordering altering the
translation tables to reflect the change
finally it increments the appropriate frequency count and adjusts cumulative
frequencies accordingly
sh performance
pp
now consider the performance of the algorithm of figure  both
in compression efficiency and execution time
rh compression efficiency
in principle when a message is coded using arithmetic coding the number of
bits in the encoded string is the same as the entropy of that message with
respect to the model used for coding
three factors cause performance to be worse than this in practice
lb
np
message termination overhead
np
the used of fixedlength rather than infiniteprecision arithmetic
np
scaling of counts so that their total is at most maxfrequency
le
none of these effects is significant as we now show
in order to isolate the effect of arithmetic coding the model will be
considered to be exact as defined above
pp
arithmetic coding must send extra bits at the end of each message causing a
message termination overhead
two bits are needed sent by doneencoding   figure  lines 
in order to disambiguate the final symbol
in cases where a bitstream must be blocked into bit characters before
encoding it will be necessary to round out to the end of a block
combining these an extra  bits may be required
pp
the overhead of using fixedlength arithmetic
occurs because remainders are truncated on division
it can be assessed by comparing the algorithms performance with
the figure obtained from a theoretical entropy calculation
which derives its frequencies from counts scaled exactly as for coding
it is completely negligible em on the order of  sup  bitssymbol
pp
the penalty paid by scaling counts is somewhat larger but still very
small
for short messages less than  sup  bytes no scaling need be done
even with messages of  sup  to  sup  bytes the overhead was found
experimentally to be less than  of the encoded string
pp
the adaptive model of figure  scales down all counts whenever the total
threatens to exceed maxfrequency
this has the effect of weighting recent events more heavily compared with
those earlier in the message
the statistics thus tend to track changes in the input sequence which can be
very beneficial
for example we have encountered cases where limiting counts to  or  bits
gives better results than working to higher precision  c
of course this depends on the source being modeled
bentley fiet alfr  consider other more explicit ways of
incorporating a recency effect

bentley sleator tarjan wei  locally adaptive
j communications of the acm

rh execution time
the program in figure  has been written for clarity not execution speed
in fact with the adaptive model of figure  it takes about  mus per
input byte on a vax to encode a text file and about the same for
decoding
however easily avoidable overheads such as procedure calls account for much
of this and some simple optimizations increase speed by a factor of 
the following alterations were made to the c version shown
lb
np
the procedures inputbit   outputbit   and
bitplusfollow   were converted to macros to eliminate
procedurecall overhead
np
frequentlyused quantities were put in register variables
np
multiplies by two were replaced by additions c 
np
array indexing was replaced by pointer manipulation in the loops
at line  of figure  and lines  of the adaptive model in figure 
le
pp
this mildlyoptimized c implementation has an execution time of
 mus mus per input byte
for encodingdecoding  bytes of english text on a vax as shown
in table 
also given are corresponding figures for the same program on an
apple macintosh and a sun
as can be seen coding a c source program of the same length took slightly
longer in all cases and a binary object program longer still
the reason for this will be discussed shortly
two artificial test files were included to allow readers to replicate the
results
alphabet consists of enough copies of the letter alphabet to fill
out  characters ending with a partiallycompleted alphabet
skewstatistics contains  copies of the string
fiaaaabaaaacfr it demonstrates that files may be encoded into less than
 bit per character output size of  bytes   bits
all results quoted used the adaptive model of figure 
pp
a further factor of  can be gained by reprogramming in assembly language
a carefully optimized version of figures  and  adaptive model was
written in both vax and m assembly language
full use was made of registers
advantage was taken of the bit codevalue to expedite some crucial
comparisons and make subtractions of half trivial
the performance of these implementations on the test files is also shown in
table  in order to give the reader some idea of typical execution speeds
pp
the vax assembly language timings are broken down in table 
these figures were obtained with the usnixs profile facility and are
accurate only to within perhaps dg
fn
dg this mechanism constructs a histogram of program counter values at
realtime clock interrupts and suffers from statistical variation as well as
some systematic errors
ef
bounds calculation refers to the initial part of encodesymbol  
and decodesymbol   figure  lines  and 
which contain multiply and divide operations
bit shifting is the major loop in both the encode and decode routines
lines  and 
the cum calculation in decodesymbol   which requires a
multiplydivide and the following loop to identify the next symbol
lines  is symbol decode
finally model update refers to the adaptive
updatemodel   procedure of figure  lines 
pp
as expected the bounds calculation and model update take the same time for
both encoding and decoding within experimental error
bit shifting was quicker for the text file than for the c program and object
file because compression performance was better
the extra time for decoding over encoding is due entirely to the symbol
decode step
this takes longer in the c program and object file tests because the loop of
line  was executed more often on average  times  times and
 times respectively
this also affects the model update time because it is the number of cumulative
counts which must be incremented in figure  lines 
in the worst case when the symbol frequencies are uniformly distributed
these loops are executed an average of  times
worstcase performance would be improved by using a more complex tree
representation for frequencies but this would likely be slower for text
files
sh some applications
pp
applications of arithmetic coding are legion
by liberating ficodingfr with respect to a model from the fimodelingfr
required for prediction it encourages a whole new view of data compression
rissanen  langdon 

rissanen langdon  universal modeling and coding

this separation of function costs nothing in compression performance since
arithmetic coding is practically optimal with respect to the entropy of
the model
here we intend to do no more than suggest the scope of this view
by briefly considering
lb
np
adaptive text compression
np
nonadaptive coding
np
compressing blackwhite images
np
coding arbitrarilydistributed integers
le
of course as noted earlier greater coding efficiencies could easily be
achieved with more sophisticated models
modeling however is an extensive topic in its own right and is beyond the
scope of this paper
pp
ul
adaptive text compression
using singlecharacter adaptive frequencies shows off arithmetic coding to
good effect
the results obtained using the program of figures  and  vary from
 bitchar for short english text files  sup  to  sup 
bytes to  bitchar for long ones  sup  to  sup  bytes
although adaptive huffman techniques do exist eg gallagher 
cormack  horspool  they lack the conceptual simplicity of
arithmetic coding

gallagher  variations on a theme by huffman


cormack horspool  adaptive huffman codes

while competitive in compression efficiency for many files they are slower
for example table  compares the performance of the mildlyoptimized c
implementation of arithmetic coding with that of the usnixs
ficompactfr program which implements adaptive huffman coding using
a similar modeldg
fn
dg ficompactfrs model is essentially the same for long files like those
of table  but is better for short files than the model used as an example
in this paper
ef
casual examination of ficompactfr indicates that the care taken in
optimization is roughly comparable for both systems yet arithmetic coding
halves execution time
compression performance is somewhat better with arithmetic coding on all the
example files
the difference would be accentuated with more sophisticated models that
predict symbols with probabilities approaching one under certain circumstances
eg letter u following q
pp
ul
nonadaptive coding
can be performed arithmetically using fixed prespecified models like that in
the first part of figure 
compression performance will be better than huffman coding
in order to minimize execution time the total frequency count
cumfreq should be chosen as a power of two so the divisions
in the bounds calculations figure  lines  and  can be done
as shifts
encodedecode times of around  mus mus should then be possible
for an assembly language implementation on a vax
a carefullywritten implementation of huffman coding using table lookup for
encoding and decoding would be a bit faster in this application
pp
ul
compressing blackwhite images
using arithmetic coding has been investigated by langdon  rissanen 
who achieved excellent results using a model which conditioned the probability
of a pixels being black on a template of pixels surrounding it

langdon rissanen  compression of blackwhite images

the template contained a total of ten pixels selected from those above and
to the left of the current one so that they precede it in the raster scan
this creates  different possible contexts and for each the probability of
the pixel being black was estimated adaptively as the picture was transmitted
each pixels polarity was then coded arithmetically according to this
probability
a  improvement in compression was attained over earlier methods
to increase coding speed langdon  rissanen used an approximate method
of arithmetic coding which avoided multiplication by representing
probabilities as integer powers of 
huffman coding cannot be directly used in this application as it never
compresses with a twosymbol alphabet
runlength coding a popular method for use with twovalued alphabets
provides another opportunity for arithmetic coding
the model reduces the data to a sequence of lengths of runs of the same symbol
eg for picture coding runlengths of black followed by white followed by
black followed by white 
the sequence of lengths must be transmitted
the ccitt facsimile coding standard hunter  robinson  for example
bases a huffman code on the frequencies with which black and white runs of
different lengths occur in sample documents

hunter robinson  facsimile

a fixed arithmetic code using these same frequencies would give better
performance adapting the frequencies to each particular document would be
better still
pp
ul
coding arbitrarilydistributed integers
is often called for when using more sophisticated models of text image
or other data
consider for instance bentley fiet alfrs  locallyadaptive data
compression scheme in which the encoder and decoder cache the last n
different words seen

bentley sleator tarjan wei  locally adaptive
j communications of the acm

a word present in the cache is transmitted by sending the integer cache index
words not in the cache are transmitted by sending a newword marker followed
by the characters of the word
this is an excellent model for text in which words are used frequently over
short intervals and then fall into long periods of disuse
their paper discusses several variablelength codings for the integers used
as cache indexes
arithmetic coding allows fianyfr probability distribution to be used as the
basis for a variablelength encoding including em amongst countless others
em the ones implied by the particular codes discussed there
it also permits use of an adaptive model for cache
indexes which is desirable if the distribution of cache hits is
difficult to predict in advance
furthermore with arithmetic coding the code space allotted to the cache
indexes can be scaled down to accommodate any desired probability for the
newword marker
sh acknowledgement
pp
financial support for this work has been provided by the
natural sciences and engineering research council of canada
sh references
sp
inn

list

in 
bp
sh appendix proof of decoding inequality
sp
using letter abbreviations for cumfreq symbol low high and
value suppose
lb
cs    left f vl times c   over hl right f   
cs
le
in other words
lb
ta nluniur
cs    vl times c   over r  epsilon   
cs  
le
ta n
wherer  hl      epsilon    r over r 
sp
the last inequality of  derives from the fact that cs must be an
integer  c
then we wish to show that  l    v    h  where l and h
are the updated values for low and high as defined below
sp
ta wa    u
al    l  left f r times cs over c right f  mark
  l  r over c  left   vl times c   over r
                             epsilon  right     from 
sp 
lineup   v         over c 
sp 
so   l    v   since both v and l are integers
and c  
sp
bh    l  left f r times cs over c right f  mark
  l   r over c  left   vl times c   over r
                                 epsilon  right     
    from 
sp 
lineup   v    r over c  left      over r  
                                                   r over r right 
   v
bp
sh captions for tables
sp
nf
ta wfigure   u
table example fixed model for alphabet fia e i o u fr
table results for encoding and decoding byte files
table breakdown of timings for vax assembly language version
table comparison of arithmetic and adaptive huffman coding
fi
sh captions for figures
sp
nf
ta wfigure   u
figure a representation of the arithmetic coding process
b like a but with the interval scaled up at each stage
figure pseudocode for the encoding and decoding procedures
figure c implementation of arithmetic encoding and decoding
figure fixed and adaptive models for use with figure 
figure scaling the interval to prevent underflow
fi
bp 
ev
nr x wsymbol
nr x wsymboliwprobability
nr x wprobabilityi
nr x w 
nr x nxnxnxnxw
nr x nlnx
in nxu
ta nxuc nxuc nxu nxu
lnxu
sp
symbolprobabilityrange
lnxu
sp
fiafr
fiefr
fiifr
fiofr
fiufr
fifr
lnxu
sp
in 
fe table   example fixed model for alphabet fia e i o u fr
bp 
ev
nr x iwfivax object programfr      w      wtime mus  wtime mus    wtime mus  wtime mus    wtime mus  wtime mus
nr x nlnx
in nxu
ta i wfivax object programfr      u w      u wtime mus  u wtime mus    u wtime mus  u wtime mus    u wtime mus  u
lnxu
sp
vaxmacintoshsun
output encode decode encode decode encode decode
bytestime mustime mustime mustime mustime mustime mus
lnxu
sp
mildly optimized c implementation
sp
fitext filefr
fic programfr
fivax object programfr
fialphabetfr
fiskewstatisticsfr
sp
carefully optimized assembly language implementation
sp
fitext filefr
fic programfr
fivax object programfr
fialphabetfr
fiskewstatisticsfr

lnxu
sp 
nr x nl
ll nluniu
fi
in wfinotesfr  u
ti wfinotesfr  u
finotesfr  c
times are measured in mus per byte of uncompressed data
sp 
the vax had a floatingpoint accelerator which reduces integer
multiply and divide times
sp 
the macintosh uses an  mhz mc with some memory wait states
sp 
the sun uses a  mhz mc
sp 
all times exclude io and operating system overhead in support of io
vax and sun figures give user time from the usnixs fitimefr
command on the macintosh io was explicitly directed to an array
sp 
the bsd c compiler was used for vax and sun aztec c g for macintosh
sp
ll nxu
nf
in 
fe table   results for encoding and decoding byte files
bp 
ev
nr x wfivax object programfr        wbounds calculation        wtime mus    wtime mus
nr x nlnx
in nxu
ta wfivax object programfr        u wbounds calculation        u wtime mus    u wtime musu
lnxu
sp
 encode decode
time mustime mus
lnxu
sp
fitext filefrbounds calculation
bit shifting
model update
symbol decodeem
other
lwulwu

sp
fic programfrbounds calculation
bit shifting
model update
symbol decodeem
other
lwulwu

sp
fivax object programfrbounds calculation
bit shifting
model update
symbol decodeem
other
lwulwu

lnxu
in 
fe table   breakdown of timings for vax assembly language version
bp 
ev
nr x wfivax object programfr      w      wtime mus  wtime mus    w      wtime mus  wtime mus
nr x nlnx
in nxu
ta wfivax object programfr      u w      u wtime mus  u wtime mus    u w      u wtime mus  u wtime musu
lnxu
sp
arithmetic codingadaptive huffman coding
output encode decodeoutput encode decode
bytestime mustime musbytestime mustime mus
lnxu
sp
fitext filefr
fic programfr
fivax object programfr
fialphabetfr
fiskewstatisticsfr
lnxu
sp 
nr x nl
ll nluniu
fi
in wfinotesfr  u
ti wfinotesfr  u
finotesfr  c
mildly optimized c implementation used for arithmetic coding
sp 
usnixs ficompactfr used for adaptive huffman coding
sp 
times are for a vax and exclude io and operating system overhead in
support of io
sp
ll nxu
nf
in 
fe table   comparison of arithmetic and adaptive huffman coding
