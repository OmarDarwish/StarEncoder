eq
delim 
en
ch   why speech output
ds rt why speech output
ds cx principles of computer speech
pp
speech is our everyday informal communication medium  but although we use
it a lot we probably dont assimilate as much information through our
ears as we do through our eyes by reading or looking at pictures and diagrams
you go to a technical lecture to get the feel of a subject em the overall
arrangement of ideas and the motivation behind them em and fill in the details
if you still want to know them from a book  you probably find out more about
the news from ten minutes with a newspaper than from a tenminute news broadcast
so it should be emphasized from the start that speech output from computers is
not a panacea  it doesnt solve the problems of communicating with computers
it simply enriches the possibilities for communication
pp
what then are the advantages of speech output  one good reason for listening
to a radio news broadcast instead of spending the time with a newspaper
is that you can listen while shaving doing the housework or driving the car
speech leaves hands and eyes free for other tasks
moreover it is omnidirectional and does not require a free line of sight
related to this is the
use of speech as a secondary medium for status reports and warning messages
occasional interruptions by voice do not interfere with other activities
unless they demand unusual concentration and people can assimilate spoken messages
and queue them for later action quite easily and naturally
pp
the second key feature of speech communication stems from the telephone
it is the universality of the telephone receiver itself that is important
here rather than the existence of a worldwide distribution network
for with special equipment a modem and a vdu one does not need speech to take advantage of
the telephone network for information transfer
but speech needs no tools other than the telephone and this gives
it a substantial advantage  you can go into a phone booth anywhere in the world
carrying no special equipment and have access to your computer within seconds
the problem of data input is still there  perhaps your computer
system has a limited word recognizer or you use the touchtone telephone
keypad or a portable calculatorsized tone generator  easy remote access
without special equipment is a great and unique asset to speech communication
pp
the third big advantage of speech output is that it is potentially very cheap
being allelectronic except for the loudspeaker speech systems are well
suited to highvolume lowcost lsi manufacture  other computer output
devices are at present tied either to mechanical moving parts or to the crt
this was realized quickly by the computer hobbies market where speech output
peripherals have been selling like hot cakes since the mid s
pp
a further point in favour of speech is that it is naturalseeming and
somehow cuddly when compared with printers or vdus  it would have been much
more difficult to make this point before the advent of talking toys like
texas instruments speak n spell in  but now it is an accepted fact that friendly
computerbased gadgets can speak em there are talking pocketwatches
that really do tell the time talking microwave ovens talking pinball machines and
of course talking calculators
it is however difficult to assess whether the appeal stems from
mechanical speechs novelty em it
is still a gimmick em and also to what extent it is tied up with
economic factors
after all most of the population dont use highquality vdus and their major
experience of realtime interactive computing is through the very limited displays
and keypads provided on video games and teletext systems
pp
articles on speech communication with computers often list many more advantages of voice output
see hill  turn  lea 

hill  manmachine interaction using speech


lea 


turn  speech as a mancomputer communication channel

for example speech
lb
np
can be used in the dark
np
can be varied from a confidential whisper to a loud shout
np
requires very little energy
np
is not appreciably affected by weightlessness or vibration
le
however these either derive from the three advantages we have discussed above
or relate
mainly to exotic applications in space modules and divers helmets
pp
useful as it is at present speech output would be even more attractive if it could
be coupled with speech input  in many ways speech input is its big brother
many of the benefits of speech output are even more striking for speech input
although people can assimilate information faster through the eyes than the
ears the majority of us can generate information faster with the mouth than
with the hands  rapid typing is a relatively uncommon skill and even high
typing rates are much slower than speaking rates although whether we can
originate ideas quickly enough to keep up with fast speech is another matter  to
take full advantage of the telephone for interaction with machines machine
recognition of speech is obviously necessary  a microwave oven calculator
pinball machine or alarm clock that responds to spoken commands is certainly
more attractive than one that just generates spoken status messages  a book
that told you how to recognize speech by machine would undoubtedly be more
useful than one like this that just discusses how to synthesize it  but the
technology of speech recognition is nowhere near as advanced as that of
synthesis em its a much more difficult problem  however because speech input
is obviously complementary to speech output and even very limited input
capabilities will greatly enhance many speech output systems it is worth
summarizing the present state of the art of speech recognition
pp
commercial speech recognizers do exist  almost invariably they accept
words spoken in isolation with gaps of silence between them rather than
connected utterances
it is not difficult to discriminate with high accuracy up to a hundred
different words spoken by the same speaker especially if the vocabulary
is carefully selected to avoid words which sound similar  if several
different speakers are to be comprehended performance can be greatly improved
if the machine is given an opportunity to calibrate their voices in a training
session and is informed at recognition time which one is to speak
with a large population of unknown speakers accurate recognition is difficult
for vocabularies of more than a few carefullychosen words
pp
a halfway house between isolated word discrimination and recognition of connected
speech is the problem of spotting known words in continuous speech  this
allows much more natural input if the dialogue is structured as keywords
which may be
interspersed by unimportant noise words  to speak in truly isolated
words requires a great deal of selfdiscipline and concentration em it is
surprising how much of ordinary speech is accounted for by vague sounds
like ums and aahs and false starts  word spotting disregards these and so
permits a more relaxed style of speech  some progress has been made on it in
research laboratories but the vocabularies that can be accomodated are still
very small
pp
the difficulty of recognizing connected speech depends crucially on what is
known in advance about the dialogue  its pragmatic semantic and syntactic
constraints  highly structured dialogues constrain very heavily the choice of
the next word  recognizers which can deal with vocabularies of over  words
have been built in research laboratories but the structure of the input has
been such that the average branching factor em the size of the set out of
which the next word must be selected em is only around  lea 

lea 

whether such
highly constrained languages would be acceptable in many practical applications
is a moot point  one commercial recognizer developed in  can cope with
up to five words spoken continuously from a basic word vocabulary
pp
there has been much debate about whether it will ever be possible for a speech
recognizer to step outside rigid constraints imposed on the utterances it can
understand and act say as an automatic dictation machine  certainly the most
advanced recognizers to date depend very strongly on a tight context being
available  informed opinion seems to accept that in ten years time
voice data entry in the office will be an important and economically feasible
prospect but that it would be rash to predict the appearance of unconstrained
automatic dictation by then
pp
lets return now to speech output and take a look at some systems which use it
to illustrate the advantages and disadvantages of speech in practical
applications
sh   talking calculator
pp
figure  shows a calculator that speaks
fc figure 
whenever a key is pressed
the device confirms the action by saying the keys name
the result of any computation is also spoken aloud
for most people the addition of speech output to a calculator is simply a
gimmick
note incidentally that speech
ul
input
is a different matter altogether  the ability to dictate lists of numbers and
commands to a calculator without lifting ones eyes from the page would have
very great advantages over keypad input  usedcar
salesmen find that speech output sometimes helps to clinch a deal  they key in
the basic car price and their bargainbasement deductions and the customer is so
bemused by the resulting price being spoken aloud to him by a machine that he
signs the cheque without thinking  more seriously there may be some small
advantage to be gained when keying a list of figures by touch from having their
values read back for confirmation  for blind people however such devices
are a boon em and there are many other applications like talking elevators
and talking clocks which benefit from even very restricted voice output
much more sophisticated is a typewriter with audio feedback designed by
ibm for the blind  although blind typists can remember where the keys on a
typewriter are without difficulty they rely on sighted proofreaders to help
check
their work  this device could make them more useful as office typists and
secretaries  as well as verbalizing the material including punctuation
that has been typed either by attempting to pronounce the words or by spelling
them out as individual letters it prompts the user through the more complex action sequences
that are possible on the typewriter
pp
the vocabulary of the talking calculator comprises the  words of table 
rf
nr x iwpercentu
nr x nlnx
in nxu
ta i
zeropercent
onelow
twoover
threeroot
fourem m
fivetimes
sixpoint
sevenoverflow
eightminus
nineplus
timesminusclear
equalsswap
ta i i i i i i i i i i i i
in 
fg table   vocabulary of a talking calculator
this represents a total of about  seconds of speech  it is stored
electronically in readonly memory rom and figure  shows the circuitry
of the speech module inside the calculator
fc figure 
there are three large integrated circuits
two of them are roms and the other is a special synthesis chip which decodes the
highly compressed stored data into an audio waveform
although the mechanisms used for storing speech by commercial devices are
not widely advertised by the manufacturers the talking calculator almost
certainly uses linear predictive coding em a technique that we will examine
in chapter 
the speech quality is very poor because of the highly compressed storage and
words are spoken in a grating monotone
however because of the very small vocabulary the quality is certainly good
enough for reliable identification
sh   computergenerated wiring instructions
pp
i mentioned earlier that one big advantage of speech over visual output is that
it leaves the eyes free for other tasks
when wiring telephone equipment during manufacture the operator needs to use
his hands as well as eyes to keep his place in the task
for some time taperecorded instructions have been used for this in certain
manufacturing plants  for example the instruction
lb
ni
red     a terminal strip    a tube socket
le
directs the operator to cut  of red wire attach one end to a specified point
on the terminal strip and attach the other to a pin of the tube socket  the
tape recorder is fitted with a pedal switch to allow a sequence of such instructions
to be executed by the operator at his own pace
pp
the usual way of recording the instruction tape is to have a human reader
dictate them from a printed list
the tape is then checked against the list by another listener to ensure that
the instructions are correct  since wiring lists are usually stored and
maintained in machinereadable form it is natural to consider whether speech
synthesis techniques could be used to generate the acoustic tape directly by
a computer flanagan
ul
et al


flanagan rabiner schafer denman 

pp
table  shows the vocabulary needed for this application
rf
nr x iiwtube socketu
nr x nlnx
in nxu
ta i i
agreenseventeen
blackleftsix
bottomlowersixteen
breakmakestrip
cnineten
capacitornineteenterminal
eightonethirteen
eighteenpthirty
elevenpointthree
fifteenrtop
fiftyredtube socket
fiverepeat coiltwelve
fortyresistortwenty
fourrighttwo
fourteensevenupper
ta i i i i i i i i i i i i
in 
fg table   vocabulary needed for computergenerated wiring instructions
it is rather larger
than that of the talking calculator em about  seconds of speech em but well
within the limits of singlechip storage in rom compressed by the linear
predictive technique  however at the time that the scheme was investigated
 the method of linear predictive coding had not been fully developed
and the technology for lowcost microcircuit implementation was not available
but this is not important for this particular application for there is
no need to perform the synthesis on a miniature lowcost computer system
nor need it
be accomplished in real time  in fact a technique of concatenating
spectrallyencoded words was used described in chapter  and it was
implemented on a minicomputer  operating much slower than realtime the system
calculated the speech waveform and wrote it to disk storage  a subsequent phase
read the precomputed messages and recorded them on a computercontrolled analogue
tape recorder
pp
informal evaluation showed the scheme to be quite successful  indeed the
synthetic speech whose quality was not high was actually preferred to
natural speech in the noisy environment of the production line for each
instruction was spoken in the same format with the same programmed pause
between the items
a list of  instructions of the form shown above was recorded and used
to wire several pieces of apparatus without errors
sh   telephone enquiry service
pp
the computergenerated wiring scheme illustrates how speech can be used to give
instructions without diverting visual attention from the task at hand
the next system we examine shows how speech output can make the telephone
receiver into a remote computer terminal for a variety of purposes
witten and madams 

witten madams  telephone enquiry service

the caller employs the touchtone keypad shown in figure  for input and the
computer generates
a synthetic voice response
fc figure 
table  shows the process of making
contact with the system
rf
fi
nh
na
in i
nr x wcomputer  
nr x wcaller  
innxu
tinxu
callerhnxunxu  dials the service
tinxu
computer  answers telephone
hello telephone enquiry service  please
enter your user number
tinxu
callerhnxunxu  enters user number
tinxu
computer  please enter your password
tinxu
callerhnxunxu  enters password
tinxu
computer  checks validity of password
if invalid the user is asked to reenter
his user number
otherwise
which service do you require
tinxu
callerhnxunxu  enters service number
in 
nf
fg table   making contact with the telephone enquiry system
pp
advantage is taken of the disparate speeds of input keyboard and
output speech to hasten the dialogue by imposing a questionanswer structure
on it with the computer taking the initiative  the machine can
afford to be slightly verbose if by so doing it makes the callers
response easier and therefore more rapid  moreover operators who
are experienced enough with the system to anticipate questions can
easily forestall them just by typing ahead for the computer is programmed
to examine its input buffer before issuing prompts and to suppress them if
input has already been provided
pp
an important aim of the system is to allow application programmers with no
special knowledge of speech to write independent services for it
table  shows an example of the use of one such application program
rf
fi
nh
na
in i
nr x wcomputer  
nr x wcaller  
innxu
tinxu
computer  stores information service  please enter
component name
tinxu
callerhnxunxu  enters sn
tinxu
computer  the component name is sn  is this correct
tinxu
callerhnxunxu  enters  system convention for yes
tinxu
computer  this component is in stores
tinxu
callerhnxunxu  enters  command for price
tinxu
computer  the component price is  pence
tinxu
callerhnxunxu  enters  command for minimum number
tinxu
computer  the minimum number of this component kept
in stores is 
tinxu
callerhnxunxu  enters sn
tinxu
computer  the component name is sn  is this correct
tinxu
callerhnxunxu  enters 
tinxu
computer  this component is not in stores
tinxu
callerhnxunxu  enters  command for delivery time
tinxu
computer  the expected delivery time is  days
tinxu
callerhnxunxu  enters 
tinxu
computer  which service do you require
in 
nf
fg table   the stores information service
the
stores information service which permits enquiries to be made of a database
holding information on electronic components kept in stock
this subsystem is driven by
ul
alphanumeric
data entered on the touchtone keypad  two or three letters are associated
with each digit in a manner which is fairly standard in touchtone telephone
applications  these are printed on a card overlay
that fits the keypad see figure   although true alphanumeric data entry
would require a multiple key press for each character
the ambiguity inherent in
a singlekeypercharacter convention can usually be resolved by the computer
if it has a list of permissible entries  for example the component names
sn and ztx are read by the machine as  and  respectively
confusion rarely occurs if the machine is expecting a valid component code
the same holds true of peoples names and file names em although with these
one must take care not to identify a series of files by similar names like
txa txb txc  it is easy for the machine to detect the rare cases
where ambiguity occurs and respond by requesting further information  the
component name is sn  is this correct  in fact the stores information
service illustrated in table  is defective in that it
ul
always
requests confirmation of an entry even when no ambiguity exists  the
use of a telephone keypad for data entry will be taken up again in chapter 
pp
a distinction is drawn throughout the system between data entries and
commands the latter being prefixed by a   in this example the
programmer chose to define a command for each possible question about a
component so that a new component name can be entered at any time
without ambiguity  the price paid for the resulting brevity of dialogue
is the burden of memorizing the meaning of the commands  this is an
inherent disadvantage of a onedimensional auditory display over the
more conventional graphical output   presenting menus by speech is tedious and
longwinded  in practice however for a simple task such as the
stores information service it is quite convenient for the caller to
search for the appropriate command by trying out all possibilities em there
are only a few
pp
the problem of memorizing commands is alleviated by establishing some
systemwide conventions  each input is terminated by a  and
the meaning of standard commands is given in table 
rf
fi
nh
na
in i
nr x w alone  
nr x wem  
ta nxu nxu
nr x nxnx
innxu
tinxu
emerase this input line regardless of what has
been typed before the 
tinxu
emstop  used to exit from any service
tinxu
emyes
tinxu
emno
tinxu
emrepeat question or summarize state of current
transaction
tinxu
 aloneemshort form of repeat  repeats or summarizes
in an abbreviated fashion
ta i i i i i i i i i i i i
in 
nf
fg table   systemwide conventions for the service
pp
a summary of services available on the system is given in
table 
rf
fi
na
in i
nr x w  
nr x wem  
nr x nxnx
innxu
ta nxu nxu
tinxu
emtells the time
tinxu
embiffo a game of nim
tinxu
emmoo a game similar to that marketed under the name mastermind
tinxu
emerror demonstration
tinxu
emspeak a file in phonetic format
tinxu
emlistening test
tinxu
emmusic allows you to enter a tune and play it
tinxu
emgives the date
sp
tinxu
emsquash ladder
tinxu
emstores information service
tinxu
emcomputes means and standard deviations
tinxu
emtelephone directory
sp
tinxu
emuser information
tinxu
emchange password
tinxu
emgripe permits feedback on services from caller
sp
tinxu
emfirst year laboratory marks entering service
sp
tinxu
emrepeat utterance allows testing of system
tinxu
emspeak utterance allows testing of system
tinxu
emenabledisable user  a nopassword guest user number
tinxu
emmount a magnetic tape on the computer
tinxu
emsetreset demonstration mode prohibits access by lowpriority users
tinxu
eminhibit games
tinxu
eminhibit the moo game
tinxu
emdisable password checking when users log in
ta i i i i i i i i i i i i
in 
nf
fg table   summary of services on a telephone enquiry system
they range from simple games and demonstrations through serious database
services to system maintenance facilities
a priority structure is imposed upon them with higher
service numbers being available only to higher priority users
services in the lowest range  can be obtained by all while
those in the highest range  are maintenance services
available only to the system designers  access to the lowernumbered
games services can be inhibited by a priority user em this was
found necessary to prevent overuse of the system  another advantage
of telephone access to an information retrieval system is that some
daytoday maintenance can be done remotely from the office telephone
pp
this telephone enquiry service which was built in  demonstrated that
speech synthesis had moved from a specialist phonetic discipline into the
province of engineering practicability  the speech was generated by rule
from a phonetic input the method is covered in chapters  and  which
has very low data storage requirements of around  bits of speech
thus an enormous vocabulary and range of services could be accomodated on a
small computer system
despite the fairly low quality of the speech the response from callers was
most encouraging  admittedly the user population was a selfselected body of
university staff which one might suppose to have high tolerance to new ideas
and a system designed for the general public would require more effort to be
spent on developing speech of greater intelligibility  although it was
observed that some callers failed to understand parts of the responses even
after repetition communication was largely unhindered in most cases users
being driven by a high motivation to help the system help them
pp
the use of speech output in conjunction with a simple input device requires
careful thought for interaction to be successful and comfortable  it is
necessary that the computer direct the conversation as much as possible
without seeming to be taking charge  provision for eliminating prompts
which are unwanted by sophisticated users is essential to avoid frustration
we will return to the topic of programming techniques for speech interaction
in chapter 
pp
making a computer system available over the telephone results in a sudden
vast increase in the user population  although peoples reaction to a new
computer terminal in every office was overwhelmingly favourable careful
resource allocation was essential to prevent the service being hogged by a
persistent few  as with all multiaccess computer systems it is particularly
important that error recovery is effected automatically and gracefully
sh   speech output in the telephone exchange
pp
the telephone enquiry service was an experimental vehicle for research on speech
interaction and was developed in 
since then speech has begun to be used in real commercial applications
one example is system x the british post offices computercontrolled
telephone exchange  this incorporates many features
not found in conventional telephone exchanges
for example if a number is found to be busy the call can be attempted
again by a repeat last call command without having to redial the full number
alternatively the last number can be stored for future redialling freeing
the phone for other calls
short code
dialling allows a customer to associate short codes with commonlydialled
numbers
alarm calls can be booked at specified times and are made automatically
without human intervention
incoming calls can be barred as can outgoing ones  a diversion service
allows all incoming calls to be diverted to another telephone either
immediately or if a call to the original number remains unanswered for
a specified period of time or if the original number is busy
threeparty calls can be set up automatically without involving the
operator
pp
making use of these facilities presents the caller with something of a problem
with conventional telephone exchanges feedback is provided on what is happening
to a call by the use of four tones em the dial tone the busy tone
the ringing tone and the number unavailable tone
for the more sophisticated interaction which is expected on the advanced
exchange a much greater variety of status signals is required
the obvious solution is to use
computergenerated spoken
messages to inform the caller when these services are invoked and to guide him
through the sequences of actions needed to set up facilities like call
redirection  for example the messages used by the exchange when a user
accesses the alarm call
service are
lb
ni
alarm call service
dial the time of your alarm call followed by squareudgd
fn 
dgdsquare is the term used for the  key on the touchtone telephoneu
ef
ni
you have booked an alarm call for seven thirty hours
ni
alarm call operator  at the third stroke it will be seven thirty
le
pp
because of the rather small vocabulary the number of messages that can be
stored in their entirety rather than being formed by concatenation of
smaller units and the short time which was available for development
system x stores speech as a time waveform slightly compressed by a timedomain
encoding operation such techniques are described in chapter 
utterances which contain variable parts like the time of alarm in the messages
above are formed by inserting separatelyrecorded digits in a fixed 
carrier message  no attempt is made to apply uniform intonation
contours to the synthetic utterances  the resulting speech is of excellent
quality being a slightly compressed recording of a human voice but sometimes
exhibits somewhat anomalous pitch contours
for example the digits comprising numbers often sound rather jerky and
outofcontext em which indeed they are
pp
even more advanced facilities can be expected on telephone exchanges in
the future  a message storage capability is one example  although
automatic call recording machines have been available for years a centralized
facility could time and date a message collect the callers identity
using the telephone keypad and allow the recipient to select messages left
for him through an interactive dialogue so that he could control the order
in which he listens to them  he could choose to leave certain messages to be
dealt with later or reroute them to a colleague  he may even wish to leave
reminders for himself to be dialled automatically at specified times like
alarm calls with userdefined information attached  the sender of a message
could be informed automatically by the system when it is delivered  none of
this requires speech recognition but it does need economical speech
ul
storage
and also speech
ul
synthesis
for time and date tags
sh   travel consultant
pp
some current research in speech communication with computers is aimed at
embedding the dialogue in a task environment which provides it with an overall
contextual framework
this in fact was the motivation for the experimental telephone enquiry service
described above and it was observed that people are able to tolerate rather
low quality speech if they feel that the interaction is worthwhile  one step
further is the use of speech
ul
input
as well as output and here the context of the conversation which is provided
by the task environment can be used to improve recognition accuracy
significantly
a system set up at bell telephone laboratories indicates what we can expect
to find operating commercially in the future levinson and shipley 

levinson shipley 

it provides timetable information about airline flights and allows the user
to make reservations
pp
an example dialogue is given in table 
rf
fi
nh
na
in i
nr x wcomputer  
nr x wcaller  
innxu
tinxu
computer  this is the bell laboratories flight information system  may i help you
tinxu
callerhnxunxu  i would like to go to chicago
tinxu
computer  please specify the day of your departure
tinxu
callerhnxunxu  i want to leave at four pm
tinxu
computer  please specify the day of your departure
tinxu
callerhnxunxu  i want to depart on the two five may
tinxu
computer  flight number nine two leaves new york at four pm arrives in chicago at five twenty five pm
tinxu
callerhnxunxu  what plane is on the flight
tinxu
computer  the aircraft is a boeing seven twenty seven
tinxu
callerhnxunxu  how much is the fare
tinxu
computer  the round trip coach fare is one hundred forty two dollars
tinxu
callerhnxunxu  i want a first class seat
tinxu
computer  one seat thank you
tinxu
callerhnxunxu  what is the fare
tinxu
computer  the round trip first class fare is one hundred eighty four dollars
tinxu
callerhnxunxu  please repeat the arrival time
tinxu
computer  the arrival time is five twenty five pm
tinxu
callerhnxunxu  i will take the flight
tinxu
computer  your reservation on flight number nine two to chicago on friday is confirmed  thank you
in 
nf
mt 
table   a conversation with an airline flight information service
from levinson and shipley 
te

levinson shipley 

the user dials the system from an
ordinary telephone  the recognition side must be trained by each user and
accepts isolated words spoken with brief pauses between them
the voice response unit has a vocabulary of around  words and
synthesizes its answers by slotting words into templates evoked by the speech
understanding part in response to a query  for example
lb
ni
this flight makes em stops
ni
flight number em leaves em at em  arrives in em at em
le
are templates which when called with specific slot fillers could produce the
utterances
lb
ni
this flight makes three stops
ni
flight number nine two leaves new york at four pm
arrives in chicago at five twentyfive pm
le
the chief research interest of the system is in its speech understanding
capabilities and the method used for speech output is relatively
straightforward  the templates and words are recorded digitized compressed
slightly and stored on disk files totalling a few hundred thousand bytes of
storage using techniques similar to those of system x
again no independent manipulation of pitch is possible and so the utterances
sound intelligible but the transition between templates and slot fillers is not
completely fluent  however the overall context of the interaction means that
the communication is not seriously disrupted even if the machine occasionally
misunderstands the man or vice versa  the users attention is drawn away from
recognition accuracy and focussed on the exchange of information with the machine
the authors conclude that progress in speech recognition can best be made by
studying it in the context of communication rather than in a vacuum or as part
of a oneway channel and the same is undoubtedly true of speech synthesis as
well
sh   reading machine for the blind
pp
perhaps the most advanced attempt to provide speech output from a computer
is the kurzweil reading machine for the blind first marketed in the late
s figure 
fc figure 
this device reads an ordinary book aloud  users adjust the reading
speed according to the content of the material and their familiarity with
it and the maximum rate has recently been improved to around  words per
minute em perhaps half as fast again as normal human speech rates
pp
as well as generating speech from text the machine has to scan the document
being read and identify the characters presented to it  a scanning camera
is used controlled by a program which searches for and tracks the lines of
text  the output of the camera is digitized and the image is enhanced
using signalprocessing techniques  next each individual letter must be
isolated and its geometric features identified and compared with a prestored
table of letter shapes  isolation of letters is not at all trivial for
many type fonts have ligatures which are combinations of characters joined
together for example the letters fi are often run together  the
machine must cope with many printed type fonts as well as typewritten ones
the textrecognition side of the kurzweil reading machine is in fact one of
its most advanced features
pp
we will discuss the problem of speech generation from text in chapter 
it has many facets  first there is pronunciation the
translation of letters to sounds  it is important to take into account
the morphological structure of words dividing them into root and endings
many words have concatenated suffixes like likeliness  these are
important to detect because a final e which appears on a root word
is not pronounced itself but affects the pronunciation of the previous
vowel  then there is the difficulty that some words look the same
but are pronounced differently depending on their meaning or on the syntactic
part that they play in the sentence
appropriate intonation is extremely difficult to generate from a plain textual
representation for it depends on the meaning of the text and the way in which
emphasis is given to it by the reader  similarly the rhythmic structure is
important partly for correct pronunciation and partly for purposes of
emphasis
finally the sounds that have been deduced from the text need to be synthesized
into acoustic form taking due account of the many and varied contextual effects
that occur in natural speech  this by itself is a challenging problem
pp
the performance of the kurzweil reading machine is not good  while it seems
to be true that some blind people can make use of it it is far from
comprehensible to an untrained listener  for example
it will miss out words and even whole phrases hesitate in a
stuttering manner blatantly mispronounce many words fail to detect
es which should be silent and give completely wrong rhythms
to words making them impossible to understand
its intonation is decidedly unnatural monotonous and often downright
misleading  when it reads completely new text to people unfamiliar with its
quirks
they invariably fail to understand more than an odd word here and there
and do not improve significantly when the text is repeated more than once
naturally performance improves if the material is familiar or expected
in some way
one useful feature is the machines ability to spell out difficult words
on command from the user
pp
while not wishing to denigrate the kurzweil machine which is a remarkable
achievement in that it integrates together many different advanced
technologies there is no doubt that the state of the art in speech synthesis
directly from unadorned text is extremely primitive at present
it is vital not to overemphasize the potential usefulness of abysmal speech
which takes a great deal of training on the part of the user before
it becomes at all intelligible  to make a rather extreme analogy
morse code could be used as
audio output requiring a great deal of training but capable of being understood
at quite high rates by an expert
it could be generated very cheaply
but clearly the man in the street would find it quite unacceptable as
an audio output medium because of the excessive effort required to learn to use
it  in many applications very bad synthetic speech is just as useless
however the issue is complicated by the fact that for people who use
synthesizers regularly synthetic speech becomes quite easily comprehensible
we will return to the problem of evaluating the quality of artificial speech
later in the book chapter 
sh   system considerations for speech output
pp
fortunately very many of the applications of speech output from computers
do not need to read unadorned text
in all the example systems described above except the reading machine
it is enough to be able to store utterances in some representation which can
include preprogrammed cues for pronunciation rhythm and intonation in
a much more explicit way than ordinary text does
pp
of course techniques
for storing audio information have been in use for decades
for example a domestic cassette tape recorder stores speech at much better
than telephone quality at very low cost  the method of direct
recording of an analogue waveform is currently used for announcements in
the telephone network to provide information such as the time weather
forecasts and even bedtime stories
however it is difficult to provide rapid access to messages stored in
analogue form and although some computer peripherals which use analogue
recordings for voiceresponse applications have been marketed em they are
discussed briefly at the beginning of chapter  em they have been
superseded by digital storage techniques
pp
although direct storage of a digitized audio waveform is used in some
voiceresponse systems the approach has certain limitations  the most
obvious one is the large storage requirement  suitable coding can reduce
the datarate of speech to as little as one hundredth of that needed by
direct digitization and textual representations reduce it by another factor
of ten or twenty  of course the speech quality is inevitably compromised
somewhat by datacompression techniques  however the cost of storage is
dropping so fast that this is not necessarily an overriding factor
a more fundamental limitation is that utterances stored directly cannot sensibly
be modified in any way to take account of differing contexts
pp
if the results of certain kinds of analyses
of utterances are stored instead of simply the digitized waveform
a great deal more flexibility can be gained
it is possible to separate out the features of intonation and amplitude from
the articulation of the speech and this raises the attractive possibility
of regenerating utterances with pitch contours different from those with which they were
recorded
the primary analysis technique used for this purpose is
ul
linear prediction
of speech and this is treated in some detail in chapter   it also reduces drastically the
datarate of speech by a factor of around 
it is likely that many voiceresponse systems in the short and mediumterm
future will use linear predictive representations for utterance storage
pp
for maximum flexibility however it is preferable to store a textual
representation of the utterance
there is an important distinction between speech
ul
storage
where an actual human utterance is recorded perhaps processed to lower
the datarate and stored for subsequent regeneration when required
and speech
ul
synthesis
where the machine produces its own individual utterances which are not based
on recordings of a person saying the same thing  the difference is summarized
in figure 
fc figure 
in both cases something is stored  for the first it is
a direct representation of an actual human utterance while for the second
it is a typed
ul
description
of the utterance in terms of the sounds or phonemes which constitute it
the accent and tone of voice of the human speaker will be apparent in
the stored speech output while for synthetic speech the accent is the
machines and the tone of voice is determined by the synthesis program
pp
probably the most attractive representation of utterances in manmachine
systems is ordinary english text as used by the kurzweil reading machine
but as noted above this poses extraordinarily difficult problems for the
synthesis procedure and these inevitably result in severely degraded speech
although in the very long term these problems may indeed be solved
most speech output systems can adopt as their representation of an utterance
a description of it which explicitly conveys the difficult features of
intonation rhythm and even pronunciation
in the kind of applications described above barring the reading machine
input will be prepared by a
programmer as he builds the software system which supports the interactive
dialogue
although it is important that the method of specifying utterances be easily
learned it is not necessary that plain english
is used  it should be simple for the programmer to enter new
utterances and modify them online in cutandtry attempts to render the
manmachine dialogue as natural as possible  a phonetic input
can be quite adequate for this especially if the system allows the
programmer to hear immediately the synthesized version of the message
he types  furthermore markers which indicate rhythm and intonation can
be added to the message so that the system does not have to deduce these features
by attempting to understand the plain text
pp
this brings us to another disadvantage of speech storage as compared with
speech synthesis  to provide utterances for a voice response system using
stored human speech one must assemble together special input hardware
a quiet room and probably a dedicated computer  if the speech is to be
heavily encoded either expensive special hardware is required or the encoding
process if performed by software on a generalpurpose computer will take
a considerable length of time perhaps hundreds of times realtime  in
either case timeconsuming editing of the speech will be necessary with
followup recordings to clarify sections of speech which turn out to be
unsuitable or badly recorded  if at a later date the voice response
system needs modification it will be necessary to recall the same speaker
or rerecord the entire utterance set  this discourages the application
programmer from adjusting his dialogue in the light of experience
synthesizing from a textual representation on the other hand allows him
to change a speech prompt as simply as he could a vdu one and evaluate
its effect immediately
pp
we will return to methods of digitizing and compacting speech in chapters 
and  and carry on to consider speech synthesis in subsequent chapters
firstly however it is necessary to take a look at what speech is and how
people produce it
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
there are remarkably few general books on speech output although a
substantial specialist literature exists for the subject
in addition to the references listed above i suggest that you look
at the following
lb nn
ainsworth

ds a ainsworth wa
ds d 
ds t mechanisms of speech recognition
ds i pergamon
nr t 
nr a 
nr o 
  book
inn
a nice easygoing introduction to speech recognition this book covers
the acoustic structure of the speech signal in a way which makes
it useful as background reading for speech synthesis as well
it complements lea  cited above which presents more recent results
in greater depth
inn
flanagan

ds a flanagan jl
as a  and rabiner lr editors
ds d 
ds t speech synthesis
ds i wiley
nr t 
nr a 
nr o 
  book
inn
this is a collection of previouslypublished research papers on speech
synthesis rather than a unified book
it contains many of the classic papers on the subject from   
and is a very useful reference work
inn
leboss

ds a leboss b
ds d 
ds k 
ds t speech io is making itself heard
ds j electronics
ds o may 
ds p 
nr p 
nr t 
nr a 
nr o 
  journalarticle
inn
the magazine
ul
electronics
is an excellent source of uptotheminute news product announcements
titbits and rumours in the commercial speech technology world
this particular article discusses the projected size of the voice
output market and gives a brief synopsis of the activities of several
interested companies
inn
witten

ds a witten ih
ds d 
ds t communicating with microcomputers
ds i academic press
ds c london
nr t 
nr a 
nr o 
  book
inn
a recent book on microcomputer technology this is unusual in that
it contains a major section on speech communication
with computers as well as ones
on computer buses interfaces and graphics
inn
le nn
eq
delim 
en
ch   what is speech
ds rt what is speech
ds cx principles of computer speech
pp
people speak by using their vocal cords as a sound source and making rapid
gestures of the articulatory organs tongue lips jaw and so on
the resulting changes in shape of the vocal tract allow production
of the different sounds that we know as the vowels and consonants of
ordinary language
pp
what is it necessary to learn about this process for the purposes of
speech output from computers
that depends crucially upon how speech is represented in the system
if utterances are stored as time waveforms em and this is what we will be
discussing in the next chapter em the structure of speech is not important
if frequencyrelated parameters of particular natural utterances are
stored then it is advantageous to take into account some of the
acoustic properties of the speech waveform
pp
this point can be brought into focus by contrasting the transmission
or storage of speech with that of reallife television pictures
as has been proposed for a videophone service
massive data reductions of the order of  can be achieved for speech
using techniques that are described in later chapters  for pictures
data reduction is still an important issue em even more so for the
videophone than for the telephone because of the vastly higher
information rates involved
unfortunately the potential for data reduction is much
smaller em nothing like the  figure quoted above
this is because speech sounds have definite characteristics imparted
by the fact that they are produced by a human vocal tract which
can be exploited for data reduction
television pictures have no equivalent generative structure for
they show just those things that the camera points at
pp
moving up from frequencyrelated parameters of
ul
particular
utterances it
is possible to store such parameters in a
ul
general
form which characterizes the sound segments that appear in spoken language
this immediately raises the issue of
ul
classification
of sound segments to form a basis for storing generalized acoustic
information and for retrieval of the information needed to synthesize
any particular utterance
speech is by nature continuous and any synthesis system based upon
discrete classification must come to terms with this by tackling
the problems of transition from one segment to another
and local modification of sound segments as a function of their context
pp
this brings us to another level of representation
so far we have talked of the
ul
acoustic
nature of speech but when we have to cope with transitions between
discrete sound segments it may be fruitful to consider
ul
articulatory
properties as well
any model of the speech production process
is in effect a model of the articulatory process that generates the speech
some speech research is concerned with
modelling
the vocal tract directly rather than modelling the acoustic output from it
one might specify for example position of tongue and posture of jaw and lips
for a vowel instead of giving frequencyrelated
characteristics of it  this is a potent
tool in linguistic research for it brings one closer to human production of
speech em in particular to the connection between brain and articulators
pp
articulatory
synthesis holds a promise of highquality speech for the transitional
effects caused by tongue and jaw inertia can be modelled directly
however this potential has
not yet been realized
speech from current articulatory models is of much poorer quality than
that from acousticallybased synthesis methods
the major problem is in gaining data about articulatory
behaviour during running speech em it is much easier to perform acoustic
analysis on the resulting sound than it is to examine the vocal organs in
action  because of this the subject is not treated in this book
we will only look at articulatory properties insofar as they help us
to understand in a qualitative way the acoustic nature of speech
pp
speech however is much more than mere articulation
consider em admittedly a rather extreme and chauvinistic example em the
number of ways a girl can say yes
breathy voice slow tempo low pitch em these are all characteristics which
affect the utterance as a whole rather than being classifiable into
individual sound segments  linguists call them prosodic or
suprasegmental features for they relate to overall aspects of the
utterance and distinguish them from segmental ones which concern
the articulation of individual segments of syllables
the most important prosodic features are pitch or fundamental frequency
of the voice and rhythm
pp
this chapter provides a brief introduction to the nature of the speech
signal  depending upon what speech output techniques we use it may be
necessary to understand something of the acoustic nature of the speech
signal the system that generates it the vocal tract commonlyused
classifications of sound segments and the prosodic aspects of speech
this material is little used in the early chapters of the book but
becomes increasingly important as the story unfolds
hence you may skip the remainder of this chapter if you wish but
should return to it later to pick up more background whenever it
becomes necessary
sh   the anatomy of speech
pp
the socalled voiced sounds of speech em like the sound you make when
you say aaah em are produced by passing air up from the lungs through
the larynx or voicebox which is situated just behind the adams apple
the vocal tract from the larynx to the lips acts as a resonant cavity
amplifying certain frequencies and attenuating others
pp
the waveform generated by the larynx however is not simply sinusoidal
if it were the vocal tract resonances would merely
give a sine wave of the same frequency but amplified or
attenuated according to how close it was to the nearest resonance  the
larynx contains two folds of skin em the vocal cords em which blow apart and flap
together again in each cycle of the pitch period
the pitch of a male voice in speech varies from as low as  hz
cycles per second to perhaps
 hz with a typical median value of  hz
for a female voice the range is higher up to about  hz in speech
singing can go much higher  a top c sung by a soprano has a frequency
of just over  hz and some opera singers can reach
substantially higher than this
pp
the flapping action of the vocal cords
gives a waveform which can be approximated by a
triangular pulse this and other approximations will be discussed in
chapter 
it has a rich spectrum of harmonics
decaying at around  dboctave and each harmonic is affected
by the vocal tract resonances
rh vocal tract resonances
a simple model of the vocal tract is an organpipelike cylindrical tube
figure 
with a sound source at one end the larynx and open at the other the lips
fc figure 
this has resonances at wavelengths l l l  where l
is the length of the tube
and these correspond to frequencies cl cl cl  hz c
being the speed of
sound in air
calculating these frequencies using a typical figure for the
distance between larynx and lips of  cm
and c   ms for the speed of sound leads to resonances at
approximately  hz  hz  hz  
pp
when excited by the harmonicrich waveform of the larynx
the vocal tract resonances produce
peaks known as
ul
formants
in the energy spectrum of the speech wave figure 
fc figure 
the lowest formant called formant one varies from around  hz
to  hz during speech the exact range depending on the size
of the vocal tract
formant two varies from around  to  hz and formant three
from around  to  hz
pp
you can easily hear the lowest formant by whispering the vowels in
the words heed hid head had hod hawed and whod
they appear to have a steadily descending pitch yet since you are
whispering there is no fundamental frequency
what you hear is the lowest resonance of the vocal tract em formant one
some masochistic people can play simple tunes with this formant by putting
their mouth in successive vowel shapes and knocking the top of their head
with their knuckles em hard
pp
a difficulty occurs when trying to identify the lower formants for speakers
with highpitched voices
when a formant frequency falls below the fundamental excitation frequency
of the voice its effect is diminished em although it is still present
the vibrato used by opera singers provides a very lowfrequency excitation
at the vibrato rate which helps to illuminate the lower formants even
when the pitch of the voice is very high
pp
of course speech is not a static phenomenon
the organpipe model describes the speech spectrum during a continuously
held vowel with the mouth in a neutral position such as for aaah
but in real speech the tongue and lips are in continuous motion
altering the shape of the vocal tract and hence the positions of the resonances
it is as if the organpipe were being squeezed and expanded in
different places all the time
say
ul
ee
as in heed and feel how close your tongue is to the roof of your mouth
causing a constriction near the front of the vocal cavity
pp
linguists and speech engineers use a special frequency analyser called a
sound spectrograph to make a threedimensional plot of the variation
of the speech energy spectrum with time
figure  shows a spectrogram of the
utterance go away
fc figure 
frequency is given on the vertical axis
and bands are shown at the beginning to indicate the scale
time is plotted horizontally
and energy is given by the darkness of any particular area
the lower few formants can be seen as dark bands extending horizontally
and they are in continuous motion
in the neutral first vowel of away the formant frequencies
pass through
approximately the  hz  hz and  hz that we calculated earlier
in fact formants two and three are somewhat lower than these values
pp
the
fine vertical striations in the spectrogram correspond to single openings of the vocal cords
pitch changes continuously throughout an utterance
and this can be seen on the spectrogram by the differences in spacing
of the striations
pitch change or
ul
intonation
is singularly important in
lending naturalness to speech
pp
on a spectrogram a continuously held vowel shows up as a static energy spectrum
but beware em what we call a vowel in everyday language is not the same thing as a
vowel in phonetic terms
say i and feel how the tongue moves continuously while youre speaking
technically this is a
ul
diphthong
or slide between two vowel positions
and not a single vowel
if you say
ul
ar
as in hard
and change slowly to
ul
ee
as in heed you will obtain a diphthong not unlike that in i
and there are many more phonetically different vowel sounds
than the a e i o and u that we normally think of
the words hood and mood have different vowels for example as do head and mead
the principal acoustic difference between the various vowel sounds
is in the frequencies of the first two formants
pp
a further complication is introduced by the nasal tract  this is
a large cavity which is coupled to the oral tract by a passage at the
back of the mouth
the passage is guarded by a flap of skin called the velum
you know about this because inadvertent opening of the velum while
swallowing causes food or drink to go up your nose
the nasal cavity is switched in and out of the vocal tract
by the velum during speech
it is used for consonants
ul
m
ul
n
and the
ul
ng
sound in the word
singing
vowels are frequently nasalized too
a very effective demonstration of the amount of nasalization in ordinary
speech can be obtained by cutting a noseshaped hole in a large
baffle which divides a room speaking normally with ones nose in the hole
and having someone listen on the other side
the frequency of occurrence of
nasal sounds and the volume of sound that is emitted
through the nose are both surprisingly large
interestingly enough when we say in conversation that someone sounds
nasal we usually mean nonnasal  when the nasal passages are
blocked by a cold nasal sounds are missing em
ul
nc
s turn into
ul
dc
s
and
ul
mc
s to
ul
bc
s
pp
when the nasal cavity is switched in to the vocal tract it introduces
formant resonances just as the oral cavity does
although we cannot
alter the shape of the nasal tract significantly the nasal formant
pattern is not fixed because the oral tract does play a part in nasal
resonances
if you say
ul
m
ul
n
and
ul
ng
continuously you can hear the difference and feel how it is produced by
altering the combined nasaloral tract resonances with your tongue position
the nasal cavity operates in parallel with
the oral one  this causes the two resonance patterns to be summed
together with resulting complications which will be discussed in chapter 
rh sound sources
speech involves sounds other than those caused by regular vibration of
the larynx
when you whisper the folds of the larynx are held slightly
apart so that the air passing between them becomes turbulent causing a noisy excitation
of the resonant cavity
the formant peaks are still present superimposed on the noise  such
aspirated sounds occur in the
ul
h
of hello and for a very short time
after the lips are opened at the beginning of pit
pp
constrictions made in the mouth produce hissy noises such as
ul
ss
ul
sh
and
ul
f
for example in
ul
ss
the tip of the tongue is high up
very close to the roof of the mouth
turbulent air passing through this constriction causes a
random noise excitation known as frication
actually the roof of the mouth is quite a complicated object
you can feel with your tongue a bony hump or ridge just behind the front
teeth and it is this that forms a constriction with the tongue for
ul
s
in
ul
sh
the tongue is flattened close to the roof of the mouth slightly farther back
in a position rather similar to that for
ul
ee
but with a narrower
constriction
while
ul
f
is produced with the upper teeth and lower lip
because they are made near the front of the mouth
the resonances of the vocal tract have little effect on these fricative
sounds
pp
to distinguish them from aspiration and frication the ordinary speech
sounds like aaah which have their source in larynx vibration are
known technically as voiced  aspirated and fricative sounds are called
unvoiced  thus the three different sound types can be classified as
lb
np
voiced
np
unvoiced fricative
np
unvoiced aspirated
le
can any of these three types occur together
it would seem that voicing and aspiration can not for the former requires
the larynx to be vibrating regularly but for the latter it must be
generating turbulent noise
however there is a condition known technically as breathy voice
which occurs when the vocal cords are slightly apart still vibrating
but with a large volume of air passing between to create turbulence
voicing can easily occur in conjunction with frication
corresponding to
ul
s
ul
sh
and
ul
f
we get the
ul
voiced
fricatives
ul
z
the sound in the middle of words like vision which i will call
ul
zh
and
ul
v
a simple illustration of voicing is to say ffffvvvvffff 
during the voiced part you can feel the larynx vibrations with a finger
on your adams apple and it can be heard quite clearly if you stop up
your ears
technically there is nothing to prevent frication and aspiration
from occurring together em they do for example when a voiced fricative
is whispered em but the combination is not an important one
pp
the complicated acoustic effects of noisy excitations in speech can be
seen in the spectrogram in figure  of
high altitude jets whizz past screaming
fc figure 
rh the sourcefilter model of speech production
we have been talking in terms of a sound source be it voiced or unvoiced
exciting the resonances of the oral and possible the nasal tract
this model which is used extensively in speech analysis and synthesis
is known as
the sourcefilter model of speech production  the reason for its success
is that the effect of the resonances can be modelled as a frequencyselective
filter operating on an input which is the source excitation
thus the frequency spectrum of the source is modified by multiplying it
by the frequency characteristic of the filter or adding it if amplitudes
are expressed logarithmically
this can be seen in figure  which shows a source
spectrum and filter characteristic which combine to give the overall
spectrum of figure 
fc figure 
pp
although as mentioned above the various fricatives are not subjected
to the resonances of the vocal tract to the same extent
that voiced and aspirated
sounds are they can still be modelled as a noise source followed by
a filter to give them their different sound qualities
pp
the sourcefilter model is an oversimplification of the actual speech
production system  there is inevitably some coupling between the vocal
tract and the lungs through the glottis during the period when
it is open  this effectively makes the filter characteristics
change during each individual cycle of the excitation
however although the effect is of interest to speech researchers
it is probably not of great significance for practical speech output
pp
one very interesting implication of the
sourcefilter model is that the prosodic features of
pitch and amplitude are largely properties of the source while
segmental ones are introduced by the filter  this makes it possible to
separate some aspects of
overall prosody from the actual segmental content of an
utterance so that for example a human utterance can be stored initially
and then spoken by a machine with a variety of different intonations
sh   classification of speech sounds
pp
the need to classify sound segments as a basis for storing generalized acoustic
information and retrieving it was mentioned earlier  there is a real
difficulty here because speech is by nature continuous and classifications are
discrete
it is important to remember this difficulty because it is all too easy
to criticize the complex and often confusing attempts of linguists to
tackle the classification task
pp
linguists call a written representation of the
ul
sounds
of an utterance a phonetic
transcription of it  the same utterance can be transcribed at
different levels of detail  simple transcriptions are called broad
and more specific ones are called narrow
perhaps the most logically satisfying kind of transcription employs units
termed phonemes  this is the broadest transcription
and is sometimes called a
ul
phonemic
transcription to emphasize that that it is in terms of phonemes
unfortunately the word phoneme is often used somewhat loosely
in its true sense a phoneme is a
ul
logical
unit rather than a physical acoustic one
and is defined in relation to a particular language by reference
to its use in discriminating different words
classifications of sounds which are based on their
semantic
role as worddiscriminators are called
ul
phonological
classifications  we could ensure that there is no ambiguity in the sense
with which we use the term phoneme by calling it a phonological unit and
the phonemic transcription could be called a phonological one
rh broad phonetic transcription
a phoneme is an abstract unit representing a set of different sounds
the issue is confused by the fact that the members of the set actually
sound very similar if not identical to the untrained ear em precisely because
the difference between them plays no part in distinguishing words from
each other in the particular language concerned
pp
take the words key and caw for example  despite the difference in
spelling both of them begin with a
ul
k
sound that belongs in english
to the same phoneme set called
ul
k
however say them two or three times each concentrating on the position of
the tongue during the
ul
k
it is quite different in each case  for key it
is raised close to the roof of the mouth in preparation for the
ul
ee
whereas in caw it is much lower down
the sound of the
ul
k
is actually quite different in the two cases
yet they belong to the same phoneme for there is no pair of words which
relies on this difference to distinguish them em key and caw are
obviously distinguished by their vowels not by the initial
consonant
you probably cannot hear clearly the difference between the two
ul
kc
s
precisely because they belong to the same phoneme and so the difference
is not important for english
pp
the point is sharpened by considering another language where we make a
distinction em and hence can hear the difference em between two sounds
that belong in the language to the same phoneme
japanese does not distinguish
ul
r
from
ul
l
japanese people
ul
do not hear
the difference between lice and rice in the same way that you do
not hear the difference between the two
ul
kc
s above
cockneys do not hear except with a special effort the difference
between has and as or haitch and aitch for the cockney dialect
does not recognize initial
ul
hc
s
pp
so what is a phoneme  it is a set of sounds whose members do not
discriminate between any words in the language under consideration
if you are mathematically minded you could think of it as an equivalence
class of sounds determined by the relationship
lb
sound sub  is related to sound sub  if sound sub  and sound sub 
do not discriminate any pair of words in the language
le
the
ul
p
and
ul
d
in
pig and dig belong to different phonemes in english
because they discriminate
the two words
ul
b
ul
f
and
ul
j
belong to different phonemes again
ul
i
and
ul
a
in hid and had belong to different phonemes too
proceeding like this a list of phonemes can be drawn up
pp
such a list is shown in table  for british english
the layout of the list does have some significance in terms of different
categories of phonemes which will be explained later  in fact
linguists use an
assortment of english letters foreign letters and special
symbols to represent phonemes  in this book we use one or twoletter
codes partly because they are more mnemonic and partly because
they are more suitable for communication to computers using standard
peripheral devices
they are
a direct transliteration of linguists standard international phonetic
association symbols
rf
nr x miiiiwyu
nr x nlnx
in nxu
ta m i i i i i i
fiuhfrthefipfrfitfrfikfr
fiafrbudfibfrfidfrfigfr
fiefrheadfimfrfinfrfingfr
fiifrhid
fiofrhodfirfrfiwfrfilfrfiyfr
fiufrhood
fiaafrhadfisfrfizfr
fieefrheedfishfrfizhfr
fierfrheardfiffrfivfr
fiuufrfoodfithfrfidhfr
fiarfrhardfichfrfijfr
fiawfrhoardfihfr
ta i i i i i i i i i i i i
in 
fg table  the phonemes of british english
pp
we will discuss the sounds which make up each of these phoneme classes
shortly  first however it is worthwhile pointing out some rather
tricky points in the definition of these phonemes
rh phonological difficulties
there are snags with phonological classification as there are
in any area where attempts are made to make completely logical
statements about human activity
consider
ul
h
and the
ul
ng
in singing
c
ul
ng
is certainly not an
ul
n
sound followed by a
ul
g
sound although
it is true that in some english accents singing is rendered with
the
ul
ng
followed by a
ul
g
at each of its two occurrences  no words
end with
ul
h
and none begin with
ul
ng
notice that we are still talking about british english
in chinese the sound
ul
ng
is a word in its own right and is a common
family name
but we must stick with one language for phonological classification  hence
it follows that there is no pair of words which is distinguished
by the difference between
ul
h
and
ul
ng
technically
they belong to the same phoneme  however technical considerations
in this case must take second place to common sense
pp
the
ul
j
in jig is another interesting case  it can be considered
to belong to a
ul
j
phoneme or to be a sequence of two
phonemes
ul
d
followed by
ul
zh
the sound in vision  there is
disagreement on this point in phonetics textbooks and we do not
have the time nor probably the inclination to consider the
pros and cons of this moot point
i have resolved the matter arbitrarily by writing it as a separate
phoneme  the
ul
ch
in choose is a similar case
c
ul
t
followed by the
ul
sh
in shoes
pp
another difficulty this time where table  does not show how to
distinguish between two sounds which
ul
do
discriminate words in many peoples english is the
ul
w
in witch
and that in which  the latter is conventionally transcribed
as a sequence of two phonemes
ul
h w
pp
the last few difficulties are all to do with deciding whether a
sound belongs to a single phoneme class or comprises a sequence
of sounds each of which belongs to a phoneme
are the
ul
j
in jug the
ul
ch
in chug and the
ul
w
in which
single phonemes or not  the definition above of a phoneme
as a set of sounds whose members do not discriminate any words
in the language does not help us to answer this question
as far as this definition is concerned we could go so far as
to call each and every word of the language an individual phoneme
it is clear that some acoustic evidence and quite a lot of judgement
is being used when phonemes such as those of table  are defined
pp
so much for the consonants  this same problem occurs in vowel sounds
particularly in diphthongs which are sequences of two vowellike sounds
do the vowels of main and man belong to different phonemes
clearly so if they are both transcribed as single units for they
distinguish the two words
notwithstanding the fact that they are sequences of separate sounds
a logically consistent system could be constructed which gave separate
unitary symbols to each diphthong
however it is usual to employ a compound symbol which indicates explicitly
the character of the two vowellike sounds involved
we will transcribe the diphthong of main as a sequence of two
vowels
ul
e
as in head and
ul
i
as in hid not i
this is done primarily for economy of symbols choosing the constituent
sounds on the basis of the closest match to existing vowel sounds
note that this again violates purely
ul
logical
criteria for identifying phonemes
rh categories of speech sounds
a phoneme is defined as a set of sounds whose members to not discriminate
between any words in the language under consideration
the phonemes themselves can be classified into groups which reflect
similarities between them
this can be done in many different ways using various criteria
for classification  in fact one branch of linguistic research
is concerned with defining a set of distinctive
features such that a phoneme class is uniquely identified by
the values of the features  distinctive features are binary
and include such things as voicedemunvoiced fricativeemnot fricative
aspiratedemunaspirated  we will not be concerned here with such
detailed classifications but it is as well to know that they exist
pp
there is an everyday distinction between vowels and consonants
a vowel forms the nucleus of every syllable and one or more consonants
may optionally surround the vowel
but the distinction sometimes becomes a little ambiguous
syllables like
ul
sh
are commonly uttered and certainly do not
contain a vowel  furthermore when we say vowel in everyday
language we usually refer to the
ul
written
vowels a e i o and u there are many more vowel sounds
a vowel in orthography is different to a vowel as a phoneme
is a diphthong a phonetic vowel  em certainly by the syllablenucleus
criterion but it is a little different from ordinary vowels because
it is a changing sound rather than a constant one
pp
table  shows one classification of the phonemes of table  which
will be useful in our later studies of speech synthesis from phonetics
it shows twelve vowels including the rather peculiar one
ul
uh
which corresponds to the first vowel in the word above
this is the sound produced by the vocal tract when it is in a relaxed
neutral position and it never occurs in prominent stressed
syllables  the vowels later in the list are almost always longer
than the earlier ones  in fact the first six
c
ul
uh a e i o uc

are often called short vowels and the last five
c
ul
ee er uu ar awc

long ones  the shortness or longness of the one in the middle
c
ul
aac

is rather ambiguous
rf
nr x wunvoiced fricative    u
nr x nxwnot classified as individual phonemesu
nr x nlnx
in nxu
ta nxu
fi
vowelc
ul
uh  a  e  i  o  u  aa  ee  er  uu  ar  aw
br
diphthongnot classified as individual phonemes
br
glide or liquidc
ul
r  w  l  y
br
stop
br
unvoiced stopc
ul
p  t  k
br
voiced stopc
ul
b  d  g
br
nasalc
ul
m  n  ng
br
fricative
br
unvoiced fricativec
ul
s  sh  f  th
br
voiced fricativec
ul
z  zh  v  dh
br
affricate
br
unvoiced affricatec
ul
ch
br
voiced affricatec
ul
j
br
aspiratec
ul
h
nf
in 
ta i i i i i i i i i i i i
fg table  phoneme categories
pp
diphthongs pose no problem here because we have not classified them
as single phonemes
pp
the remaining categories are consonants  the glides are quite
similar to vowels and diphthongs though for they are voiced
continuous sounds  you can say them and prolong them
this is also true of the fricatives 
ul
r
is interesting
because it can be realized acoustically in very different ways
some people curl the tip of the tongue
back em a socalled retroflex action of the tongue  many people
cannot do this and their
ul
rc
s sound like
ul
wc
s
the stage scotsmans
ul
r
is a trill where the tip of the tongue vibrates against the roof of the mouth
ul
l
is also
slightly unusual for it is the only english phoneme which is lateral em
air passes either side of it in two separate passages  welsh
has another lateral sound a fricative which is written ll as
in llandudno
pp
the next category is the stops  these are formed by stopping up
the mouth so that air pressure builds up behind the lips and
releasing this pressure suddenly  the result is a little
explosion and the stops are often called plosives which
usually creates a very short burst of fricative noise and in some cases
aspiration as well  they are further subdivided into voiced and
unvoiced stops depending upon whether voicing starts as soon as
the plosion occurs sometimes even before or well after it
if you put your hand in front of your mouth when saying pit you
can easily feel the puff of air that signals the plosion on the
ul
p
and probably on the
ul
t
as well
pp
in a sense nasals are really stops as well and they are often
called stops for the oral tract is blocked although the nasal
one is not  the peculiar fact that the nasal
ul
ng
never occurs at the beginning of a word in english was mentioned
earlier  notice that for stops and nasals there is a similarity in the
ul
vertical
direction of table  between
ul
p
ul
b
and
ul
m
ul
t
ul
d
and
ul
n
and
ul
k
ul
g
and
ul
ng
ul
p
is an unvoiced version of
ul
b
try saying them
and
ul
m
is a nasalized version for
ul
b
is what you get when you
have a cold and try to say
ul
mc

these three sounds are all made
at the front of the mouth while
ul
t
ul
d
and
ul
n
which bear the
same resemblance to each other are made in the middle and
ul
k
ul
g
and
ul
ng
are made at the back  this introduces another
possible classification according to
ul
place of articulation
pp
the unvoiced fricatives are quite straightforward except perhaps
for
ul
th
which is the sound at the beginning of thigh
they are paired with the voiced fricatives on the basis of place
of articulation  the voiced version of
ul
th
is the
ul
dh
at
the beginning of thy
ul
zh
is a fairly rare phoneme which
is heard in the middle of vision  affricates are similar to
fricatives but begin with a stopped posture and we mentioned earlier
the controversy as to whether they should be considered to be
single phonemes or
sequences of stop phonemes and fricatives
finally comes the lonely aspirate
ul
h
aspiration does occur
elsewhere in speech during the plosive burst of unvoiced stops
rh narrow phonetic transcription
the phonological classification outlined above is based upon a clear
rationale for distinguishing between sounds according to how
they affect meaning em although the rationale does become
somewhat muddied in difficult cases
narrower transcriptions are not so systematic
they use units called
ul
allophones
which are defined by reference to physical acoustic criteria rather
than purely logical ones
phone is a more oldfashioned term for the same thing
and the misused word phoneme is often employed where allophone is
meant that is as a physical rather than a logical
unit  each phoneme has several allophones
more or less depending on how narrow or broad the transcription is
and the allophones are different acoustic realizations of the same
logical unit
for example the
ul
kc
s in key and caw may be considered as different
allophones in a slightly narrow transcription
although we will not use symbols for allophones here
they are often indicated by diacritical marks in a text
which modify the basic phoneme classes
for example a tilde  over a vowel means that it is nasalized while a small
circle underneath a consonant means that it is devoiced
pp
allophonic variation in speech is governed by a mechanism called
ul
coarticulation
where a sound is affected by those that come either side of it
keycaw is a clear example of this where the tongue
position in the
ul
k
anticipates that of the following vowel em high
in the first case low in the second
most allophonic variation in english is anticipatory in that the sound
is influenced by the following articulation rather than by
preceding ones
pp
nasalization is a feature which applies to vowels in english through
anticipatory coarticulation
in many languages for example french it is a
ul
distinctive
feature for vowels in that it serves to distinguish one vowel phoneme class
from another
that this is not so in english sometimes tempts us to assume
incorrectly that nasalization does not occur in vowels
it does typically when the vowel is followed by a nasal consonant and it is
important for synthesis that nasalized vowel allophones are recognized and
treated accordingly
pp
coarticulation can be predicted by phonological rules which show
how a phonemic sequence will be realized by allophones
such rules have been studied extensively by linguists
pp
the reason for coarticulation and for the existence of allophones
lies in the physical constraints imposed by the motion
of the articulatory organs em particularly their acceleration and deceleration
an immensely crude model is that the brain decides what phonemes to
say for it is concerned with semantic things and the definition
of a phoneme is a semantic one
it then takes this sequence and translates it into neural commands
which actually move the articulators into target positions
however other commands may be issued and executed before these targets
are reached and this accounts for coarticulation effects
phonological rules for converting a phonemic sequence to an
allophonic one are a sort of discrete model of the process
particularly for work involving computers it is possible that this
rulebased approach will be overtaken by potentially more accurate
methods which attempt to model the continuous articulatory phenomena
directly
sh   prosody
pp
the phonetic classification introduced above divides speech into
segments and classifies these into phonemes or allophones
riding on top of this stream of segments are other more global
attributes that dictate the overall prosody of the utterance
prosody is defined by the oxford english dictionary as the
science of versification laws of metre
which emphasizes the aspects of stress and rhythm that are central
to classical verse
there are however many other features which are more or less
global
these are collectively called prosodic or equivalently suprasegmental
features for they lie above the level of phoneme or syllable segments
pp
prosodic features can be split into two basic categories  features
of voice quality and features of voice dynamics
variations in voice quality which are sometimes called
paralinguistic phenomena are accounted for by anatomical
differences and longterm muscular idiosyncrasies like a sore
throat and have little part to play in the kind of applications
for speech output that have been sketched in chapter 
variations in voice dynamics occur in three dimensions  pitch
or fundamental frequency of the voice time and amplitude
within the first the pattern of pitch variation or
ul
intonation
can be distinguished from the overall range within which that variation
occurs
the time dimension encompasses the rhythm of the speech pauses and the
overall tempo em whether it is uttered quickly or slowly
the third dimension amplitude is of relatively minor importance
intonation and rhythm work together to produce an effect commonly called
stress and we will elaborate further on the nature of stress and discuss
algorithms for synthesizing intonation and rhythm in chapter 
pp
these features have a very important role to play in communicating meaning
they are not fancy optional components
it is their neglect which is largely responsible for the laymans
stereotype of computer speech
a caricature of living speech em abrupt arhythmic and in a grating
monotone em
which was well characterized by isaac asimov when he wrote of speaking
all in capital letters
pp
timing has a syntactic function in that it sometimes helps to
distinguish nouns from
verbs
c
ul
exc
tract versus exc
ul
tractc

and adjectives from verbs appc
ul
roxc
imate versus approxic
ul
matec
 em although segmental aspects play a part here too for the vowel
qualities differ in each pair of words
nevertheless if you make a mistake when assigning stress to words
like these in conversation you are very likely to be queried as
to what you actually said
pp
intonation has a big effect on meaning too
pitch often em but by no means always em rises on a question
the extent and abruptness of the rise depending on features like whether
a genuine informationbearing reply or merely confirmation is expected
a distinctive pitch pattern accompanies the introduction of a new topic
in conjunction with rhythm intonation can be used to bring out contrasts
as in
lb
ni
he didnt have a
ul
red
car he had a
ul
black
one
le
in general the intonation patterns used by a reader depend not only on
the text itself but on his interpretation of it and also on his
expectation of the listeners interpretation of it
for example
lb
ni
he had a
ul
red
car i think you thought it was black
ni
he had a red
ul
bic
cycle i think you thought it was a car
le
pp
in natural speech prosodic features are significantly influenced by
whether the utterance is generated spontaneously or read aloud
the variations in spontaneous speech are enormous
there are all sorts of emotions which are plainly audible in
everyday speech  sarcasm excitement rudeness disagreement
sadness fright love
variations in voice quality certainly play a part here
even with ordinary cooperative friendly conversation the need to find
words and somehow fit them into an overall utterance produces great
diversity of prosodic structures
applications for speech output from computers do not however call for
spontaneous conversation but for a controlled delivery which is
like that when reading aloud
here the speaker is articulating utterances which have been set out for
him reducing his cognitive load to one of understanding and interpreting
the text rather than generating it
unfortunately for us linguists are quite rightly
primarily interested in living
spontaneous speech rather than preprepared readings
pp
nevertheless the richness of prosody in speech even when reading from
a book should not be underestimated
read aloud to an audience and listen to the contrasts in voice dynamics
deliberately introduced for varietys sake
if stories are to be read there is even a case for controlling voice
ul
quality
to cope with quotations and affective imitations
pp
we saw earlier that the sourcefilter model is particularly
helpful in distinguishing prosodic features which are largely
properties of the source from segmental ones which belong to
the filter
pitch and amplitude are primarily source properties
rhythm and speed of speaking are not but neither are they filter
properties for they belong to the sourcefilter system as a whole
and not specifically to either part of it
the difficult notion of stress is from an acoustic point of view
a combination of pitch rhythm and amplitude
even some features of voice quality can be attributed to the source
like laryngitis although others em cleft palate badlyfitting
dentures em affect segmental features as well
sh   further reading
pp
this chapter has been no more than a cursory introduction to some
of the difficult problems of linguistics and phonetics
here are some readable books which discuss these problems further
lb nn
abercrombie
ds f 

ds a abercrombie d
ds d 
ds t elements of general phonetics
ds i edinburgh univ press
nr t 
nr a 
nr o 
  book
inn
this is an excellent book which covers all of the areas of this
chapter in much more detail than has been possible here
inn
brown
ds f 

ds a brown gill
as a  currie kl
as a  and kenworthy j
ds d 
ds t questions of intonation
ds i croom helm
ds c london
nr t 
nr a 
nr o 
  book
inn
an intensive study of the prosodics of colloquial living speech
is presented with particular reference to intonation  although
not particularly relevant to speech output from computers
this book gives great insight into how conversational speech
differs from reading aloud
inn
fry
ds f 

ds a fry db
ds d 
ds t the physics of speech
ds i cambridge university press
ds c cambridge england
nr t 
nr a 
nr o 
  book
inn
this is a simple and readable account of speech science with a good
and completely nonmathematical introduction to frequency analysis
inn
ladefoged
ds f 

ds a ladefoged p
ds d 
ds t a course in phonetics
ds i harcourt brace and johanovich
ds c new york
nr t 
nr a 
nr o 
  book
inn
usually books entitled a course on  are dreadfully dull but
this is a wonderful exception  an exciting readable almost racy
introduction to phonetics full of little experiments you can try
yourself
inn
lehiste
ds f 

ds a lehiste i
ds d 
ds t suprasegmentals
ds i mit press
ds c cambridge massachusetts
nr t 
nr a 
nr o 
  book
inn
this fairly comprehensive study of the prosodics of speech
complements ladefogeds book which is mainly concerned with segmental
phonetics
inn
oconnor
ds f 

ds a oconnor jd
ds d 
ds t phonetics
ds i penguin
ds c london
nr t 
nr a 
nr o 
  book
inn
this is another introductory book on phonetics
it is packed with information on all aspects of the subject
inn
le nn
eq
delim 
en
ch   speech storage
ds rt speech storage
ds cx principles of computer speech
pp
the most familiar device that produces speech output is the ordinary tape
recorder which stores information in analogue form on magnetic tape
however this is unsuitable for speech output from computers
one reason is that it is difficult to access different utterances quickly
although randomaccess tape recorders do exist they are expensive and
subject to mechanical breakdown because of the stresses associated with
frequent starting and stopping
pp
storing speech on a rotating drum instead of
tape offers the possibility of access to any track within one revolution time
for example the ibm  audio response unit employs drums rotating twice
a second which are able to store up to  msec words  these can be accessed
randomly within half a second at most
although one can
arrange to store longer words by allowing overflow on to an adjacent track at
the end of the rotation period the discrete timeslots provided by this
system make it virtually impossible for it to generate connected utterances
by assembling appropriate words from the store
pp
the cognitronics speechmaker has a similar structure but with
the analogue speech waveform recorded on photographic film
storing audio waveforms optically is not an unusual technique for this is how
soundtracks are recorded on ordinary movie films  the original version of
the speaking clock of the british post office used optical storage in
concentric tracks on flat glass discs
it is described by speight and gill 
who include a fascinating account of how the utterances are synchronized

speight gill 

a  hz signal from a pendulum clock was used to supply current to an electric
motor which drove a shaft equipped with cams and gears that rotated
the glass discs containing utterances for seconds minutes and hours
at appropriate speeds
pp
a second reason for avoiding analogue storage is price  it is difficult to see how a randomaccess
tape recorder could be incorporated into a talking pocket calculator or
childs toy without considerably inflating the cost
solidstate electronics is much cheaper than mechanics
pp
but the best reason is that in many of the applications we have discussed
it is necessary to form utterances by concatenating separatelyrecorded
parts  it is totally infeasible for example to store each and every
possible telephone number as an individual recording  and
utterances that are formed by concatenating individual words which were
recorded in isolation or in a different context do not sound completely
natural  for example in an early experiment stowe and hampton  recorded
individual words on acoustic tape spliced the tape with the words in a different
order to make sentences and played the result to subjects who were scored on
the number of key words which they identified correctly

stowe hampton 

the overall conclusion was that while embedding a word in normallyspoken sentences
ul
increases
the probability of recognition because the extra context gives clues about the
word embedding a word in a constructed sentence where intonation and rhythm
are not properly rendered
ul
decreases
the probability of recognition  when the speech was uttered slowly
however a considerable improvement was noticed indicating that if the
listener has more processing time he can overcome the lack of proper intonation
and rhythm
pp
nevertheless many presentday voice response systems
ul
do
store what amounts to a direct recording of the acoustic wave
however the storage medium is digital rather than analogue
this means that standard computer storage devices can be used providing
rapid access to any segment of the speech at relatively low cost em for
the economics of massproduction ensures a low price for randomaccess
digital devices compared with randomaccess analogue ones
furthermore it reduces the amount of special equipment needed for speech
output  one can buy very cheap speech inputoutput interfaces for home computers
which connect to standard hobby buses
another advantage of digital over analogue recording is that
integrated circuit readonly memories roms
can be used for handheld devices which need small quantities of speech
hence this chapter begins by showing how waveforms are stored digitally
and then describes some techniques for reducing the data needed for a given
utterance
sh   storing waveforms digitally
pp
when an analogue signal is converted to digital form it is made discrete
both in time and in amplitude  discretization in time is the operation of
ul
sampling
whilst in amplitude it is
ul
quantizing
it is worth pointing out that the transmission of analogue information by
digital means is called pcm standing for pulse code modulation in
telecommunications jargon
much of the theory of digital signal processing investigates signals which
are sampled but not quantized or quantized into sufficiently many levels to
avoid inaccuracies  the operation of quantization being nonlinear
is not very amenable to theoretical analysis  quantization introduces issues
such as accumulation of roundoff noise in arithmetic operations
which although they are very important in practical implementations can only
be treated theoretically under certain somewhat unrealistic assumptions
in particular independence of the quantization error from sample to sample
rh sampling
a fundamental theorem of telecommunications states that a signal can only be
reconstructed accurately from a sampled version if it does not contain
components whose frequency is greater than half the frequency at which the
sampling takes place  figure a shows how a component of slightly greater
than half the sampling frequency can masquerade as far as an observer with
access only to the sampled data can tell as a component at slightly less
than half the sampling frequency
fc figure 
call the sampling interval t seconds so that the
sampling frequency is t hz
then components at tf tf tf and so on all masquerade
as a component at tf  similarly components at frequencies just under
the sampling frequency masquerade as very lowfrequency components as shown
in figure b  this phenomenon is often called aliasing
pp
thus the continuous infinite frequency axis for the unsampled signal where
two components at different frequencies can always be distinguished maps
into a repetitive frequency axis when the signal is sampled  as depicted
in figure  the frequency
interval t t udgd
fn 
sp
udgdintervals are specified in brackets with a square bracket representing
a closed end of the interval and a round one representing an open one
thus the interval t t specifies the range t    frequency
   t
ef
is mapped back into the band  t as are the
intervals t t  t t and so on
fc figure 
furthermore the interval t t between half the sampling frequency and the sampling
frequency is mapped back into the interval
below half the sampling frequency but this time the mapping is backwards
with frequencies at just under t being mapped to frequencies slightly greater
than zero and frequencies just over t being mapped to ones
just under t
the best way to represent a repeating frequency axis like this is as a circle
figure  shows how the linear frequency axis for continuous systems maps
on to a circular axis for sampled systems
fc figure 
for present purposes it is
easiest to imagine the bottom half of the circle as being reflected into
the top half so that traversing the upper semicircle in the anticlockwise direction
corresponds to frequencies increasing from  to t half the sample frequency
and returning along the lower semicircle is actually the same as coming
back round the upper one and corresponds to frequencies from t to t
being mapped into the range t to 
pp
as far as speech is concerned then we must ensure that before sampling a
signal no significant components at greater than half the sample frequency
are present  furthermore the sampled signal will only contain information
about frequency components less than this so the sample frequency must be
chosen as twice the highest frequency of interest
for example consider telephonequality speech
telephones provide a familiar standard of speech quality which
although it can only be an approximate standard
will be much used throughout this book
the telephone network
aims to transmit only frequencies lower than  khz  we saw in the
previous chapter that this region will contain the informationbearing formants
and some em but not all em of the fricative and aspiration energy
actually transmitting speech through the telephone system degrades its
quality very significantly probably more than you realize since everyone is
so accustomed to telephone speech  try the dialadisc service and compare
it with highfidelity music for a striking example of the kind of degradation
suffered
pp
for telephone speech the sampling frequency must be chosen to be
at least  khz
since speech contains significant amounts of energy above  khz it should be
filtered before sampling to remove this otherwise the higher components
would be mapped back into the baseband and distort the lowfrequency information
because it is difficult to make filters that cut off very sharply the
sampling frequency is chosen rather greater than twice the highest frequency of
interest  for example the digital telephone network samples at  khz
the presampling filter should have a cutoff frequency of  khz aim for
negligible distortion below  khz and transmit negligible components
above  khz em for these are reflected back into the band of interest
namely  to  khz  figure  shows a block diagram for the input hardware
fc figure 
rh quantization
before considering specifications for the presampling filter let us turn
from discretization in time to discretization in amplitude that is
quantization
this is performed by an ad converter analoguetodigital which takes as input
a constant analogue voltage produced by the sampler and generates a
corresponding binary value as output  the simplest correspondence is
ul
uniform
quantization where the amplitude range is split into equal regions by points
termed quantization levels and the output is a binary representation of
the nearest quantization level to the input voltage
typically bit conversion is used for speech giving  quantization
levels and the signal is adjusted to have zero mean so that half the
levels correspond to negative input voltages and the other half to positive
ones
pp
it is at first sight surprising that as many as  bits are needed for
adequate representation of speech signals  research on the digital telephone
network for example has concluded that a signaltonoise ratio of
some  db is enough to avoid undue harshness of quality loss
of intelligibility and listener fatigue for speech at a comfortable
level in an otherwise reasonably good channel
rabiner and schafer  suggest that about  db signaltonoise ratio
would most likely provide adequate quality in a communications system

rabiner schafer  digital processing of speech signals

but bit quantization seems to give a very much better signaltonoise
ratio than these figures  to estimate its magnitude note that for nbit quantization
the error for each sample will lie between
lb

   over    sup n    and       over    sup n 

le
assuming that it is uniformly distributed in this range em an assumption
which is likely to be justified if the number of levels is sufficiently
large em leads to a meansquared error of
lb
eq
integral from  sup n to  sup n e sup  pe de
en
le
where pe the probability density function of the error e is a constant
which satisfies the usual probability normalization constraint namely
lb
eq
integral from  sup n to  sup n  pe de  
en
le
hence pe sup n  and so the meansquared error is   sup n 
this is    log sub   sup n  db or around  db for bit
quantization
pp
this noise level is relative to the maximum amplitude range of the conversion
a maximumamplitude sine wave has a power of  db relative to the same
reference giving a signaltonoise ratio of some  db  this is far in excess
of that needed for telephonequality speech  however look at the very peaky
nature of the typical speech waveform given in figure 
fc figure 
if clipping is to be avoided the maximum amplitude level of the ad converter
must be set at a value which makes the power of the speech signal very much
less than a maximumamplitude sine wave  furthermore different people
speak at very different volumes and the overall level fluctuates constantly
with just one speaker  experience shows that while  or bit quantization
may provide sufficient signaltonoise ratio to preserve telephonequality
speech if the overall speaker levels are carefully controlled about  bits
are generally required to provide highquality representation of speech with
a uniform quantization  with  bits a sine wave whose amplitude is only 
of the fullscale value would be digitized with a signaltonoise ratio
of around  db the most pessimistic figure quoted above for adequate quality
even then it is useful if the speaker is provided
with an indication of the amplitude of his speech  a trafficlight
indicator with red signifying clipping overload orange a suitable level
and green too low a value is often convenient for this
rh logarithmic quantization
for the purposes of speech
ul
processing
it is essential to have the signal quantized uniformly  this is because
all of the theory applies to linear systems and nonlinearities introduce
complexities which are not amenable to analysis
uniform quantization although a nonlinear operation is linear in the
limiting case as the number of levels becomes large and for most purposes
its effect can be modelled by assuming that the quantized signal is obtained
from the original analogue one by the addition of a small amount of
uniformlydistributed quantizing noise as in fact was done above
usually the quantization noise is disregarded in subsequent analysis
pp
however the peakiness of the speech signal illustrated in figure  leads
one to suspect that a nonlinear representation for example a logarithmic one
could provide a better signaltonoise ratio over a wider range of input
amplitudes and hence be more useful than linear quantization em at least
for speech storage and transmission
and indeed this is the case  linear quantization has the unfortunate effect
that the absolute noise level is independent of the signal level so that an excessive
number of bits must be used if a reasonable ratio is to be achieved for peaky
signals  it can be shown that a logarithmic representation like
lb
eq
y        k  log  x
en
le
where x is the original signal and y is the value which is to be quantized
gives a
signaltonoise
ul
ratio
which is independent of the input signal level
this relationship cannot be realized physically for it is undefined when the signal
is negative and diverges when it is zero
however realizable approximations to it can be made which retain the advantages
of constant signaltonoise ratio within a useful range of signal amplitudes
figure  shows the logarithmic relation with one widelyused approximation to it
called the alaw
fc figure 
the idea of nonlinearly quantizing a signal to achieve adequate signaltonoise
ratios for a wide variety of amplitudes is called companding a contraction
of compressingexpanding  the original signal can be retrieved from
its alaw compression by antilogarithmic expansion
pp
figure  also
shows one common coding scheme which is a piecewise linear approximation
to the alaw  this provides an bit code and gives the equivalent
of bit linear quantization for small signal levels  it approximates
the alaw in  linear segments  for positive and  for negative
inputs
consider the positive part of the curve  the first two segments which
are actually collinear correspond exactly to bit linear conversion
thus the output codes  to  correspond to inputs from  to 
in equal steps  remember that both positive and negative signals
must be converted so a bit linear converter will allocate  levels
for positive signals and  for negative ones  the next
segment provides bit linear quantization
output codes  to  corresponding to inputs from  to 
similarly the next segment corresponds to bit quantization covering
inputs from  to   and so on the last section giving bit
quantization of inputs from  to  the fullscale positive value
negative inputs are converted similarly
for signal levels of less than  that is  sup  this implementation
of the alaw provides full bit precision
as the signal level increases the precision decreases gradually to  bits
at maximum amplitudes
pp
logarithmic encoding provides what is in effect a floatingpoint representation
of the input  the conventional floatingpoint format however is not used
because many different codes can represent the same value  for example with
a bit exponent preceding a bit mantissa the words 
  and  represent the numbers
  times   sup     times   sup 
    times   sup   c
and    times   sup   respectively
which are the same  some floatingpoint conventions assume that an unwritten
 bit precedes the mantissa except when the whole word is zero but this
gives decreased resolution around zero em which is exactly where we want the
resolution to be greatest  table  shows the bit alaw codes
rf
ini
ta i wbits    u
bit codewordbit sign bit
bits bit exponent
bits bit mantissa
sp
ta i i
ul
 codeword   interpretation
sp
 hw    u  times   sup 

 hw    u  times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 
  sup       times   sup 

  sup       times   sup 

 hw  u    times   sup negative numbers treated as
above with a sign bit of 
 hw u  sup       times   sup 
ta i i i i i i i i i i i i
in 
fg table   bit alaw codes with their floatingpoint equivalents
according
to the piecewise linear approximation of figure  written in a notation which
suggests floating point  each linear segment has a different exponent except
the first two segments which as explained above are collinear
pp
logarithmic encoders and decoders are available from many semiconductor
manufacturers as singlechip devices
called codecs for coderdecoder  intended for use on digital communication
links these generally provide a serial output bitstream which
should be converted to parallel by a shift register if the data is intended
for a computer
because of the potentially vast market for codecs in telecommunications
they are made in great quantities and are consequently very cheap
estimates of the speech quality necessary for telephone applications indicate
that somewhat less than this accuracy is needed em bit logarithmic encoding
was used in early digital communications links and it may be that even  bits
are adequate  however during the transition period when digital
networks must coexist with the present analogue one it is anticipated that
a particular telephone call may have to pass through several links some
using analogue technology and some being digital  the possibility of
several successive encodings and decodings has led telecommunications
engineers to standardize on bit representations leaving some margin
before additional degradation of signal quality becomes unduly distracting
pp
unfortunately world telecommunications authorities cannot agree on a single
standard for logarithmic encoding  the alaw which we have described
is the european standard but there is another system called
the mulaw which is used universally in north america  it also is available
in singlechip form with an bit code  it has very similar
quantization error characteristics to the alaw and would be indistinguishable
from it on the scale of figure 
rh the presampling filter
now that we have some idea of the accuracy requirements for quantization
let us discuss quantitative specifications for the presampling filter
figure  sketches the characteristics of this filter
fc figure 
assume a
sampling frequency of  khz and a range of interest from  to  khz
although all components at frequencies above  khz will fold back into
the    khz baseband those below  khz fold back above  khz and are
therefore outside the range of interest  this gives a guard band between
 and  khz which separates the passband from the stopband  the filter
should transmit negligible components in the stopband above  khz
to reduce the harmonic distortion caused by aliasing to the same level
as the quantization noise in bit linear conversion the stopband
attenuation should be around  db the signaltonoise ratio for a fullscale
sine wave  passband ripple is not so critical
for two reasons  whilst the presence of aliased components means that
information has been lost about the frequency components within the range of
interest passband ripple does not actually cause a loss of information but
only a distortion and could if necessary be compensated by a suitable
filter acting on the digitized waveform  secondly distortion of the
passband spectrum is not nearly so audible as the frequency images caused
by aliasing  hence one usually aims for a passband ripple of around  db
pp
the pass and stopband targets we have mentioned above can be achieved with
a th order elliptic filter  while such a filter is often used in
highquality signalprocessing systems for telephonequality speech
much less stringent specifications seem to be sufficient  figure  for
example shows a template which has been recommended by telecommunications
authorities
fc figure 
a th order elliptic filter can easily meet this specification
such filters implemented by switchedcapacitor means are available in
singlechip form  integrated ccd chargecoupled device
filters which meet the same specification
are also marketed  indeed some codecs provide input filtering on the same
chip as the ad converter
pp
instead of implementing a filter by analogue means to meet the aliasing
specifications digital filtering can be used  a high samplerate ad
converter operating at say  khz and preceded by a very simple lowpass
presampling filter is followed by a digital filter which meets the
desired specification and its output is subsampled to provide an  khz sample
rate  while such implementations may be economic where a multichannel digitizing
capability is required as in local telephone exchanges where the subscriber
connection is an analogue one they are unlikely to prove costeffective for
a single channel
rh reconstructing the analogue waveform
having digitized and stored a signal it needs to be passed though a da
converter digitaltoanalogue and lowpass filter when replayed
da converters are cheaper than ad converters and the characteristics of the
lowpass filter for output can be the same as those for input
however the desampling operation introduces an additional distortion which
has an effect on the component at frequency f of
lb
eq
 sin  pi ff sub s  over  pi ff sub s   
en
le
where f sub s is the sampling frequency  an aperture correction filter is
needed to compensate for this although many systems simply do without it
such a filter is sometimes incorporated into the codec chip
rh summary
for telephonequality speech existing codec chips
coupled if necessary with integrated presampling filters can
be used at a remarkably low cost
for higherquality speech storage the analogue interface can become quite complex
a comprehensive study of the problems as they relate to digitization of audio
which demands much greater fidelity than speech has been made by blesser 

blesser 

he notes the following sources of error amongst others
lb
np
slewrate distortion in the presampling filter for signals at the upper end
of the audio band
np
insufficient filtering of highfrequency input signals
np
noise generated by the sampleandhold amplifier or presampling filter
np
acquisition errors because of the finite settling time of the sampleandhold
circuit
np
insufficient settling time in the ad conversion
np
errors in the quantization levels of the ad and da converters
np
noise in the converters
np
jitter on the clock used for timing input or output samples
np
aperture distortion in the output sampler
np
noise in the output filter as a result of limited dynamic range of the
integrated circuits
np
powersupply noise injection or ground coupling
np
changes in characteristics as a result of temperature or ageing
le
care must be taken with the analogue interface to ensure that the precision
implied by the resolution of the ad and da converters is not compromised
by inadequate analogue circuitry  it is especially important to eliminate
highfrequency noise caused by fast edges on nearby computer buses
sh   coding in the time domain
pp
there are several methods of coding the time waveform of a speech signal to
reduce the data rate for a given signaltonoise ratio or alternatively to
reduce the signaltonoise ratio for a given data rate  they almost all require
more processing both at the encoding for storage and decoding for
regeneration ends of the digitization process  they are sometimes used to
economize on memory in systems using stored speech
for example the system x telephone exchange and the travel consultant described
in chapter  and so will be described here  however it is to be expected
that simple timedomain coding techniques will be superseded by the more complex
linear predictive method which is covered in chapter  because this
can give a much more substantial reduction in the data rate for only a small
degradation in speech quality  hence the aim of this section is to introduce
the ideas in a qualitative way  theoretical development and summaries of
results of listening tests can be found elsewhere eg rabiner and schafer 

rabiner schafer  digital processing of speech signals

the methods we will examine are summarized in table 
rf
nr x wlinear pcm      u
nr x nxw    adaptive quantization or adaptive predictionu
nr x nlnx
in nxu
ta nxu
lnxuul
sp
linear pcmlinearlyquantized pulse code modulation
sp
log pcmlogarithmicallyquantized pulse code modulation
    instantaneous companding
sp
apcmadaptively quantized pulse code modulation
    usually syllabic companding
sp
dpcmdifferential pulse code modulation
sp
adpcmdifferential pulse code modulation with either
    adaptive quantization or adaptive prediction
    or both
sp
dmdelta modulation bit dpcm
sp
admdelta modulation with adaptive quantization
lnxuul
ta i i i i i i i i i i i i
in 
fg table   timedomain encoding techniques
rh syllabic companding
we have already studied one timedomain encoding technique namely logarithmic
quantization or log pcm sometimes called instantaneous companding  a more
sophisticated encoder could track slowly varying trends in the overall amplitude
of the speech signal and use this information to adjust the quantization
levels dynamically  speech coding methods based on this principle are called
adaptive pulse code modulation systems apcm  because the overall amplitude
changes slowly it is sufficient to adjust the quantization relatively infrequently
compared with the sampling rate and this is often done at rates approximating
the syllable rate of running speech leading to the term syllabic companding
a block floatingpoint format can be used with a common exponent being
stored every m samples with m say  for a  msec block rate at  khz
sampling but the mantissa being stored at the regular sample rate  the overall
energy in the block
lb
sum from nh to hm xn sup     m   say
le
is used to determine a suitable exponent and every sample
in the block em namely
xh xh  xhm em is scaled according to that exponent
note that for speech transmission systems this method necessitates a delay of
m samples at the encoder and indeed some methods base the exponent on the
energy in the last block to avoid this  for speech storage however the delay
is irrelevant  a rather different nonsyllabic method of adaptive pcm is
continually to change the step size of a uniform quantizer by multiplying it by
a constant at each sample which is based on the magnitude of the previous code
word
pp
adaptive quantization exploits information about the amplitude of the signal
and as a rough generalization yields a reduction of one bit per sample
in the data rate for telephonequality speech over ordinary logarithmic
quantization for a given signaltonoise ratio  alternatively for the
same data rate an improvement of  db in signaltonoise ratio can be obtained
some results for actual schemes are given by rabiner and schafer 

rabiner schafer  digital processing of speech signals

however there is other information in the time waveform of speech namely the
sampletosample correlation which can be exploited to give further reductions
rh differential coding
differential pulse code modulation dpcm in its simplest form uses the
present speech sample as a prediction of the next one
and stores the prediction error em that is the sampletosample difference
this is a simple case of predictive encoding
referring back to the speech waveform displayed in figure 
it seems plausible that the data rate can be reduced by transmitting the difference
between successive samples instead of their absolute values  less bits are
required for the difference signal for a given overall accuracy because it
does not assume such extreme values as the absolute signal level
actually the improvement is not all that great em about    db in
signaltonoise ratio or just under one bit per sample for a given
signaltonoise ratio em for the difference signal can be nearly as large as
the absolute signal level
pp
if dpcm is used in conjunction with adaptive quantization giving one form of
adaptive differential pulse code modulation adpcm both the overall amplitude
variation and the sampletosample correlation are exploited leading to a
combined gain of    db in signaltonoise ratio or just under two bits
reduction per sample for telephonequality speech  another form of adaptation
is to alter the predictor by multiplying the previous sample value by a
parameter which is adjusted for best performance
then the transmitted signal at time n is
lb
eq
en    xn  axn
en
le
where the parameter a is adapted and stored on a syllabic timescale  this
leads to a slight improvement in signaltonoise ratio which can be combined
with that achieved by adaptive quantization  much more substantial benefits
can be realized by using a weighted sum of the past several up to  speech
samples and adapting all the weights  this is the basic idea of linear
prediction which is developed in chapter 
rh delta modulation
the coding methods presented so far all increase the complexity of the
analoguetodigital interface or if the sampled waveform is coded
digitally they increase the processing required before and after storage
one method which considerably
ul
simplifies
the interface is the limiting case
of dpcm with just bit quantization  only the sign of the difference between
the current and last values is transmitted  figure  shows the conversion
hardware
fc figure 
the encoding part is essentially the same as a tracking da
where the value in a counter is forced to track the analogue input by
incrementing or decrementing the counter according as the input exceeds or
falls short of the analogue equivalent of the counters contents  however
for this encoding scheme called delta modulation the incrementdecrement
signal itself forms the discrete representation of the waveform instead of the counters
contents  the analogue waveform can be reconstituted from the bit stream with
another counter and da converter  alternatively an allanalogue implementation
can be used both for the encoder and decoder with a capacitor as integrator
whose charging current is controlled digitally  this is a much cheaper realization
pp
it is fairly obvious that the sampling frequency for delta modulation will need
to be considerably higher than for straightforward pcm  figure  shows
an effect called slope overload which occurs when the sampling rate is too low
fc figure 
either a higher sample rate or a larger step size will reduce the overload
however larger steps increase the noise level of the alternate s and s
that occur when no input is present em called granular noise  a compromise
is necessary between slope overload and granular noise for a given bit rate
delta modulation results in lower data rates than logarithmic quantization
for a given signaltonoise ratio if that ratio is low poorquality speech
as the desired speech quality is increased its data rate grows faster than
that of logarithmic pcm  the crossover point occurs at much lower than
telephone quality speech and so although delta modulation is used for some
applications where the permissible data rate is severely constrained
it is not really suitable for speech output from computers
pp
it is profitable to adjust the step size leading to
ul
adaptive
delta modulation
a common strategy is to increase or decrease the step size by a multiplicative
constant which depends on whether the new transmitted bit will be equal to
or different from the last one  that is
lb nnnn
ni nn
stepsizen    stepsizen times   if xnxnxn
or xnxnxn
br
slope overload condition
ni nn
stepsizen  stepsizen  if xnxnxn
or xnxnxn
br
granular noise condition
le nnnn
despite these adaptive equations the step size should be constrained to
lie between a predetermined fixed maximum and minimum to prevent it from
becoming so large or so small that rapid accomodation to changing input signals is
impossible
then in a period of potential slope overload the step size will grow preventing
overload possibly to its maximum value when overload may resume  in a quiet
period it will decrease to its minimum value which determines the granular
noise in the idle condition  note that the step size need not be stored for
it can be deduced from the bit changes in the digitized data  although
adaptation improves the performance of delta modulation it is still inferior to
pcm at telephone qualities
rh summary
it seems that adpcm with
adaptive quantization and adaptive prediction can provide a worthwhile
advantage for speech storage reducing the number of bits needed per sample of
telephonequality speech from  for logarithmic pcm to perhaps  and the data
rate from  kbits to  kbits  disadvantages are additional complexity
in the encoding and decoding processes and the fact that byteoriented storage
with  bitssample in logarithmic pcm is more convenient for computer use
for low quality speech where hardware complexity is to be minimized
adaptive delta modulation could provide worthwhile em although the ready
availability of pcm codec chips reduces the cost advantage
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
probably the best single reference on timedomain coding of speech is
the book by rabiner and schafer  cited above
however this does not contain a great deal of information on practical
aspects of the analoguetodigital conversion process this is
covered by blesser  above who is especially interested in
highquality conversion for digital audio applications
and garrett  below
there are many textbooks in the telecommunications area which
are relevant to the subject of the chapter
although they concentrate primarily on fundamental theoretical aspects rather
than the practical application of the technology
lb nn
cattermole

ds a cattermole kw
ds d 
ds t principles of pulse code modulation
ds i iliffe
ds c london
nr t 
nr a 
nr o 
  book
inn
this is a standard definitive work on pcm and provides a good grounding
in the theory
it goes into the subject in much more depth than we have been able to here
inn
garrett

ds a garrett ph
ds d 
ds t analog systems for microprocessors and minicomputers
ds i reston publishing company
ds c reston virginia
nr t 
nr a 
nr o 
  book
inn
garrett discusses the technology of data conversion systems including
ad and da converters and basic analogue filter design in a
clear and practical manner
inn
inose

ds a inose h
ds d 
ds t an introduction to digital integrated communications systems
ds i peter peregrinus
ds c stevenage england
nr t 
nr a 
nr o 
  book
inn
inoses book is a recent one which covers the whole area of digital
transmission and switching technology
it gives a good idea of what is happening to the telephone networks
in the era of digital communications
inn
steele

ds a steele r
ds d 
ds t delta modulation systems
ds i pentech press
ds c london
nr t 
nr a 
nr o 
  book
inn
again a standard work this time on delta modulation techniques
steele gives an excellent and exhaustive treatment of the subject from a
communications viewpoint
inn
le nn
eq
delim 
en
ch   speech analysis
ds rt speech analysis
ds cx principles of computer speech
pp
digital recordings of speech provide a jumpingoff point for
further processing of the audio waveform which is usually necessary for
the purpose of speech output
it is difficult to synthesize natural sounds by concatenating
individuallyspoken words
pitch is perhaps the most perceptually significant contextual effect
which must be
taken into account when forming connected speech out of isolated words
the intonation of an utterance which manifests itself as a
continually changing pitch is a holistic property of the utterance
and not the sum of components determined by the individual words alone
happily and quite coincidentally communications engineers in their quest
for reducedbandwidth telephony have invented methods of coding speech that
separate the pitch information from that carried by the articulation
pp
although these analysis techniques which were first introduced in the late
s dudley  were originally implemented by analogue means em and
in many systems still are blankenship  describes a recent
switchedcapacitor realization em there is a continuing trend
towards digital implementations particularly for the more sophisticated coding
schemes

dudley 


blankenship 

it is hard to see how the technique of linear prediction of speech
which is described in detail in chapter  could be accomplished in the
absence of digital processing
some groundwork is laid for the theory of digital signal analysis in this
chapter
the ideas are not presented in a formal axiomatic way but are developed as
and when they are needed to examine some of the structures that turn out to be
useful in speech processing
pp
most speech analysis views speech according to the sourcefilter model which
was introduced in chapter  and aims to separate the effects of the source from
those of the filter  the frequency spectrum of the vocal tract filter is of
great interest and the technique of discrete fourier transformation is
discussed in this chapter  for many purposes it is better to extract the formant
frequencies from the spectrum and use these alone or in conjunction with their
bandwidths to characterize it  as far as the signal source in the sourcefilter
model is concerned its most interesting features are pitch and amplitude em the
latter being easy to estimate  hence we go on to look at pitch extraction
related to this is the problem of deciding whether a segment of speech has
voiced or unvoiced excitation or both
pp
estimating formant and pitch parameters is one of the messiest areas of
speech processing  there is a delightful paper which points this out
schroeder  entitled parameter estimation in speech a lesson in unorthodoxy

schroeder 

it emphasizes that the most successful estimation procedures have often relied
on intuition based on knowledge of speech signals and their production in the
human vocal apparatus rather than routine applications of wellestablished
theoretical methods
fortunately the emphasis of the present book is on speech
ul
output
which involves parameter estimation only in so far as it is needed to produce
coded speech for storage and to illuminate the acoustic nature of speech
for the development of synthesis by rule from phonetics or text
hence the many methods of formant and pitch estimation are treated rather
cursorily and qualitatively here  our main interest is in how to
ul
use
such information for speech output
pp
if the incoming speech can be analysed into its formant frequencies amplitude
excitation mode and pitch if voiced it is quite easy to resynthesize
it directly from these parameters  speech synthesizers are described in the
next chapter  they can be realized in either analogue or digital
hardware the former being predominant in production systems and the latter
in research systems em although as in other areas of electronics the balance
is changing in favour of digital implementations
sh   the channel vocoder
pp
a direct representation of the frequency spectrum of a signal can be obtained
by a bank of bandpass filters  this is the basis of
the
ul
channel vocoder
which was the first device that attempted to take advantage of the sourcefilter
model for speech coding dudley 

dudley 

the word vocoder is a contraction
of
ul
voc
ice
ul
coder
the energy in each filter band is
estimated by rectification and smoothing and the resulting approximation to
the frequency spectrum is transmitted or stored  the source properties are
represented by the type of excitation voiced or unvoiced and if voiced
the pitch  it is not necessary to include the overall amplitude of the speech
explicitly because this is conveyed by the energy levels from the separate
bandpass filters
pp
figure  shows the encoding part of a channel vocoder which has been used
successfully for many years holmes 

holmes  jsru channel vocoder

fc figure 
we will discuss the block labelled preemphasis shortly
the shape of the spectrum is estimated by  bandpass filters whose spacing
and bandwidth decrease slightly with decreasing frequency to obtain the rather
greater resolution that is needed in the lower frequency region
as shown in table 
rf
nr x niwuwbandwidth
nr x nlnx
in nxu
ta n i i
lnxuul
sp
nr x wchannel
nr x wcentre
nr x wanalysis
hnxuchannelhnxucentrehnxuanalysis
nr x wnumber
nr x wfrequency
nr x wbandwidth
hnxunumberhnxufrequencyhnxubandwidth
nr x whz
hnxuhzhnxuhz
lnxuul
sp



















lnxuul
ta i i i i i i i i i i i i
in 
fg table   filter specifications for a vocoder analyser after holmes 

holmes  jsru channel vocoder

the  db points
of adjacent filters are halfway between their centre frequencies so that there
is some overlap between bands
the filter characteristics do not need to have very sharp edges because the energy
in neighbouring bands is fairly highly correlated  indeed there is a
disadvantage in making them too sharp because the phase delays associated
with sharp cutoff filters induce smearing of the spectrum in the time domain
this particular channel vocoder uses secondorder butterworth bandpass filters
pp
for regenerating speech stored in this way an excitation of unit impulses
at the specified pitch period for voiced sounds or white noise for unvoiced
sounds is produced and passed through a bank of bandpass filters similar
to the analysis ones  the excitation has a flat spectrum for regular impulses
have harmonics at multiples of the repetition frequency which are all of the
same size and so the spectrum of the output signal is completely determined
by the filter bank  the gain of each filter is controlled by the stored
magnitude of the spectrum at that frequency
pp
the frequency spectrum and voicing pitch of speech change at much slower rates
than the time waveform  the changes are due to movements of the articulatory
organs tongue lips etc in the speaker and so are limited in their speed
by physical constraints  a typical rate of production of phonemes is  per
second but in fact the spectrum can change quite a lot within a single
phoneme especially a stop sound
between  and  msec  hz and  hz
is generally thought to be a satisfactory interval for transmitting or storing
the spectrum to preserve a reasonably faithful representation of the speech
of course the entire spectrum as well as the source characteristics must
be stored at this rate
the channel vocoder described by holmes  uses  bits to encode
the information

holmes  jsru channel vocoder

repeated every  msec this gives a data rate of  bits em very
considerably less than any of the timedomain encoding techniques
pp
it needs some care to encode the output of  filters the excitation type
and the pitch into  bits of information  holmes uses  bits for pitch
logarithmically encoded
and one bit for excitation type
this leaves  bits to encode the output of the  filters and so a differential
technique is used which transmits just the difference between adjacent
channels em for the spectrum does not change abruptly in the frequency domain
three bits are used for the absolute level in channel  and two bits
for each channeltochannel difference giving a total of  bits for the whole
spectrum  the remaining two bits per frame are reserved for signalling or
monitoring purposes
pp
a  bits channel vocoder degrades the speech in a telephone channel quite
perceptibly  it is sufficient for interactive communication where
if you do not understand something you can always ask for it to be repeated
it is probably not good enough for most voice response applications
however the vocoder principle can be used with larger filter banks and much
higher bit rates and still reduce the data rate substantially below that
required by log pcm
sh   preemphasis
pp
there is an
overall  dboctave trend in speech radiated from the lips
as frequency increases
we will discuss why this is so in the next chapter
notice that this trend means that the signal power is reduced
by a factor of  or the signal amplitude by a factor of  for each
doubling in frequency
for vocoders and indeed for other methods of spectral analysis of speech
it is usually desirable to equalize this by a  dboctave lift prior to
processing so that the channel outputs occupy a similar range of levels
on regeneration the output speech is passed through an inverse filter which
provides  dboctave of attenuation
pp
for a digital system such preemphasis
can either be implemented as an analogue circuit which precedes the presampling
filter and digitizer or as a digital operation on the sampled and quantized
signal  in the former case the characteristic is usually flat up to a certain
breakpoint which occurs somewhere between  hz and  khz em the exact
position does not seem to be critical em at which point the  dboctave lift
begins  although deemphasis on output ought to have an exactly inverse
characteristic it is sometimes modified or even eliminated altogether in an
attempt to counteract approximately
the  sin pi ff sub s  pi ff sub s   distortion
introduced by the desampling operation which was discussed in an earlier
section  above half the sampling frequency the characteristic of the
preemphasis is irrelevant because any effect will be suppressed by the presampling
filter
pp
the effect of a  dboctave lift can also be achieved digitally by differencing
the input  the operation
lb
eq
yn   xn  axn
en
le
is suitable where the constant parameter a is usually chosen between  and 
the latter value gives straightforward differencing and this amounts to
creating a dpcm signal as input to the spectral analysis  figure  plots
the frequency response of this operation with a sample frequency of  khz
for two values of the parameter together with that of a  dboctave lift
above  hz
fc figure 
the vertical positions of the plots have been adjusted to give
the same gain  db at  khz
the difference at  khz the upper end of the telephone spectrum is just
over  db  at frequencies below the breakpoint in this case  hz the
difference between analogue and digital preemphasis can be very great  for
a the attenuation at dc zero frequency is  db below that at  khz
which happens to be close to that of the analogue filter for frequencies below the
breakpoint  however if the breakpoint had been at  khz there would have been
 db difference between the analogue and a plots at dc  and of course
the a characteristic has infinite attenuation at dc
in practice however the exact form of the preemphasis does not seem to be at all
critical
pp
the above remarks apply only to voiced speech  for unvoiced speech there appears
to be no real need for preemphasis indeed it may do harm by reinforcing
the already large highfrequency components  there is a case for altering the
parameter a according to the excitation mode of the speech  a for voiced
excitation and a for unvoiced gives preemphasis just when it is needed
this can be achieved by expressing the parameter in terms of the autocorrelation
of the incoming signal as
lb
eq
a    r over r  
en
le
where r is the correlation of the signal with itself delayed by one sample
and r is the correlation without delay that is the signal variance
this is reasonable intuitively because high sampletosample correlation
is to be expected in voiced speech so that r is very nearly as great as
r and the ratio becomes  whereas little or no sampletosample correlation
will be present in unvoiced speech making the ratio close to   such a
scheme is reminiscent of adpcm with adaptive prediction
pp
however this sophisticated preemphasis method does not seem to be worthwhile
in practice  usually the breakpoint in an analogue preemphasis filter is
chosen to be rather greater than  hz to limit the amplification of fricative
energy  in fact the channel vocoder described by holmes  has the
breakpoint at  khz limiting the gain to  db at  khz two octaves above

holmes  jsru channel vocoder

sh   digital signal analysis
pp
you may be wondering how the frequency response for the digital preemphasis
filters displayed in figure  can be calculated  suppose a digitized
sinusoid is applied as input to the filter
lb
eq
yn    xn  axn
en
le
a sine wave of frequency f has equation  xt    sin   pi ft and when
sampled at t t t  where t is the sampling interval  msec for
an  khz sample rate this becomes  xn    sin   pi fnt  it is much
more convenient to consider a complex exponential
input  e sup  j pi fnt  em the response to a sinusoid can then be derived
by taking imaginary parts if necessary  the output for this input is
lb
eq
yn    e sup j pi fnt ae sup j pi fnt   
ae sup j pi ft e sup j pi fnt 
en
le
a sinusoid at the same frequency as the input  the
factor  ae sup j pi ft  is complex with both amplitude and phase
components  thus the output will be a phaseshifted and amplified version
of the input  the amplitude response at frequency f is therefore
lb
eq
   ae sup j pi ft    
  a sup   acos pi ft  sup  
en
le
or
lb
eq
  log sub    a sup     a cos  pi ft
en
db
le
normalizing to  db at  khz and assuming  khz sampling yields
lb
eq
    log sub    a sup   a cos   pi f over  
   log sub    a sup   a cos  pi over  
en
db
le
with a and  this gives the graphs of figure 
pp
frequency responses for analogue filters are often plotted with a logarithmic
frequency scale as well as a logarithmic amplitude one to bring out the
asymptotes in dboctave as straight lines  for digital filters the response
is usually drawn on a
ul
linear
frequency axis extending to half the sampling frequency  the response is
symmetric about this point
pp
analyses like the above are usually expressed in terms of the ztransform
denote the unit delay operation by z sup   the choice of the inverse rather
than z itself is of course an arbitrary matter but the convention has stuck
then the filter can be characterized
by figure  which signifies that the output is the input minus a delayed
and scaled version of itself
fc figure 
the transfer function of the filter is
lb
eq
hz      az sup  
en
le
and we have seen that the effect of the system on a complex exponential of
frequency f is to multiply it by
lb
eq
  ae sup j pi ft
en
le
to get the frequency response from the transfer function replace z sup 
by e sup j pi ft  amplitude and phase responses can then be found by
taking the modulus and angle of the complex frequency response
pp
if z sup  is treated as an
ul
operator
it is quite in order to summarize the action of the filter by
lb
eq
yn    xn  az sup  xn      az sup  xn
en
le
however it is usual to derive from the sequence xn a
ul
transform
xz upon which z sup  acts as a
ul
multiplier
if the transform of xn is defined as
lb
eq
xz    sum from n infinity to infinity xn z sup n 
en
le
then on multiplication by z sup  we get a new transform say vz
lb
eq
vz    z sup  xz  
 z sup  sum from n infinity to infinity xn z sup n  
 sum xnz sup n  
 sum xnz sup n 
en
le
vz can also be expressed as the transform of a new sequence say vn by
lb
eq
vz    sum from n infinity to infinity vn z sup n 
en
le
from which it becomes apparent that
lb
eq
vn    xn
en
le
thus vn is a delayed version of xn and we have accomplished what we
set out to do namely to show that the delay
ul
operator
z sup  can be treated as an ordinary
ul
multiplier
in the ztransform domain where ztransforms are defined as the infinite
sums given above
pp
in terms of ztransforms the filter can be written
lb
eq
yz      az sup  xz
en
le
where z sup  is now treated as a multiplier
the transfer function of the filter is
lb
eq
hz    yz over xz      az sup  
en
le
the ratio of the output to the input transform
pp
it may seem that little has been gained by inventing this rather abstract
notion of transform simply to change an operator to a multiplier  after
all the equation of the filter is no simpler in the transform domain than
it was in the time domain using z sup  as an operator  however we will
need to go on to examine more complex filters  consider for example the
transfer function
lb
eq
hz    az sup  bz sup  over cz sup  dz sup   
en
le
if z sup  is treated as an operator it is not immediately obvious how
this transfer function can be realized by a timedomain recurrence relation
however with z sup  as an ordinary multiplier in the transform domain we can
make purely mechanical manipulations with infinite sums to see what the transfer
function means as a recurrence relation
pp
it is worth noting the similarity between the ztransform in the discrete
domain and the fourier and laplace transforms in the continuous domains
in fact the ztransform plays an analogous role in digital signal processing
to the laplace transform in continuous theory for the delay operator
z sup 
performs a similar service to the differentiation operator s
recall first the continuous fourier transform
lb

gf   
integral from  infinity to infinity gte sup j pi ft dt
    where f is real
le
and the laplace transform
lb

fs   
integral from  to infinity fte sup st dt
    where s is complex
le
the main difference between these two transforms is that the range of integration
begins at infinity for the fourier transform and at  for the laplace
advocates of the fourier transform which typically include people involved with
telecommunications enjoy the freedom from initial conditions which is bestowed
by an origin way back in the mists of time  advocates of laplace including
most analogue filter theorists invariably
consider systems where all is quiet before t em altering the origin
of measurement of time to achieve this if necessary em and welcome the opportunity
to include initial conditions explicitly
ul
without
having to worry about what happens in the mists of time
although there is a twosided laplace transform where the integration begins
at infinity it is not generally used because it causes some convergence
complications  ignoring this difference between the transforms by considering
signals which are zero when t the fourier spectrum can be found from the
laplace transform by writing  sj pi f that is by considering values
of s which lie on the imaginary axis
pp
the ztransform is
lb

hz    sum from n to infinity hnz sup n
    or    
hz    sum from n infinity to infinity hnz sup n 

le
depending on whether a onesided or twosided transform is used  the advantages
and disadvantages of one and twosided transforms are the same as in the
analogue case
z plays the role of e sup st  and so it is not surprising that the response
to a sampled sinusoid input can be found by setting
lb
eq
z    e sup j pi ft
en
le
in hz as we proved explicitly above for the preemphasis filter
pp
the above relation between z and f means that realvalued frequencies correspond
to points where z that is the unit circle in the complex zplane
as you travel anticlockwise around this unit circle starting from the
point z the corresponding frequency increases from  to t halfway
round z to t when you get back to the beginning z again
frequencies greater than the sampling frequency are aliased back into the
sampling band corresponding to further circuits of z with frequency
going from t to t t to t and so on  in fact this is the circle
of figure  which was used earlier to explain how sampling affects the frequency
spectrum
sh   discrete fourier transform
pp
let us return from this brief digression into techniques of digital signal
analysis to the problem of determining the frequency spectrum of speech
although a bank of bandpass filters such as is used in the channel vocoder
is the perhaps most straightforward way to obtain a frequency spectrum
there are other techniques which are in fact more commonly used in digital speech
processing
pp
it is possible to define the fourier transform of a discrete sequence of
points  to motivate the definition consider first the
ordinary fourier transform ft which is
lb

gt   
integral from  infinity to infinity gfe sup j pi ft df

gf   
integral from  infinity to infinity gte sup j pi ft dt 

le
this takes a continuous time domain into a continuous frequency domain
sometimes you see a normalizing factor  pi multiplying the integral in
either the forward or the reverse transform  this is only needed
when the frequency variable is expressed in radianss and we will find it
more convenient to express frequencies in hz
pp
the fourier series fs which should also be familiar to you
operates on a periodic time waveform or equivalently
one that only exists for a finite period of time which is notionally extended
periodically  if a period lies in the time range b then the transform is
lb

gt   
sum from r   infinity to infinity gre sup j pi rtb

gr     over b  integral from  to b gte sup j pi rtb dt 

le
the fourier series takes a periodic timedomain function into a discrete frequencydomain one
because of the basic duality between the time and frequency domains in the
fourier transforms it is not surprising that another version of the transform
can be defined which takes a periodic
ul
frequencyc
domain function into a
discrete
ul
timec
domain one
pp
fourier transforms can only deal with a finite stretch of a time signal
by assuming that the signal is periodic for if gt is evaluated from
its transform gr according to the formula above and t is chosen outside
the interval b then a periodic extension of the function gt is obtained
automatically
furthermore periodicity in one domain implies discreteness in the other
hence if we transform a
ul
finite
stretch of a
ul
discrete
time waveform
we get a frequencydomain representation which is also finite or equivalently
periodic and discrete
this is the discrete fourier transform dft
and takes a discrete periodic timedomain function into a discrete
periodic frequencydomain one as illustrated in figure 
fc figure 
it is defined by
lb

gn   
 over n  sum from r to ngre sup   j pi rnn

gr    sum from n to n gne sup   j pi rnn 

le
or writing  we sup j pi n
lb

gn   
 over n  sum from r to ngrw sup rn

gr    sum from n to n gnw sup rn 

le
sp
the n in the first equation is the same normalizing
factor as the b in the fourier series
for the finite time domain is n
in the discrete case and b in the fourier series case
it does not matter
whether it is written into the forward or the reverse transform but it is usually
placed as shown above as a matter of convention
pp
as illustrated by figure  discrete fourier transforms
take an input of n real values representing equallyspaced time samples
in the interval b and produce as output n complex values representing
equallyspaced frequency samples in the interval nb
fc figure 
note that the endpoint of this frequency interval is the sampling frequency
it seems odd that the input is real and the output is the same number of
ul
complex
quantities  we seem to be getting some numbers for nothing
however this isnt so for it is easy to show that if the input sequence is
real the output frequency
spectrum has a symmetry about its midpoint half the sampling frequency
this can be expressed as
lb
dft symmetry 
 mark g half n r  g half n r sup   if g is realvalued
le
where  denotes the conjugate of a complex quantity
that is ajb sup   ajb
pp
it was argued above that the frequency spectrum in the dft is periodic with
the spectrum from  to the sampling frequency being repeated regularly up and
down the frequency axis  it can easily be seen from the dft equation that
this is so  it can be written
lb
dft periodicity lineup gnr  gr  always
le
figure  illustrates the properties of symmetry and periodicity
fc figure 
sh   estimating the frequency spectrum of speech using the dft
pp
speech signals are not exactly periodic  although the waveform in a particular
pitch period will usually resemble those in the preceding and following pitch
periods it will certainly not be identical to them
as the articulation of the speech changes the formant positions will alter
as we saw in chapter  the pitch itself is certainly not constant
hence the fundamental assumption of the dft that the waveform is periodic
is not really justified  however the signal is quasiperiodic for changes
from period to period will not usually be very great  one way of computing
the shortterm frequency spectrum of speech is to use
ul
pitchsynchronous
fourier transformation where single pitch periods are isolated from the
waveform and processed with the dft  this gives a rather accurate estimate
of the spectrum  unfortunately it is difficult to determine the beginning
and end of each pitch cycle as we shall see later in this chapter when
discussing pitch extraction techniques
pp
if a finite stretch of a speech waveform is isolated and fourier transformed
without regard to pitch of the speech then the periodicity assumption will
be grossly violated  figure  illustrates that the effect is the same
as
multiplying the signal by a rectangular
ul
window function
which is  except during the period to be analysed where it is 
fc figure 
the windowed sequence will almost certainly have discontinuities at its edges
and these will affect the resulting spectrum  the effect can be analysed
quite easily but we will not do so here  it is enough to say that the
high frequencies associated with the edges of the window cause considerable
distortion of the spectrum  the effect can be alleviated by
using a smoother window than a rectangular one
and several have been investigated extensively  the commonlyused windows of
bartlett blackman and hamming are illustrated in figure 
fc figure 
pp
because the dft produces the same number of frequency samples equally spaced
as there were points in the time waveform there is a tradeoff between
frequency resolution and time resolution for a given sampling rate
for example a point transform with a sample rate of  khz gives the 
equallyspaced frequency components between  and  khz that are shown in table

rf
nr x wtime domain
nr x wfrequency domain
ini
ta i i i
hinnxutime domainhinnxufrequency domain
sp
sampletimesamplehnfrequency
numbernumber
nr x iw
lnxuullnxuul
sp
 musec hz




nr x w
hinnxuhinnxu
hinnxuhinnxu
hinnxuhinnxu
sp

 musec hz
lnxuullnxuul
ta i i i i i i i i i i i i
in 
mt 
table   time domain and frequency domain samples for a point dft
with  khz sampling
te
the top half of the frequency spectrum is of no interest because
it contains the complex conjugates of the bottom half in reverse order
corresponding to frequencies greater than half the sampling frequency
thus for a  hz resolution in the frequency domain
 time samples or a  msec stretch of speech needs to be transformed
a common technique is to take overlapping periods in the time domain to
give a new frequency spectrum every  msec  from the acoustic point
of view this is a reasonable rate to recompute the spectrum for as noted
above when discussing channel vocoders the rate of change in the spectrum
is limited by the speed that the speaker can move his vocal organs and
anything between  and  msec is a reasonable figure for transmitting
or storing the spectrum
pp
the dft is a complex transform and speech is a real signal  it is possible
to do two dfts at once by putting one time waveform into the real parts
of the input and another into the imaginary parts  this destroys the dft
symmetry property for it only holds for real inputs  but given the dft
of a complex sequence formed in this way it is easy to separate out the
dfts of the two real time sequences  if the two time sequences are
xn and yn then the transform of the complex sequence
lb
eq
gn    xn  jyn
en
le
is
lb
eq
gr    sum from n to n xnw sup rn  ynw sup rn  
en
le
it follows that the complex conjugate of the aliased parts of the spectrum
in the upper frequency region are
lb
eq
gnr sup     sum from n to n xnw sup nrn
 ynw sup nrn  
en
le
and this is the same as
lb
eq
gnr sup     sum from n to n xnw sup rn
 ynw sup rn  
en
le
because w sup n is  recall the definition of w
and so w sup nn is  for any n
thus
lb
eq
xr    gr  gnr sup   over 

yr    gr  gnr sup   over 
en
le
extracts the transforms xr and yr of the original sequences
x and y
pp
with speech this trick is frequently used to calculate two spectra at once
using point transforms a new estimate of the spectrum can be obtained
every  msec by taking overlapping  msec stretches of speech with a
computational requirement of one point transform every  msec
sh   the fast fourier transform
pp
straightforward calculation of the dft expressed as
lb
eq
gr    sum from n to n gnw sup nr 
en
le
for r    n takes n sup  operations where each operation
is a complex multiply and add for w is of course a complex number
there is a better way invented in the early sixties which reduces this to
n  log sub  n operations em a very considerable improvement
dubbed the fast fourier transform fft for historical reasons it would actually
be better called the fourier transform with the straightforward method above
known as the slow fourier transform  there
is no reason nowadays to use the slow method except for tiny transforms
it is worth describing the basic principle of the fft for it is surprisingly
simple  more details on actual implementations can be found in brigham 

brigham 

pp
it is important to realize that the fft involves no approximation
it is an
ul
exact
calculation of the values that would be obtained by the slow method
although it may be affected differently by roundoff errors
problems of aliasing and windowing occur in all discrete fourier transforms
and they are neither alleviated nor exacerbated by the fft
pp
to gain insight into the working of the fft imagine the sequence gn split
into two halves containing the even and odd points
respectively
lb
even half en is g g    gn
br
odd  half on is g g    gn
le
then it is easy to show that if g is the transform of g
e the transform of e
and o that of o then
lb

gr    er  w sup r or  for  r   half n 
le
and
lb

g half n r     er  w sup  half n r or  for  
r     half n 
le
calculation of the e and o transforms involves  half n sup  operations each
while combining them together according to the above relationship occupies
n operations  thus the total is  n  half n sup    operations which is considerably
less than n sup 
pp
but dont stop there  the even half can itself be broken down into
even and odd parts to expedite its calculation and the same with the odd half
the only constraint is that the number of elements in the sequences splits
exactly into two at each stage
providing n is a power of  then we are left at the end with some point
transforms to do  but transforming a single point leaves it unaffected  check
the definition of the dft  a quick calculation shows that the number of operations
needed is not  n  half n sup  but n log sub  n
figure  compares this with n sup  the number of operations for
straightforward dft calculation and it can be seen that the fft is very much
faster
fc figure 
pp
the only restriction on the use of the fft is that n must be a power of two
if it is not alternative more complicated algorithms can be used which
give comparable computational advantages  however for speech processing
the number of samples that are transformed is usually arranged to be a power
of two  if a pitch synchronous analysis is undertaken the
time stretch that is to be transformed is dictated by the length of the pitch
period and will vary from time to time  then it is usual to pad out the
time waveform with zeros to bring the number of samples up to a power of two
otherwise if differentlength time stretches were transformed the scale
of the resulting frequency components would vary too
pp
the fft provides very worthwhile cost savings over the use of a bank of
bandpass filters for spectral analysis  take the example of a point
transform with  khz sampling giving  frequency components spaced
by  hz from  up to almost  khz  this can be computed on overlapping
 msec stretches of the time waveform giving a new spectrum every  msec
by a single fft calculation every  msec putting successive pairs of
time stretches in the real and imaginary parts of the complex input sequence
as described earlier  the fft algorithm requires n log sub  n operations
which is  when n  an additional  operations are required
for the windowing calculation  repeated every  msec this gives
a rate of  operations per second  to achieve a much lower frequency
resolution with  bandpass filters each of which are fourthorder
will need a great deal more operations  each filter will need between  and 
multiplications per sample depending on its exact digital implementation  but new
samples appear every 
ul
microc
seconds and so somewhere around a million
operations will be required every second
if we increased the frequency resolution to that obtained by the fft 
filters would be needed requiring between  and  million operations
sh   formant estimation
pp
once the frequency spectrum of a speech signal has been calculated it may
seem a simple matter to estimate the positions of the formants  but it is
not  spectra obtained in practice are not usually like the idealized ones
of figure   one reason for this is that unless the analysis is
pitchsynchronous the frequency spectrum of the excitation source is mixed
in with that of the vocal tract filter  there are other reasons which will
be discussed later in this section  but first let us consider how to
extract the vocal tract filter characteristics from the combined spectrum
of source and filter  to do so we must begin to explore the theory of linear
systems
rh discrete linear systems
figure  shows an input signal exciting a filter to produce an output
signal
fc figure 
for present purposes imagine the input to be a glottal
waveform the filter a vocal tract one and the output a
speech signal which is then subjected to highfrequency deemphasis
by radiation from the lips
we will consider here
ul
discrete
systems so that the input xn and output yn are sampled signals
defined only when n is integral  the theory is quite similar for continuous
systems
pp
assume that the system is
ul
linear
that is if input x sub  n produces output y sub  n and
input x sub  n produces output y sub  n
then the sum of x sub  n and
x sub  n will produce the sum of y sub  n and y sub  n
it is easy to show from this that for any constant multiplier a
the input axn will produce output ayn em it is pretty obvious
when a
or indeed any positive integer for then axn can be written as
xnxn 
assume further that the system is
ul
timeinvariant
that is if input xn
produces output yn then a timeshifted version of x
say xnn sub   for
some constant n sub  will produce the same output only timeshifted namely
ynn sub 
pp
now consider the discrete delta function delta n which is  except at
n when it is 
if this single impulse is presented as input to the system the output is called
the
ul
impulse response
and will be denoted by hn
the fact that the system is timeinvariant guarantees that the response does
not depend upon the particular time at which the impulse occurred so that
for example the impulsive input delta nn sub   will produce output
hnn sub  
a deltafunction input and corresponding impulse response are shown in figure

pp
the impulse response of a linear timeinvariant system is an extremely useful
thing to
know for it can be used to calculate the output of the system for any input
at all  specifically an input signal xn can be written
lb
eq
xn   sum from k infinity to infinity  xk delta nk 
en
le
because delta nk is nonzero only when kn and so for any
particular value of n the summation contains only
one nonzero term em that is xn
the action of the system on each term of the sum is to produce an output
xkhnk because xk is just a constant and
the system is linear
furthermore the complete input xn is just the sum of such terms and since
the system is linear the output is the sum of xkhnk
hence the response of the system to an arbitrary input is
lb
eq
yn   sum from k infinity to infinity  xk hnk 
en
le
this is called a
ul
convolution sum
and is sometimes written
lb
eq
yn  xn  hn
en
le
pp
lets write this in terms of ztransforms  the twosided ztransform of yn
is
lb
eq
yz   sum from n infinity to infinity ynz sup n  
 sum from n  sum from k xkhnk z sup n 
en
le
writing z sup n as  z sup nk z sup k  and interchanging the order
of summation this becomes
lb
eq
yz mark   sum from k  sum from n  hnkz sup nk xkz sup k
en
br
eq
lineup   sum from k hzz sup k    hz sum from k xkz sup
k hzxz 
en
le
thus convolution in the time domain is the same as multiplication in the
ztransform domain a very important result  applied to the linear system of
figure  this means that the output ztransform is the input ztransform
multiplied by the ztransform of the systems impulse response
pp
what we really want to do is to relate the frequency spectrum of
the output to the response of the system and the spectrum of the
input
in fact frequency spectra are very closely connected with ztransforms  a
periodic signal xn which repeats every n samples has dft
lb
eq
sum from n to n xne sup j pi rnn 
en
le
and its ztransform is
lb
eq
sum from n infinity to infinity xn z sup n 
en
le
hence the dft is the same as the ztransform of a single cycle of the signal
evaluated at the points  z e sup j pi rn  for r    n
in other
words the frequency components are samples of the ztransform at n
equallyspaced points around the unit circle
hence the frequency spectrum at the output of a linear system is the product of
the
input spectrum and the frequency response of the system itself that is the
transform of its impulse response function
it should be admitted that this statement is somewhat questionable
because to get from ztransforms to dfts we have assumed that
a single cycle only is transformed em and the impulse response function of
a system is not necessarily periodic  the real action of the system is
to multiply ztransforms not dfts  however it is useful in imagining
the behaviour of the system to think in terms of products of dfts and in
practice it is always these rather than ztransforms which are computed
because of the existence of the fft algorithm
pp
figure  shows the frequency spectrum of a typical voiced speech signal
fc figure 
the overall shape shows humps at the formant positions like those in the
idealized figure   however superimposed on this is an oscillation
in the frequency domain at the pitch frequency  this occurs because the
transform of the vocal tract filter has been multiplied by that of the
pitch pulse the latter having components at harmonics of the pitch frequency
the oscillation must be suppressed before the formants
can be estimated to any degree of accuracy
pp
one way of eliminating the oscillation is to perform pitchsynchronous
analysis
this removes the influence of pitch from the frequency domain by dealing with
it in the time domain  the snag is of course that it is not easy to estimate
the pitch frequency  some techniques for doing so are discussed in the next
main section
another way is to use linear predictive analysis which really does get rid
of pitch information without having to estimate the pitch period first  a
smooth
frequency spectrum can be produced using the analysis techniques described in
chapter  which provides
a suitable startingpoint for formant frequency estimation
the third method is to remove the pitch ripple from the frequency spectrum
directly  this will be discussed in an intuitive rather than a
theoretical way because linear predictive methods are becoming dominant
in speech processing
rh cepstral processing of speech
suppose the frequency spectrum of figure  were actually a time waveform
to remove the highfrequency pitch ripple is easy  just filter it out
however
filtering removes
ul
additive
ripples whereas this is a
ul
multiplicative
ripple  to turn multiplication into addition take logarithms  then the
procedure would be
lb
np
compute the dft of the speech waveform windowed overlapped
np
take the logarithm of the transform
np
filter out the highfrequency part corresponding to pitch ripple
le
pp
filtering is often best done using the dft  if the rippled waveform of figure
 is transformed a strong component could be expected at the ripple
frequency with weaker ones at its harmonics  these components can be
simply removed by setting them to zero and inversetransforming the result
to give a smoothed version of the original frequency spectrum
a spectrum of the logarithm of a frequency spectrum is often called a
ul
cepstrum
em a sort of backwards spectrum  the horizontal axis of the cepstrum
having the dimension of time is called quefrency  note that highfrequency
signals have low quefrencies and vice versa  in practice
because the pitch ripple is usually well above the quefrency of interest for
formants the upper end of the cepstrum is often simply cut off from a fixed
quefrency which corresponds to the maximum pitch expected  however identifying
the pitch peaks of the cepstrum has the useful byproduct of giving the pitch
period of the original speech
pp
to summarize then the procedure for spectral smoothing by the cepstral method
is
lb
np
compute the dft of the speech waveform windowed overlapped
np
take the logarithm of the transform
np
take the dft of this logtransform calling it the cepstrum
np
identify the lowestquefrency peak in the spectrum as the pitch
confirming it by examining its harmonics which should be
equally spaced at the pitch quefrency
np
remove pitch effects from the cepstrum by cutting off its highquefrency
part above either the pitch quefrency or some constant representing the maximum
expected pitch which is the minimum expected pitch quefrency
np
inverse dft the resulting cepstrum to give a smoothed spectrum
le
rh estimating formant frequencies from smoothed spectra
the difficulties of formant extraction are not over even when a smooth frequency
spectrum has been obtained  a simple peakpicking algorithm which identifies
a peak at the kth frequency component whenever
lb

xk  xk
  and  
xk  xk

le
will quite often identify formants incorrectly
it helps to specify in advance minimum and maximum formant frequencies em say
 hz and  khz for threeformant identification and ignore peaks lying
outside these limits  it helps to estimate
the bandwidth of the peaks and reject those with bandwidths greater than
 hz em for real formants are never this wide  however if two formants are
very close then they may appear as a single wide peak and be rejected by
this criterion  it is usual to take account of formant positions identified
in previous frames under these conditions
pp
markel and gray  describe in detail several estimation algorithms

markel gray  linear prediction of speech

their simplest uses the number of peaks identified in the raw spectrum
under  khz and with
bandwidths greater than  hz to determine what to do  if exactly three
peaks are found they are used as the formant positions  it is claimed that
this happens about  to  of the time
if only one peak is found the present frame is ignored and the
previouslyidentified
formant positions are used this happens less than  of the time
the remaining cases are two peaks em corresponding to omission of one formant em
and four peaks em corresponding to an extra formant being included  more
than
four peaks never occurred in their data  under these conditions
a nearestneighbour measure is used for disambiguation  the measure is
lb
eq
v sub ij     f sup   sub i k  f sub j k 
en
le
where f sub j sup k is the jth formant frequency defined
in the previous frame
k and  f sup   sub i k is the ith raw data frequency estimate
for frame k
if two peaks only are found this measure is used to identify
the closest peaks in the previous frame and then the
third peak of that frame is taken to be the missing formant
position  if four peaks are found the measure is used to
determine which of them is furthest from the previous formant
values and this one is discarded
pp
this procedure works forwards using the previous frame to
disambiguate peaks given in the current one  more sophisticated
algorithms work backwards as well identifying
ul
anchor points
in the data which have clearlydefined formant positions and
moving in both directions from these to disambiguate
neighbouring frames of data  finally absolute limits can be
imposed upon the magnitude of formant movements between frames
to give an overall smoothing to the formant tracks
pp
very often people will refine the result of such automatic formant
estimation procedures by hand looking at the tracks knowing
what was said and making adjustments in the light of their
experience of how formants move in speech  unfortunately it is difficult to
obtain highquality formant tracks by completely automatic
means
pp
one of the most difficult cases in formant estimation is where
two formants are so close together that the individual peaks
cannot be resolved  one simple solution to this problem is to
employ analysisbysynthesis whereby once a formant is
identified a standard formant shape at this position is
synthesized and
subtracted from the
logarithmic spectrum coker 

coker 

then even if two formants
are right on top of each other the second is not missed because
it remains after the first one has been subtracted
pp
unfortunately however the single peak which appears when
two formants are close together usually does not correspond exactly with the
position of either one
there is one rather advanced signalprocessing technique that
can help in this case
the frequency spectrum of
speech is determined by
ul
poles
which lie in the complex zplane inside the unit circle  they
must be inside the unit circle if the system is stable  those
familiar with laplace analysis of analogue systems may like to note that the
left half of the splane corresponds with the inside of the unit
circle in the zplane  as shown earlier computing a dft is tantamount to
evaluating the ztransform at equallyspaced points around the
unit circle  however better resolution is obtained by
evaluating around a circle which lies
ul
inside
the unit circle but
ul
outside
the outermost pole position  such a circle is sketched in
figure 
fc figure 
pp
recall that the fft is a fast way of calculating the dft of a
sequence  is there a similarly fast way of evaluating the
ztransform inside the unit circle  the answer is yes and the
technique is known as the chirp ztransform because it
involves considering a signal whose frequency increases
linearly em just like a radar chirp signal  the chirp method
allows the ztransform to be computed quickly at equallyspaced
points along spirallyshaped contours around the origin of the
zplane em corresponding to signals of linearly increasing
complex frequency  the spiral nature of these curves is not of
particular interest in speech processing  what
ul
is
of interest though is that the spiral can begin at any point
on
the z axis and its pitch can be set arbitrarily
if we begin spiralling at z say and set the pitch
to zero the contour becomes a circle inside the unit one with
radius   such a circle is exactly what is needed to refine
formant resolution
sh   pitch extraction
pp
the last section discussed how to characterize the vocal tract filter
in the sourcefilter model of speech production  this one looks
at how the most important property of the source em that is the
pitch period em can be derived  in many ways pitch extraction
is more important from a practical point of view than is formant
estimation  in a voiceoutput system formant estimation is
only necessary if speech is to be stored in formantcoded form
for linear predictive storage of speech or for speech synthesis
from phonetics or text formant extraction is unnecessary em
although of course general information about formant
frequencies and formant tracks in natural speech is needed
before a synthesisfromphonetics system can be built
however knowledge of the pitch contour is needed for
many different purposes  for example compact encoding of
linearly predicted speech relies on the pitch being estimated and
stored as a parameter separate from the articulation
significant improvements in frequency analysis can be made by
performing pitchsynchronous fourier transformations
because the need to window is eliminated
many synthesisfromphonetics systems require the pitch contour
for utterances to be stored rather computed from markers in the
phonetic text
pp
another issue which is closely bound up with pitch extraction is
the voicedunvoiced distinction   a good pitch estimator ought to
fail when presented with aperiodic input such as an unvoiced
sound and so give a reliable indication of whether the frame of
speech is voiced or not
pp
one method of pitch estimation which uses the cepstrum has been outlined
above  it involves a substantial amount of computation
and has a high degree of complexity  however if implemented
properly it gives excellent results because the sourcefilter
structure of the speech is fully utilized
another method using the
linear prediction residual will be described in chapter 
again this requires a great deal of computation of a fairly sophisticated
nature and gives good results em although it relies on a
somewhat more
restricted version of the sourcefilter model than cepstral
analysis
rh autocorrelation methods
the most reliable way of estimating the pitch of a periodic
signal which is corrupted by noise is to examine its
shorttime autocorrelation function
the autocorrelation of a signal xn with lag k is defined as
lb
eq
phi k    sum from n infinity to infinity  xnxnk 
en
le
if the signal is quasiperiodic with slowly varying period
a finite stretch of it can be isolated with a window
wi which is  when i is outside the range n
beginning this window at sample m gives the windowed signal
lb
eq
xnwnm
en
le
whose autocorrelation
the
ul
shorttime
autocorrelation of the signal x at point m is
lb
eq
phi sub m k   sum from n  xnwnmxnkwnmk 
en
le
pp
the autocorrelation function exhibits peaks at lags which correspond to
the pitch periods and multiples of it  at such lags the signal is in
phase with a delayed version of itself giving high correlation
the pitch of natural speech ranges about three octaves from  hz lowpitched men to around
 hz children  to ensure that at least two pitch cycles are seen even at
the
low end the window needs to be at least  msec long and the autocorrelation
function calculated for lags up to  msec  the peaks which occur at lags
corresponding to multiples of the pitch become smaller as the multiple
increases because the speech waveform will change slightly and the pitch
period is not perfectly constant  if signals at the high end of the pitch
range  hz are
viewed through a  msec autocorrelation window considerable smearing of
pitch resolution in the time domain is to be expected  finally for unvoiced
speech no substantial peaks of autocorrelation will occur
pp
if all deviations from perfect periodicity can be attributed to
additive white gaussian noise then it can be shown from
standard detection theory that autocorrelation methods are
appropriate for pitch identification  unfortunately this is
certainly not the case for speech signals  although the
shorttime autocorrelation of voiced speech exhibits peaks at
multiples of the pitch period it is not clear that it is any
easier to detect these peaks in the autocorrelation function
than it is in the original time waveform  to take a simple
example if a signal contains a fundamental and inphase first
and second harmonics
lb
eq
xn  a sin  pi fnt  b sin  pi fnt  c sin  pi fnt 
en
le
then its autocorrelation function is
lb
eq
phi k  a sup  cos pi fktb sup  cos pi
fktc sup  cos  pi fkt over   
en
le
there is no reason to believe that detection of the fundamental
period of this signal will be any easier in the autocorrelation
domain than in the time domain
pp
the most common error of pitch detection by autocorrelation
analysis is that the periodicities of the formants are confused
with the pitch  this typically leads to the repetition time
being identified as  t sub pitch    t sub formant  where the
ts are the periods of the pitch and first formant  fortunately
there are simple ways of processing the signal nonlinearly to
reduce the effect of formants on pitch estimation using autocorrelation
pp
one way
is to lowpass filter the
signal with a cutoff above the maximum pitch period say 
hz  however formant  is often below this value  a different
technique which may be used in conjunction with filtering is
to centreclip the signal as shown in figure 
fc figure 
this
removes many of
the ripples which are associated with formants  however it
entails the use of an adjustable clipping threshold to cater for
speech of varying amplitudes  sondhi  who introduced the
technique set the clipping level at  of the maximum
amplitude

sondhi 

an alternative which achieves
much the same effect without the need to fiddle with thresholds
is to cube the signal or raise it to some other high odd
power before taking the autocorrelation  this highlights the
peaks and suppresses the effect of lowamplitude parts
pp
for very accurate pitch detection it is best to combine the evidence
from several different methods of analysis of the time waveform
the autocorrelation function provides one source of evidence
and the cepstrum provides another
a third source comes from the time waveform itself
mcgonegal
ul
et al
 have described a semiautomatic method of pitch
detection which uses human judgement to make a final decision based upon these
three sources of evidence

mcgonegal rabiner rosenberg  sapd

this appears to provide highly accurate pitch contours at the expense of
considerable human effort em it takes an experienced user  minutes to
process each second of speech
rh speeding up autocorrelation
calculating the autocorrelation function is an
arithmeticintensive procedure  for large lags it can best be
done using fft methods although there are simpler arithmetic
tricks which speed it up without going to such complexity
however with the availability of analogue delay lines using
chargecoupled devices autocorrelation can now be done
effectively and cheaply by analogue sampleddata hardware
pp
nevertheless some techniques to speed up digital
calculation of shorttime autocorrelations are in wide use  it
is tempting to hardlimit the signal so that it becomes binary
figure a thus eliminating multiplication
fc figure 
this can be
disastrous however because hardlimited speech is known to
retain considerable intelligibility and therefore the formant
structure is still there  a better plan is to take
centreclipped speech and hardlimit that to a ternary signal
figure b  this simplifies the computation considerably
with essentially no degradation in performance dubnowski
ul
et al


dubnowski schafer rabiner  digital hardware pitch detector

pp
a different approach to reducing the amount of calculation is to
perform a kind of autocorrelation which does not use
multiplications  the
average magnitude difference function
which is defined by
lb
eq
dk   sum from n infinity to infinity  xnxnk 
en
le
has been used for this purpose with some success ross
ul
et al


ross schafer cohen freuberg manley 

it exhibits dips at pitch periods instead of the peaks of the
autocorrelation function
rh featureextraction methods
another possible way of extracting pitch in the time domain is to try to
integrate information from different sources to give reliable
pitch estimates  several features of the time
waveform can be defined each of which provides an estimate of the pitch period
and
an overall estimate can be obtained by majority vote
pp
for example suppose that the only feature of the speech
waveform which is retained is the height and position of the
peaks where a peak is defined by the simplistic criterion
lb

xn  xn
  and  
xn  xn 

le
having found a peak which is thought to represent a pitch pulse
one could define a blanking period based upon the current
pitch estimate within which the next pitch pulse could not
occur  when this period has expired the next pitch pulse is
sought  at first a stringent criterion should be used for
identifying the next peak as a pitch pulse but it can gradually be
relaxed if time goes on without a suitable pulse being
located  figure  shows a convenient way of doing this  a
decaying exponential is begun at the end of the blanking period
and when a peak shows above it is identified as a pitch pulse
fc figure 
one big advantage of this type of algorithm is that the data is
greatly reduced by considering peaks only em which can be
detected by simple hardware  thus it can permit realtime
operation on a small processor with minimal specialpurpose
hardware
pp
such a pitch pulse detector is exceedingly simplistic and will
often identify the pitch incorrectly  however it can be used
in conjunction with other features to produce good pitch
estimates  gold and rabiner  who pioneered the
approach used six features

gold rabiner  parallel processing techniques for pitch periods

lb
np
peak height
np
valley depth
np
valleytopeak height
np
peaktovalley depth
np
peaktopeak height if greater than 
np
valleytovalley depth if greater than 
le
the features are symmetric with regard to peaks and valleys
the first feature is the one described above and the second one works in
exactly the same way
the third feature records the
height between each valley and the succeeding peak and fourth
uses the depth between each peak and the succeeding valley  the
purpose of the final two detectors is to eliminate secondary
but rather large peaks from consideration  figure  shows
the kind of waveform on which the other features might
incorrectly double the pitch but the last two features identify
correctly
fc figure 
pp
gold and rabiner also included the last two pitch estimates from each
feature detector
furthermore for each feature the present estimate
was added to the previous one to make a fourth and the previous one to
the one before that to make a fifth and all three were added together
to make a sixth so that for each feature there were  separate estimates of
pitch  the reason for this is that if three consecutive estimates of the
fundamental period are t sub  t sub  and t sub  then if some peaks are
being falsely identified the actual period could be any of
lb
eq
t sub   t sub   t sub   t sub  
t sub   t sub   t sub  
en
le
it is essential to do this because
a feature of a given type can occur more than once in a pitch period em
secondary peaks usually exist
pp
six features each contributing six separate estimates makes  estimates
of pitch in all
an overall figure was obtained from this
set by selecting the most popular estimate within some
prespecified tolerance  the complete scheme has been
evaluated extensively rabiner
ul
et al
 and compares
favourably with other methods

rabiner cheng rosenberg mcgonegal 

pp
however it must be admitted that this procedure seems to be rather
ul
ad hoc
as are many other successful speech parameter estimation
algorithms  specifically it is not easy to predict what
kinds of waveforms it will fail on and evaluation of it can
only be pragmatic  when used to
estimate the pitch of musical
instruments and singers over a octave range  hz to  khz
instances were found where it failed dramatically tucker and bates 

tucker bates 

this is of
course a much more difficult problem than pitch estimation for
speech where the range is typically  octaves
in fact for speech the feature
detectors are usually preceded by
a lowpass filter to attenuate the myriad
of peaks
caused by higher formants and this
is inappropriate for
musical applications
pp
there is evidence which shows that additional features can
assist with pitch identification  the above features are all
based upon the signal amplitude and could be described as
ul
secondary
features derived from a single
ul
primary
feature  other primary features can easily be defined
tucker and bates  used a centreclipped waveform and considered only
the peaks rising above the central region

tucker bates 

they defined two
further primary features in addition to the peak amplitude  the
ul
time width
of a peak period for which it is
outside the clipping level and its
ul
energy
again outside the clipping level  the primary
features are shown in figure 
fc figure 
secondary features are
defined based on these three primary ones and pitch estimates
are made for each one  a further innovation was to combine the
individual estimates on a way which is based upon
autocorrelation analysis reducing to some degree the
ul
adhocery
of the pitch detection process
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
there are a lot of books on digital signal analysis although in general
i find them rather turgid and difficult to read
lb nn
ackroyd

ds a ackroyd mh
ds d 
ds t digital filters
ds i butterworths
ds c london
nr t 
nr a 
nr o 
  book
inn
here is the exception to prove the rule
this book
ul
is
easy to read
it provides a good introduction to digital signal processing
together with a wealth of practical design information on digital filters
inn
committeeidsp

ds a ieee digital signal processing committee
ds d 
ds t programs for digital signal processing
ds i wiley
ds c new york
nr t 
nr a 
nr o 
  book
inn
this is a remarkable collection of tried and tested fortran programs
for digital signal analysis
they are all available from the ieee in machinereadable form on magnetic
tape
included are programs for digital filter design discrete fourier transformation
and cepstral analysis as well as others like linear predictive analysis
see chapter 
each program is accompanied by a concise wellwritten description of how
it works with references to the relevant literature
inn
oppenheim

ds a oppenheim av
as a  and schafer rw
ds d 
ds t digital signal processing
ds i prentice hall
ds c englewood cliffs new jersey
nr t 
nr a 
nr o 
  book
inn
this is one of the standard texts on most aspects of digital signal processing
it treats the ztransform digital filters and discrete fourier transformation
in far more detail than we have been able to here
inn
rabiner

ds a rabiner lr
as a  and gold b
ds d 
ds t theory and application of digital signal processing
ds i prentice hall
ds c englewood cliffs new jersey
nr t 
nr a 
nr o 
  book
inn
this is the other standard text on digital signal processing
it covers the same ground as oppenheim and schafer  above
but with a slightly faster and consequently more difficult presentation
it also contains major sections on specialpurpose hardware for
digital signal processing
inn
rabiner

ds a rabiner lr
as a  and schafer rw
ds d 
ds t digital processing of speech signals
ds i prentice hall
ds c englewood cliffs new jersey
nr t 
nr a 
nr o 
  book
inn
probably the best single reference for digital speech analysis
as it is for the timedomain encoding techniques of the last chapter
unlike the books cited above it is specifically oriented to speech processing
inn
le nn
eq
delim 
en
ch   resonance speech synthesizers
ds rt resonance speech synthesizers
ds cx principles of computer speech
pp
this chapter considers the design of speech synthesizers which
implement a direct electrical analogue of
the resonance properties of the vocal tract by providing a filter for each
formant whose resonant frequency is to be controlled  another method is the
channel vocoder with a bank of fixed filters whose gains are varied to match
the spectrum of the speech as described in chapter   this is not generally
used for synthesis from a written representation however because it is hard
to get good quality speech  it
ul
is
used sometimes for lowbandwidth
transmission and storage for
it is fairly easy to analyse natural speech into fixed frequency bands
a second alternative to the resonance synthesizer is the linear predictive
synthesizer which at present is used quite extensively and is likely to become
even more popular  this is covered in the next chapter
another alternative is the articulatory synthesizer which
attempts to model the vocal tract directly rather than
modelling the acoustic output from it
although as noted in chapter  articulatory synthesis holds a promise of
highquality speech em for the coarticulation effects caused by tongue
and jaw inertia can be modelled directly em this has not yet been realized
pp
the sourcefilter model of speech production indicates that an electrical
analogue of the vocal tract can be obtained by considering the source
excitation and the filter that produces the formant frequencies separately
this approach was pioneered by fant  and we shall present much of his
work in this chapter

fant  acoustic theory of speech production

there has been some discussion over whether the sourcefilter model really
is a good one and some
synthesizers
explicitly introduce an element of
subglottal coupling which simulates the effect of the lung cavity
on the vocal tract transfer function during the periods when the glottis is
open for an example see rabiner 

rabiner  digital formant synthesizer jasa

however this is very much a loworder effect when considering
speech synthesized by rule from a written representation for the software
which calculates parameter values to drive the synthesizer is a far greater
source of degradation in speech quality
sh   overall spectral considerations
pp
figure  shows the sourcefilter model of speech production
fc figure 
for voiced speech the excitation source produces a waveform whose frequency
components decay at about  dboctave as we shall see in a later section
the excitation passes into the vocal tract filter  conceptually this can best
be viewed as an infinite series of formant filters although for implementation
purposes only the first few are modelled explicitly and the effect of the rest
is lumped together into a higherformant compensation network  in either case
the overall frequency profile of the filter is a flat one upon which humps are
superimposed at the various formant frequencies  thus the output of the
vocal tract filter falls off at  dboctave just as the input does
however measurements of actual speech show a  dboctave decay with increasing
frequency  this is explained by the effect of radiation of speech from the
lips which in fact has a differentiating action producing a  dboctave
rise in the frequency spectrum  this  dboctave lift is similar to that
provided by a treble boost control on a radio or amplifier  speech synthesized
without it sounds unnaturally heavy and bassy
pp
these overall spectral shapes which are derived from considering the human
vocal tract are summarized in the upper annotations in figure   but there
is no real necessity for a synthesizer to model the frequency characteristics
of the human vocal tract at intermediate points  only the output speech is of
any concern  because the system is a linear one the filter blocks in the
figure can be shuffled around to suit engineering requirements  one such
requirement is the desire to minimize internallygenerated noise in the
electrical implementation most of which will arise in the vocal tract filter
because it is much more complicated than the other components  for this
reason an excitation source with a flat spectrum is often preferred as shown
in the lower annotations  this can be generated either by taking the desired
glottal pulse shape with its  dboctave falloff and passing it through a
filter giving  dboctave lift at higher frequencies or if the pulse shape
is to be stored digitally by storing its second derivative instead
then the radiation compensation which is now more properly called
spectral equalization will comprise a  dboctave falloff to give the
required trend in the output spectrum
pp
for a given pitch period this scheme yields exactly the same spectral
characteristics as the original system which modelled the human vocal tract
however when the pitch varies there will be a difference for sounds with
higher excitation frequencies will be attenuated by  dboctave in the new
system and  dboctave in the old by the final spectral equalization
in practice the pitch of the human voice lies quite low in the frequency
region em usually below  hz em and if all filter characteristics begin
their rolloff at this frequency the two systems will be the same  this
simplifies the implementation with a slight compromise in its accuracy in
modelling the spectral trend of human speech for the overall  dboctave
decay actually begins at a frequency of around  hz  if this is
implemented some adjustment will need to be made to the amplitudes to ensure
that highpitched sounds are not attenuated unduly
pp
the discussion so far pertains to voiced speech only  the source spectrum of
the random excitation in unvoiced sounds is substantially flat and combines
with the radiation from the lips to give a  dboctave rise in the output
spectrum  hence if spectral equalization is changed to  dboctave to
accomodate a voiced excitation with flat spectrum the noise source should
show a  dboctave rise to give the correct overall effect
sh   the excitation sources
pp
in human speech the excitation source for voiced sounds is produced by two
flaps of skin called the vocal cords  these are blown apart by pressure from
the lungs  when they come apart the pressure is relieved and the muscles
tensioning the skin cause the flaps to come together again  subsequently the
lung pressure em called subglottal pressure em builds up once more and the
process is repeated  the factors which influence the rate and nature of
vibration are muscular tension of the cords and the subglottal pressure  the detail
of the excitation has considerable importance to speech synthesis because it
greatly influences the apparent naturalness of the sound produced  for example
if you have inflamed vocal cords caused by laryngitis the sound quality
changes dramatically  old people who do not have proper muscular control over
their vocal cord tension produce a quavering sound  shouted speech can easily
be distinguished from quiet speech even when the volume cue is absent em you
can verify this by fiddling with the volume control of a tape recorder em because
when shouting the vocal cords stay apart for a much smaller fraction of the
pitch cycle than at normal volumes
rh voiced excitation in natural speech
there are two basic ways to examine the shape of the excitation source in
people  one is to use a dentists mirror and highspeed photography to observe
the vocal cords directly  although it seems a lot to ask someone to speak
naturally with a mirror stuck down the back of his throat the method has been
used and photographs can be found for example in flanagan 

flanagan  speech analysis synthesis and perception

the second
technique is to process the acoustic waveform digitally identifying the
formant positions and deducting the formant contributions from the waveform by
filtering  this leaves the basic excitation waveform which can then be
displayed  such techniques lead to excitation shapes like those sketched in
figure  in which the gradual opening and abrupt closure of the vocal cords
can easily be seen
fc figure 
pp
it is a fact that if a periodic function has one or more discontinuities its frequency
spectrum will decay at sufficiently high frequencies at the rate of  dboctave
for example the components of the square wave
lb

gt    mark 
  for 
  t  h

br

lineup 
  for 
h  t  b

le
can be calculated from the fourier series
lb
eq
gr     over b  integral from  to b gte sup j pi rtb dt
   j over  pi r e sup j pi rhb 
en
le
so gr is proportional to r and the change in one octave is
lb
eq
log sub   gr over gr
log sub    over 
   
en
 db
le
however if the discontinuities are ones of slope only then the asymptotic decay
at high frequencies is  dboctave  thus the glottal excitation of figure 
will decay at this rate
note that it is not the
ul
number
but the
ul
type
of discontinuities which are important in determining the asymptotic spectral
trend
rh voiced excitation in synthetic speech
there are several ways that glottal excitation can be simulated in a synthesizer
four of which are shown in figure 
fc figure 
the square pulse and the sawtooth pulse
both exhibit discontinuities and so will have the wrong asymptotic rate of
decay  dboctave instead of  dboctave  a better bet is the triangular
pulse  this has the correct decay for there are only discontinuities of slope
however although the asymptotic rate of decay is of first importance the fine
structure of the frequency spectrum at the lower end is also significant and
the fact that there are two discontinuities of slope instead of just one in the
natural waveform means that the spectra cannot match closely
pp
rosenberg  has investigated several different shapes using listening
tests and he found that the polynomial approximation sketched in figure 
was preferred by listeners

rosenberg 

this has one slope discontinuity and comprises
three sections
lb
gt      for   t  t sub     flat during the period of closure
sp
gt    a u sup    u where
u  tt sub  over t sub  t sub       for
t sub   t  t sub   opening phase
sp
sp
gt    a   v sup  where
v  tt sub  over bt sub       for
t sub   t  b    closing phase
le
it is easy to see that the joins between the first and second section and
between the second and third section are smooth but that the slope of the third
section at the end of the cycle when tb is
lb
eq
dg over dt     a
en
le
a is the maximum amplitude of the pulse and is reached when tt sub 
pp
a much simpler glottal pulse shape to implement is the filtered impulse
passing an impulse through a filter with characteristic
lb
eq
 over st sup 
en
le
imparts a  dboctave decay after frequency t  this gives a pulse shape of
lb
eq
gt    a t over t e sup tt 
en
le
which is sketched in figure 
fc figure 
the pulse is the wrong way round in time
when compared with the desired one but this is not important under most
listening conditions because phase differences are not noticeable this
point is discussed further below
the maximum is reached when tt and has
height a  the value zero is never actually attained for the decay to it
is asymptotic and if the slight discontinuity between pulses shown in the
figure is left the asymptotic rate of decay of the frequency spectrum will
be  dboctave rather than  dboctave  however in a real implementation
involving filtering an impulse there will be no such discontinuity for the
next pulse will start off where the last one ended
pp
this seems to be an attractive scheme because of its simplicity
and indeed is sometimes used in speech synthesis  however it does not have
the right properties when the pitch is varied for in real glottal
waveforms the maximum occurs at a fixed
ul
fraction
of the period whereas the filtered impulses maximum is at a fixed time t
if t is chosen to make the system correct at high pitch frequencies say
 hz then the pulse will be much too narrow at low pitches and sound rather
harsh  the only solution is to vary the filter parameters with the pitch
leading to complexity again
pp
holmes  has made an extensive study of the effect of the glottal
waveshape on the naturalness of highquality synthesized speech

holmes  influence of glottal waveform on naturalness

he employed a rather special speech synthesizer which provides far more
comprehensive and sophisticated control than most  it was driven by parameters
which were extracted from natural utterances by hand em but the process of
generating and tuning them took many months of a skilled persons time
by using the pulse shape
extracted from the natural utterance he found that synthetic and natural
versions could actually be made indistinguishable to most people even under highquality
listening conditions using headphones  performance dropped quite drastically
when one of rosenbergs pulse shapes similar to the threesection one given
above was used  holmes also investigated phase effects and found that whilst
different pulse shapes with identical frequency spectra could easily be
distinguished when listening over headphones there was no perceptible difference
if the listener was placed at a comfortable distance from a loudspeaker in
a room  this is attributable to the fact that the room itself imposes a
complex modification to the phase characteristics of the speech signal
pp
although a great deal of care must be taken with the glottal pulse shape for very
highquality synthetic speech for speech synthesized by rule from a written
representation the degradation which stems from incorrect control of the
synthesizer parameters is much greater than that caused by using a slightly
inferior glottal pulse  the triangular pulse illustrated in figure 
has been found quite satisfactory for speech synthesis by rule
rh unvoiced excitation
speech quality is much less sensitive to the characteristics of the unvoiced
excitation  broadband white noise will serve admirably  it is quite
acceptable to generate this digitally using a pseudorandom feedback shift
register  this gives a bit sequence whose autocorrelation is zero except at
multiples of the repetition length  the repetition length
can easily be made as long as the number of states in the shift
register less one em in this case the configuration is called
maximal length gaines 

gaines  stochastic computing advances in information science

for example an bit maximallength shift register will repeat
every  sup   cycles  if the bitstream is used as a source of analogue
noise the autocorrelation function will have triangular parts whose width is
twice the clock period as shown in figure 
fc figure 
according to a wellknown
result the weinerkinchine theorem see for example chirlian 
the power density of the frequency
spectrum is the same as the fourier transform of the autocorrelation function

chirlian 

since the feedback shift register gives a periodic autocorrelation function
its transform is a fourier series  the rth frequency component is
lb
eq
gr    r sup  over  pi sup  r sup  t
cos pi rt over r  
en
le
here t is the clock period and  r sup n t  is the repetition time of
an nbit shift register
pp
the spectrum is a bar spectrum with components spaced
at
lb

 over r over  sup n t   hz
le
these are very close together em with n and
sampling at  khz  musec
the spacing becomes under  hz em and so it is reasonable to treat the
spectrum as continuous with
lb
eq
gf     over  pi sup  f sup  tcos  pi ft 
en
le
this spectrum is sketched in figure a and the measured result of an actual
implementation in figure b
fc figure 
the  db point occurs when
lb
eq
gf over g  over   
en
le
and g is t  hence at the  db point
lb
eq
cos  pi ft over  pi sup  f sup  t sup 
    over   
en
le
which has solution  ft
thus a pseudorandom shift register generates
noise whose spectrum is substantially flat up to half the clock frequency
anything over  khz is therefore a suitable clocking rate for speechquality
noise  choose  khz to err on the conservative side  if the repetition occurs
in less than  or  seconds it can be heard quite clearly but above this figure
it is not noticeable  an bit shift register clocked at  khz repeats
every   sup       seconds which is more than adequate
sh   simulating vocal tract resonances
pp
the vocal tract from glottis to lips can be modelled as an unconstricted
tube of varying crosssection with no side branches and no subglottal coupling
this has an allpole transfer function which can be written in the form
lb
eq
hs   
w sub  sup  over s sup   b sub  s  w sub  sup 
w sub  sup  over s sup   b sub  s  w sub  sup     
en
le
there is an unspecified conceptually infinite number of terms in the
product  each of them produces a peak in the energy spectrum
and these are the formants we observed in chapter 
pp
formants appear even in an oversimplified
model of the tract as a tube of uniform crosssection with a sound source
at one end the larynx and open at the other the lips
this extremely crude model was discussed in chapter  and surprisingly
perhaps it gives a good approximation to the observed formant frequencies
for a neutral relaxed vowel such as that in
ul
ac
bove
pp
speech is made by varying the postures of the various organs of the vocal tract
different vowels for example result largely from different tongue positions
and lip postures  naturally such physical changes alter the frequencies of the
resonances and successful automatic speech synthesis depends upon
successful movement of the formants  fortunately only the first three or
four resonances need to be altered even for extremely realistic synthesis and
virtually all existing synthesizers provide control over these formants only
rh analysis of a single formant
each formant is modelled as a secondorder resonance with transfer function
lb
eq
hs    w sub c sup  over s sup   b s  w sub c sup   
en
le
as will be shown below w sub c is the nominal resonant frequency in
radianss and b is the
approximate  db bandwidth of the resonance  the term w sub c sup  in the
numerator adjusts the gain to be unity at dc s
pp
to calculate the frequency response of the formant write  sjw  then the
energy spectrum is
lb
eq
hjw sup   mark  
w sub c sup  over w sup   w sub c sup   sup   b sup  w sup 
en
sp
sp
eq
lineup  
w sub c sup  over
w sup  w sub c sup   b sup  over   sup  
b sup  w sub c sup b sup  over   
en
sp
le
this reaches a maximum when the squared term in the denominator of the second
expression is zero namely when  ww sub c sup   b sup   sup 
however
formant bandwidths are low compared with their centre frequencies and so to
a good approximation the peak occurs
at  ww sub c  and is of amplitude  w sub c b  that
is  log sub  w sub c b db above the dc gain
at frequencies higher than the peak the energy falls off as w sup 
a factor of  for each doubling
in frequency and so the asymptotic decay is  dboctave
pp
at the points which are  db below the peak
lb
eq
hjw sub db  sup    
 over  hjw sub max  sup    
 over   times  w sub c sup  over b sup   
en
le
and it is easy to show that
this is satisfied by  w sub db    w sub c    b  to a
good approximation neglecting higher powers of bw sub c   figure 
summarizes the shape of an individual formant resonance
fc figure 
pp
the bandwidth of a formant is fairly constant regardless of the formant
frequency  this makes the formant filter a slightly unusual one  most
engineering applications which use variablefrequency resonances require
the bandwidth to be a constant proportion of the resonant
frequency em the ratio
w sub c b often called the q of the filter is to be constant
for formants we wish the q to increase linearly with resonant frequency
since the amplitude gain of the formant at resonance is w sub c b
this peak gain increases as the formant frequency is increased
pp
although it is easy to measure formant frequencies on a spectrogram
cf chapter 
it is not so easy to measure bandwidths accurately  one rather unusual method
was reported by van den berg  who took a subject who had had a partial
laryngectomy an operation which left an opening into the vocal tract near
the larynx position  into this he inserted a sound source and made a
sweptfrequency calibration of the vocal tract

berg van den 

almost as bizarre is a
technique which involves setting off a spark inside the mouth of a subject
as he holds his articulators in a given position
pp
the results of several different kinds of experiment are reported by dunn 
and are summarized in table  along with the formant frequency ranges

dunn 

rf
ini
ta i i
nr x wrange of formant
nr x wrange of bandwidths
hnxurange of formanthnxurange of bandwidths
nr x wfrequencies hz
nr x was measured in different
hnxufrequencies hzhnxuas measured in different
nr x wexperiments hz
hnxuexperiments hz
nr x w  
nr x w  
nr x iwrange of formantwas measured in different
nr x wrange of formant
hnxulnxuul
sp
formant hnxu  hnxu  
formant hnxu  hnxu  
formant hnxu  hnxu  
hnxulnxuul
ta i i i i i i i i i i i i
ini
mt 
table   different estimates of formant bandwidths with range of
formant frequencies for reference
te
note that the bandwidths really are narrow compared with the resonant frequencies
of the filters except at the lower end of the formant  range  choosing the
lowest bandwidth estimate leads to an amplification factor at resonance of  for formant 
when its frequency is at the top of its range and formant  happens to give
the same value
rh series synthesizers
the simplest realization of the vocal tract filter is a chain of formant
filters in series as illustrated in figure 
fc figure 
this leads to particular difficulties if the frequencies of two formants
stray close together  the worst case occurs if formants  and  have the
same resonant frequencies at the top of the range of formant  namely  hz
in this case and if the bandwidths of the formants are set to the lowest
estimates a combined amplification factor
of   times   is
obtained at the point of resonance em that is
 db above the dc value  this is enough
to tax most analogue implementations and can evoke clipping in the formant
filters with a very noticeable effect on speech quality  this
extreme case will not occur during synthesis of realistic speech for
although the formant
ul
ranges
overlap the values for any particular human sound will not coincide exactly  however
it illustrates the difficulty of designing a series synthesizer which copes
sensibly with arbitrary parameter settings and explains why designers often
choose formant bandwidths in the top half of the ranges given in table 
pp
the problem of excessive amplification within a series synthesizer can be
alleviated to a small extent by choosing carefully the order in which the
filters are placed in the chain  in a linear system of course the order in
which the components occur does not matter
in physical implementations however it is advantageous to minimize extreme
amplification at intermediate points  by placing the formant  filter between
formants  and  the formant  resonance is attenuated somewhat before it
reaches formant   continuing with the extreme example above where both
formants  and  were set to  hz assume that formant  is at its
nominal value of  hz  it provides attenuation at approximately  dboctave
above this and so at the formant  peak  octaves higher the attenuation
is  db  thus the gain at  hz
which is   log sub        db after
passing through the formant  filter is reduced to  db by formant  only
to be increased by   log sub        db to
a value of  db by formant 
this avoids the extreme  db gain of formants  and  combined
pp
figure  shows only three formant filters modelled explicitly
the effect of the rest em and they do have an effect although it is small
at low frequencies em is
incorporated by lumping them together into the higherformant correction filter
to calculate the characteristics of this filter assume that the lumped
formants have the values given by the simple uniformtube model of chapter 
namely  hz for formant   hz for formant  and in general
n hz for formant n  the effect of each of these on the spectrum is
lb
eq
 log sub   w sub n sup  over w sup  w sub n sup   sup 
b sub n sup  w sup 
     log sub  w sup  over w sub n sup  sup 
 b sub n sup  w sup  over w sub n sup 
en
db
le
following from what was calculated above
we will have to approximate this by assuming that
b sub n sup  w sub n sup  is
negligible em this is quite reasonable for these higher formants because
table  shows that the bandwidth does not increase in proportion to the
formant frequency range em and approximate the logarithm by the first
term of its series expansion
lb
eq
  log sub   w sup  over w sub n sup  sup 
    log sub   e  log sub e 
w sup  over w sub n sup 
    log sub   e  times  w sup  over w sub n sup   
en
le
pp
now the total effect of formants    at frequency f hz as distinct
from w radianss is
lb
eq
 log sub   e  times  sum from n to infinity
f sup  over  sup  n sup   
en
le
this expression is
lb
eq
 log sub   e  times 
f sup  over  sup sum from n to infinity
 over n sup   sum from n to   over n sup 
 
en
le
the infinite sum can actually be calculated in closed form and is equal
to  pi sup    hence the total correction is
lb
eq
 log sub   e  times f sup  over  sup 
pi sup  over   sum from n to   over n sup 
    times  sup  f sup 
en
db
le
pp
although this may at first seem to be a rather small correction
it is in fact  db when
f khz  on further reflection this is not an unreasonable figure for the
 dboctave decays contributed by formants   and  must all be annihilated
by the higherformant correction to give an overall flat spectral trend
in fact formant  will contribute
 dboctave from  hz  octaves to  khz representing  db formant
 will contribute  dboctave from  hz  octaves to  khz representing
 db and formant  will contribute  dboctave from  hz  octave to  khz
representing  db
these sum to  db
pp
if the first five formants are synthesized explicitly instead of just the
first three the correction is
lb
eq
 log sub   e  times  f sup  over  sup 
pi sup  over   sum from n to   over n sup 
    times  sup   f sup 
en
db
le
giving a rather more reasonable value of  db when f khz  in actual
implementations fixed filters are sometimes included explicitly for
formants  and   although this lowers the gain of the higherformant
correction filter the total amplification at  khz of the combined correction
is still  db  if one is less demanding and aims for a synthesizer that
produces a correct spectrum only up to  khz it is  db
this places quite stringent requirements on the preceding formant filters if
the stray noise that they generate internally is not to be amplified to
perceptible magnitudes by the correction filter at high frequencies
pp
explicit inclusion of fixed filters for formants  and  undoubtedly improves
the accuracy of the higherformant correction  recall that the above derivation
of the correction filter characteristic used the firstorder approximation
lb
eq
log sub e w sup  over w sub n sup 
    w sup  over w sub n sup   
en
le
which is only valid if w  w sub n
thus it only holds at frequencies less than
the highest explicitly synthesized formant
and so with formants   khz and
  khz included a reasonable correction should be obtained for
telephonequality speech  however detailed analysis with a secondorder
approximation shows that the coefficient of the neglected term is in fact
small fant 

fant  acoustic theory of speech production

a second perhaps more compelling reason for explicitly
including a couple of fixed formants is that the otherwise enormous amplification
provided by the correction can be distributed throughout the formant chain
we saw earlier why there is reason to prefer the
order femfemf over femfemf
with explicit formants  and  a suitable order which helps
to keep the amplification at intermediate points in the chain within reasonable
bounds is femfemfemfemf
rh parallel synthesizers
a series synthesizer models the vocal tract resonances by a chain of formant
filters in series  a parallel synthesizer utilizes a parallel connection of
filters as illustrated in figure 
fc figure 
pp
consider a parallel combination of two formants with individuallycontrollable
amplitudes  the combined transfer function is
lb
eq
hs  mark   a sub  w sub  sup  over
s sup   b sub  s  w sub  sup 
a sub  w sub  sup  over s sup   b sub  s  w sub  sup 
en
sp
sp
eq
lineup    a sub  w sub  sup   a sub  w sub  sup  s sup 
a sub  b sub  w sub  sup   a sub  b sub  w sub  sup  s
 a sub  a sub  w sub  sup  w sub  sup  
over
 s sup  b sub  sw sub  sup  
s sup  b sub  sw sub  sup   
en
le
if the formant bandwidths b sub  and b sub 
are equal and the amplitudes are
chosen as
lb
eq
a sub   w sub  sup  over w sub  sup  w sub  sup 

a sub   w sub  sup  over w sub  sup  w sub  sup   
en
le
then the transfer function becomes the same as that of a twoformant series synthesizer
namely
lb
eq
hs    w sub  sup  over s sup   b sub  s  w sub  sup 
  w sub  sup  over s sup   b sub  s  w sub  sup   
en
le
the argument can be extended to any number of formants under the assumption
that the formant bandwidths are equal  note that the signs of a sub 
and a sub 
differ  in general the formant amplitudes for a parallel synthesizer alternate
in sign
pp
in theory therefore it would be possible to use five parallel formants to
model a fiveformant series synthesizer exactly  then the same higherformant
correction filter would be needed for the parallel synthesizer as for the
series one  if the formant amplitudes were set slightly incorrectly however
the five filters would not combine to give a total of  dboctave highfrequency
decay above the resonances  it is easy to see this in the context of the
simplified twoformant combination above  if the amplitudes were not chosen
exactly right then the s sup 
term in the numerator would not be quite zero
then the decay in the twoformant combination would be  dboctave instead
of  dboctave and in the fiveformant case the decay would in fact still be
 dboctave  advantage can be taken of this to equalize the levels
within the synthesizer so that large amplitude variations do not occur
this can best be done by associating relatively lowgain fixed correction filters
with each formant instead of providing one comprehensive correction to the
combined spectrum  these are shown in figure 
suitable correction filters
have been determined empirically by holmes 

holmes  speech synthesis

they provide a  dboctave
lift above  hz for formant  and  dboctave lift above  hz for formant
  formants  and  are uncorrected whilst for formant  the correction begins
as a  dboctave decay above  hz and increases to an  dboctave decay
above  khz
pp
the disadvantage of a parallel synthesizer is that the amplitudes of the
formants must be specified as well as their frequencies  furthermore the
formant bandwidths should all be equal but they are often chosen to be such
in series synthesizers because of the uncertainty as to their exact
values  however the extra amplitude parameters clearly give greater
control over the frequency spectrum of the synthesized speech
pp
a good example of how this extra control can usefully be exploited is the
synthesis of nasal sounds
nasalization introduces a cavity parallel to the oral tract as illustrated
in figure  and this causes zeros in the transfer function
fc figure 
it is as if two different copies of the vocal tract transfer function one for
the oral and the other for the nasal passage were added
together  we have seen the effect of this above when considering parallel
synthesis  the combination
lb
eq
hs    a sub  w sub o sup  over
s sup   b sub o s  w sub o sup 
a sub  w sub n sup 
over s sup   b sub n s  w sub n sup   
en
le
where the subscript o stands for oral and n for nasal
produces zeros in the
numerator unless the amplitudes are carefully adjusted to avoid them
these cannot be modelled by a series synthesizer but they obviously can be
by a parallel one
pp
although they are certainly needed for accurate imitation of human speech
transfer function zeros to simulate nasal sounds are not essential for
synthesis of intelligible english  it is not difficult to get a sound
like a nasal consonant
c
ul
n
or
ul
mc

with an allpole synthesizer
nevertheless it is certainly true that a parallel synthesizer gives better
ul
potential
control over the spectrum than a series one  whether the added flexibility
can be used properly by a synthesisbyrule computer program is another matter
rh implementation of formant filters
formant filters can be built in either analogue or digital form  a
secondorder resonance is needed whose centre frequency can be controlled
but whose bandwidth is fixed  if the control can be arranged as two
tracking resistors then the simple analogue configuration of figure 
with two operational amplifiers will suffice
fc figure 
pp
the transfer function of this arrangement is
lb
eq
   c sub  r sub  c sub  r sub   over
 s sup    over c sub  r sub s
 over c sub  r sub  c sub  r sub    
en
le
which characterizes it as a lowpass resonator with dc gain
of   r sub  r sub    bandwidth of   pi c sub  r sub  hz  and
centre frequency of   pi c sub  r sub  c sub  r sub   sup  hz
tracking r sub  with r sub  ensures that the dc gain remains constant
and that the centre frequency follows  r sub  sup   moreover
neither is especially sensitive to slight departures from exact tracking
of r sub  with r sub 
such a filter has been used in a simple handcontrolled speech synthesizer
built for demonstration and amusement witten and madams 

witten madams  chatterbox

however the need for tracking resistors and the inverse square root variation
of the formant frequency with r sub  makes it rather unsuitable for serious
applications
pp
a better analogue filter is the ringofthree configuration
shown in figure 
fc figure 
ignore the secondary output for now  control
is achieved over the centre frequency by two multipliers driven from
the same control input k  these have a highimpedance output producing a
current kx if the input voltage is x
it is not too difficult to show that the transfer function of the circuit is
lb
eq
   k sup  over c sup   over
 s sup    over rc s
k sup  r sup  over r sup  c sup    
en
le
suppose that r is chosen so that  k sup  r sup      then this is a
unitygain resonator with constant bandwidth   pi rc hz  and centre
frequency  k pi c hz  note that it is the combination of both multipliers that
makes the centre frequency grow linearly with k  with one multiplier there
would be a squareroot relationship
pp
the ringofthree filter of figure  is arranged in a slightly unusual
way with an inverting stage at the beginning and the two resonant stages
following it  this ensures that the signal level at intermediate
points in the filter does not exceed that at the output and gives the filter
the best chance of coping with a wide range of input amplitudes without
clipping  this contrasts markedly with the resonator of figure  where
the voltage at the output of the first integrator is wb times the final output em a
factor of  in the worst case
pp
for a digital implementation of a formant consider the recurrence relation
lb
eq
yn   a sub  yn  a sub  yn  a sub  xn 
en
le
where xn is the input and yn the output at time n
yn and yn are the previous two values of the output
and a sub  a sub  and a sub  are real constants
the minus sign is in front of the second term because it makes a sub 
turn out to be
positive  to calculate the ztransform version of this relationship multiply
through by z sup n and sum from n infinity to infinity 
lb nn
eq
sum from n infinity to infinity ynz sup n  mark 
a sub  sum from n infinity to infinity ynz sup n 
a sub  sum from n infinity to infinity ynz sup n 
a sub  sum from n infinity to infinity xnz sup n
en
sp
eq
lineup   a sub  z sup   sum ynz sup n 
a sub  z sup   sum ynz sup n
 a sub   sum xnx sup n  
en
le nn
writing this in terms of ztransforms
lb
eq
yz   a sub  z sup  yz  a sub  z sup  yz  a sub  xz 
en
le
thus the inputoutput transfer function of the system is
lb
eq
hz   yz over xz
 a sub   over a sub  z sup  a sub  z sup   
en
le
pp
we learned in the previous chapter that the frequency response is obtained
from the ztransform of a system by replacing z sup 
by  e sup j pi ft  where f is the frequency variable in hz
hence the amplitude response of the digital formant filter is
lb
eq
he sup j pi ft  sup 
   left  a sub  over a sub  e sup j pi ft
a sub  e sup j pi ft   right  sup   
en
sp
le
it is fairly obvious from this that a dc gain of  is obtained if
lb
eq
a sub       a sub    a sub  
en
le
for  e sup j pi ft  is  at a frequency of  hz  some manipulation is
required to show that under the usual assumption that the bandwidth is
small the centre frequency is
lb
eq
 over  pi t  cos sup   a sub  over  a sub  sup  
en
hz
le
furthermore the  db bandwidth of the resonance is given approximately by
lb
eq
  over  pi t  log sub e a sub  
en
hz
le
pp
as an example figure  shows an amplitude response for this digital filter
fc figure 
the parameters a sub  a sub  and a sub 
were generated from the above
relationships for a sampling frequency of  khz centre frequency of  khz
and bandwidth of  hz
it exhibits a peak of approximately the right bandwidth at the correct
frequency  khz  note that the response is flat at half the sampling
frequency for the frequency response from  khz to  khz is just a reflection of
that up to  khz
this contrasts sharply with that of an analogue formant filter also shown
in figure  which slopes
at  dboctave at frequencies above resonance
pp
the behaviour of a digital formant filter at frequencies above
resonance actually makes it preferable to an analogue implementation
we saw earlier that considerable trouble must be taken with the latter to
compensate for the cumulative effect of  dboctave at higher frequencies for
each of the formants
this is not necessary with digital implementations for the response of
a digital formant filter is flat at half the sampling frequency  in fact further
study shows that digital synthesizers without any higherpole correction
give a closer approximation to the vocal tract than analogue ones with higherpole
correction gold and rabiner 

gold rabiner  analysis of digital and analogue formant synthesizers

rh timedomain methods
an interesting alternative to frequencydomain speech synthesis is to construct
the formants in the time domain  when a secondorder resonance is excited by
an impulse an exponentially decaying sinusoid is produced as illustrated by
figure 
fc figure 
the oscillation occurs at the resonant frequency of the filter
while the decay is related to the bandwidth  in fact if the formant filter
has transfer function
lb
eq
w sup  over s sup   b s  w sup   
en
le
the time waveform for impulsive excitation is
lb
eq
xt   w e sup bt  sin  wt 
en
neglecting  b sup  w sup 
le
it is the combination of several such time waveforms coupled with the regular
reappearance of excitation at the pitch period that produces the characteristic
wiggly waveform of voiced speech
pp
now suppose we take a sine wave of frequency w and multiply it by a
decaying exponential  e sup bt  this gives a signal
lb
eq
xt   e sup bt  sin  wt 
en
le
which is identical with the filtered impulse except for a factor w
if there are several formants in parallel all with the same bandwidth
the exponential factor is the same for each
lb
eq
xt   e sup bt  a sub   sin  w sub  t
  a sub   sin  w sub  t    a sub    sin  w sub  t 
en
le
a sub  a sub  and a sub  control the formant amplitudes
as in an ordinary parallel synthesizer
except that they need adjusting to account for the missing
factors w sub  w sub  and w sub 
pp
a neat way of implementing such a synthesizer digitally is to store one cycle of a
sine wave in a readonly memory rom  then the formant frequencies can be
controlled by reading the rom at different rates  for example if twice the
basic frequency is desired every second value should be read
multiplication is needed for amplitude control of each formant  this can be
accomplished by shifting the digital word each place shifted accounts for
 db of attenuation  finally the exponential damping factor can be
provided in analogue hardware by a single capacitor after the da converter
this implementation gives a system for hardwaresoftware synthesis which
involves an absolutely minimal amount of extra hardware apart from the computer
and does not need hardware multiplication for realtime operation
it could easily be made to work in real time with a microprocessor coupled
to a da converter damping capacitor and fixed tonecontrol filter to give
the required spectral equalization
pp
because the overall spectral decay of an impulse exciting a secondorder
formant filter is  dboctave the appropriate equalization is  dboctave
lift at high frequencies to give an overall  dboctave spectral trend
pp
note however that this synthesis model is an extremely basic one  only
impulsive excitation can be accomodated  for fricatives which we will
discuss in more detail below a different implementation is needed  a
hardware noise generator with a few fixed filters em one
for each fricative type em will suffice for a simple system  more damaging
is the lack of aspiration where random noise excites the vocal tract resonances
this cannot be simulated in the model  the
ul
h
sound can be provided by
treating it as a fricative and although it will not sound completely realistic
because there will be no variation with the formant positions of adjacent phonemes
this can be tolerated because
ul
h
is not too important for speech intelligibility
a bigger disadvantage is the lack of proper aspiration control for producing
unvoiced stops which as mentioned in chapter  consist of an silent phase
followed by a burst of aspiration
experience has shown that although it is difficult to drive such a synthesizer
from a software synthesisbyrule system quite intelligible output can
be obtained if parameters are derived from real speech and tweaked by hand
then for each aspiration burst the most closelymatching fricative sound
can be used
sh   aspiration and frication
pp
the model of the vocal tract as a filter which affects the frequency spectrum
of the basic voiced excitation breaks down if there are constrictions in it
for these introduce new sound sources caused by turbulent air
the generation of unvoiced excitation has been discussed earlier in this
chapter  now we must consider how to simulate the filtering action of
the vocal tract for unvoiced sounds
pp
aspiration and frication need to be dealt with separately  the former
is caused by excitation at the vocal cords em the cords are held
so close together that turbulent noise is produced
this noise passes through the same vocal tract filter that modifies voiced
sounds and the same kind of formant structure can be observed
all that is needed to simulate it is to replace the voiced excitation
source by white noise as shown in the upper part of figure 
fc figure 
pp
speech can be whispered by substituting aspiration for voicing throughout
of course there is no fundamental frequency associated with aspiration
an interesting way of assessing informally the degradation caused by inadequate
pitch control in a speech synthesisbyrule system is to listen to
whispered speech in which pitch variations play no part
pp
voiced and aspirative excitation are rarely produced at the same time
in natural speech but see the discussion in chapter  about breathy voice
however the excitation can change from one to the other quite quickly and
when this happens there is no discontinuity in the formant structure
pp
fricative or sibilant excitation is quite different from aspiration
because it introduces a new sound source at a different place from the vocal
cords  the constriction which produces the sound may be at the lips
the teeth the hard ridge just behind the top front teeth or further
back along the palate
these positions each produce a different sound
c
ul
f
ul
th
ul
s
and
ul
sh
respectively  however smooth transitions from one of these sounds to another
do not occur in natural speech and dynamical movement of the frequency
spectrum during a fricative is unnecessary for speech synthesis
pp
it is necessary however to be able to produce an approximation to the
noise spectrum for each of these sound types  this is commonly achieved
by a single highpass resonance whose centre frequency can be controlled
this is the purpose of the secondary output
of the formant filter of figure 
taking the output from this point gives a highpass instead of a lowpass
resonance and this same filter configuration is quite acceptable for
fricatives  figure  shows the fricative sound path as a noise generator
followed by such a filter
pp
unlike aspiration fricative excitation is frequently combined with voicing
this gives the voiced fricative sounds
ul
v
ul
dh
ul
z
and
ul
zh
it is possible to produce frication and aspiration together and although
there are no examples of this in english speech synthesisbyrule
programs often use a short burst of aspiration
ul
and
frication when simulating the opening of unvoiced stops
separate amplitude controls are therefore needed for voicing and frication
but the former can be used for aspiration as well with a glottal excitation
type switch to indicate aspiration rather than voicing
sh   summary
pp
a resonance speech synthesizer consists of a vocal tract filter excited by
either a periodic pitch pulse or aspiration noise  in addition a set of
sibilant sounds must be provided  the vocal tract filter is dynamic with
three controllable resonances  these coupled with some fixed spectral
compensation give it a fairly high order em about  complex poles are
needed  although several different sibilant sound types must be simulated
dynamical movement is less important in fricative sound spectra than
for voiced and aspirated sounds because
smooth transitions between one fricative and another are not important
in speech
however fricative timing and amplitude must be controlled rather precisely
pp
the speech synthesizer is controlled by several parameters
these include fundamental frequency if voiced amplitude of voicing
frequency of the first few em typically three em formants
aspiration amplitude sibilance amplitude and frequency of one or more
sibilance filters
additionally if the synthesizer is a parallel one parameters for the
amplitudes of individual formants will need to be included
it may be that some control over formant bandwidths is provided too
thus synthesizers have from eight up to about  parameters klatt 
describes one with  parameters

klatt  software for a cascadeparallel formant synthesizer

pp
the parameters are supplied to the synthesizer at regular intervals of time
for a parameter synthesizer the control can be thought of as a set of
 graphs each representing the time evolution of one parameter
they are usually called parameter
ul
tracks
the terminology dating from the days when a track was painted on a glass
slide for each parameter to provide dynamic control of the synthesizer
lawrence 

lawrence 

the pitch track is often called a pitch
ul
contour
this is a common phoneticians usage
do not confuse this with the everyday meaning of contour
as a line joining points of equal height on a map em a pitch contour is
just the time evolution of the pitch frequency
pp
for computercontrolled synthesizers of course the parameter tracks
are sampled typically every  to  msec
the rate is determined by the need to generate fast amplitude transitions
for nasals and stop consonants
contrast it with the  musec sampling period needed to digitize
telephonequality speech
the raw data rate for a parameter synthesizer updated every  msec
is  parameterssec or  kbits if each parameter is represented
by  bits
this is a substantial reduction over the  kbits needed for pcm representation
for speech synthesis by rule chapter  these parameter tracks
are generated by a computer program from a phonetic or english
version of the utterance lowering the data rate by a further one or two
orders of magnitude
pp
filters for speech
synthesizers can be implemented in either analogue or digital form
highorder filters are usually broken down into secondorder sections in
parallel or in series  a third possibility which has not been discussed
above is to implement a single highorder filter directly  finally the
action of formant filters can be synthesized in the time domain  this gives
eight possibilities which are summarized in table 
rf
in i
ta i i
nr x wanalogue
nr x wdigital
hnxuanaloguehnxudigital
nr x iwliljencrants wmorris and paillet 
nr x wliljencrants 
hnxulnxuul
sp
nr x wrice 
nr x wrabiner fiet alfr
serieshnxurice hnxurabiner fiet alfr
nr x wliljencrants 
nr x wholmes 
parallelhnxuliljencrants hnxuholmes 
nr x wunpublished
nr x wunpublished
timedomainhnxuunpublishedhnxuunpublished
nr x wem
nr x wmorris and paillet 
highorder filterhnxuemhnxumorris and paillet 
hnxulnxuul
ta i i i i i i i i i i i i
ini
fg table   implementation options for resonance speech synthesizers

rice  byte


rabiner jackson schafer coker 


liljencrants 


holmes  influence of glottal waveform on naturalness


morris and paillet 

all but one have certainly been used as the basis for synthesis and
the table includes reference to published descriptions
pp
each method has advantages and disadvantages  series decomposition obviates
the need for control over the amplitudes of individual formants but does
not allow synthesis of sounds which use the nasal tract as well as the oral
one for these are in parallel  analogue implementation of series synthesizers
is complicated by the need for higherpole correction and the fact that
the gains at different frequencies can vary widely throughout the system
higherpole correction is not so important for digital synthesizers
parallel decomposition eliminates some of these problems  higherpole correction
can be implemented individually for each formant  however the formant
amplitudes must be controlled rather precisely to simulate the vocal tract
which is essentially serial
timedomain synthesis is associated with low hardware costs but does not
easily allow proper control over the excitation sources  in particular
it cannot simulate dynamical movement of the spectrum during aspiration
implementation of the entire vocal tract model as a single highorder filter
without breaking it down into individual formants in series or parallel
is attractive from the computational point of view because less arithmetic
operations are required  it is best analysed in terms of linear predictive
coding which is the subject of the next chapter
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
historicallyminded readers should look at the early speech synthesizer
designed by lawrence 
this and other classic papers on the subject
are reprinted in flanagan and rabiner 
a good description of a quite sophisticated parallel synthesizer can
be found in holmes  above and another of a switchable
seriesparallel one in klatt  who even includes a listing of
the fortran program that implements it
here are some useful books on speech synthesizers
lb nn
fant

ds a fant g
ds d 
ds t acoustic theory of speech production
ds i mouton
ds c the hague
nr t 
nr a 
nr o 
  book
inn
fant really started the study of the vocal tract as an acoustic system
and this book marks the beginning of modern speech synthesis
inn
flanagan

ds a flanagan jl
ds d 
ds t speech analysis synthesis and perception nd expanded edition
ds i springer verlag
ds c berlin
nr t 
nr a 
nr o 
  book
inn
this book is the speech researchers bible and like the bible its not
all that easy to read
however it is an essential reference source for speech acoustics and
speech synthesis as well as for human speech perception
inn
flanagan

ds a flanagan jl
as a  and rabiner lreditors
ds d 
ds t speech synthesis
ds i dowsen hutchinson and ross
ds c stroudsburg pennsylvania
nr t 
nr a 
nr o 
  book
inn
i recommended this book at the end of chapter  as a collection of
classic papers on the subject of speech synthesis and synthesizers
inn
holmes

ds a holmes jn
ds d 
ds t speech synthesis
ds i mills and boom
ds c london
nr t 
nr a 
nr o 
  book
inn
this little book by one of britains foremost workers in the field
introduces the subject of speech synthesis and speech synthesizers
it has a particularly good discussion of parallel synthesizers
inn
le nn
eq
delim 
en
ch   linear prediction of speech
ds rt linear prediction of speech
ds cx principles of computer speech
pp
the speech coding techniques which were discussed in chapter  operate
in the time domain while the analysis and synthesis techniques
of chapters  and  are
based in the frequency domain  linear prediction is a relatively
new method of speech analysissynthesis
introduced in the early s and used
extensively since then which is primarily a timedomain coding method
but can be used to give frequencydomain parameters like formant
frequency bandwidth and amplitude
pp
it has several advantages over other speech analysis techniques and is
likely to become increasingly dominant in speech output systems
as well as bridging the gap between time and frequencydomain techniques it
is of equal value for both speech storage and speech synthesis and forms
an extremely convenient basis for speechoutput systems which use highquality
stored speech for routine messages and synthesis from phonetics or text
for unusual or exceptional conditions  linear prediction can be used to
separate the excitation source properties of pitch and amplitude from the
vocal tract filter which governs phoneme articulation or in other words
to separate much of the prosodic from the segmental information
hence it makes it easy to use stored segmentals with synthetic prosody
which is just what is needed to enhance the flexibility of stored speech by
providing overall intonation contours for utterances formed by word
concatenation see chapter 
pp
the frequencydomain analysis technique
of fourier transformation necessarily involves approximation because it
applies only to periodic waveforms and so the artificial operation
of windowing is required to suppress the aperiodicity of real
speech  in contrast the linear predictive technique being a timedomain
method can em in certain forms em deal more rationally with aperiodic
signals
pp
the basic idea of linear predictive coding is exactly the same as
one form of adaptive differential pulse code modulation which
was introduced briefly in chapter   there it was noted that a speech
sample xn can be predicted quite closely by the previous sample
xn  the prediction can be improved by multiplying the previous
sample by a number say a sub  which is adapted on a syllabic
timescale  this can be utilized for speech coding by transmitting
only the prediction error
lb
eq
enxna sub  xn
en
le
and using it and the value of a sub  to reconstitute the signal
xn at the receiver  it is worthwhile noting that
exactly the same relationship was used for digital
preemphasis in chapter  with the value of a sub 
being constant at about  em although
the possibility of adapting it to take into account the difference
between voiced and unvoiced speech was discussed
pp
an obvious extension is to use several past values of the signal to form
the prediction instead of just one  different multipliers for each would
be needed so that the prediction error could be written as
lb
eq
en mark xna sub  xna sub  xna sub p xnp
en
sp
eq
lineup xnsum from k to p a sub k xnk
en
le
the multipliers a sub k should be adapted to minimize the error signal
and we will consider how to do this in the next section  it turns out
that they must be recalculated and transmitted on a timescale that is
rather faster than syllabic but much slower than
the basic sampling rate  intervals
of  msec are usually used compare this with the  musec sampling
rate for telephonequality speech
a configuration for highorder adaptive differential
pulse code modulation is shown in figure 
fc figure 
pp
figure  shows typical time waveforms for each of the ten coefficients
over a second stretch of speech
fc figure 
notice that they vary much more slowly than say the speech waveform of
figure 
pp
turning the above relationship into ztransforms gives
lb
eq
ezxzsum from k to p a sub k z sup k xz
sum from k to p a sub k z sup k xz
en
le
rewriting the speech signal in terms of the error
lb
eq
xz over  sum a sub k z sup k ez 
en
le
pp
now let us bring together some facts from the previous chapter which will
allow the timedomain technique of linear prediction to be interpreted
in terms of the frequencydomain formant model of speech  recall that speech
can be viewed as an excitation source passing through a vocal tract filter
followed by another filter to model the effect of radiation from the lips
the overall spectral levels can be reassigned as in figure  so that
the excitation source has a  dboctave spectral profile and hence is
essentially impulsive
considering the vocal tract filter as a series connection
of digital formant filters its transfer function is the product of terms like
lb
eq
 over b sub  z sup  b sub  z sup  
en
le
where b sub  and b sub  control the position and bandwidth of the formant resonances
the  dboctave spectral compensation can be modelled by the
firstorder digital filter
lb
eq
 over bz sup  
en
le
the product of all these terms when multiplied out will have the
form
lb
eq
 over c sub  z sup  c sub  z sup  
c sub q z sup q  
en
le
where q is twice the number of formants plus one and the cs are calculated
from the positions and bandwidths of the formant resonances and the spectral
compensation parameter  hence
the ztransform of the speech is
lb
eq
xz over  sum from k to q c sub k z sup k iz 
en
le
where iz is the transform of the impulsive excitation
pp
this is remarkably similar to the linear prediction relation given earlier  if
p and q are the same then the linear predictive coefficients a sub k
form a pth order polynomial which is the same as that obtained by multiplying
together the secondorder polynomials representing the individual formants
together with the firstorder one for spectral compensation
furthermore the predictive error ez can be identified with the
impulsive excitation iz  this raises the very interesting
possibility of parametrizing the error signal by its frequency and
amplitude em two relatively slowlyvarying quantities em instead of
transmitting it samplebysample at an  khz rate  this is how
linear prediction separates out the excitation properties of the source
from the vocal tract filter  the source parameters can be derived
from the error signal and the vocal tract filter is represented by
the linear predictive coefficients
figure  shows how this can be used for speech transmission
fc figure 
note that
ul
no
signals need now be transmitted at the speech sampling rate for the
source parameters vary relatively slowly  this leads to an extremely
low data rate
pp
practical linear predictive coding schemes operate with a value of p between
 and  corresponding approximately to formant and formant synthesis
respectively  the a sub ks are recalculated every  to  msec and
transmitted to the receiver  also the pitch and amplitude
of the speech are estimated and transmitted at the same rate
if the speech
is unvoiced there is no pitch value  an unvoiced flag is
transmitted instead
because the linear predictive coefficients are intimately related to
formant frequencies and bandwidths a frame rate in the region
of  to  msec is appropriate because this approximates the maximum rate
at which acoustic events happen in speech production
pp
at the receiver the excitation waveform
is reconstituted
for voiced speech it is impulsive at the specified
frequency and with the specified amplitude while for unvoiced speech it
is random with the specified amplitude  this signal en together
with the transmitted parameters a sub   a sub p is used
to regenerate the speech waveform by
lb
eq
xnensum from k to p a sub k xnk 
en
le
em which is the inverse of the transmitters formula for calculating en
namely
lb
eq
enxnsum from k to p a sub k xnk 
en
le
this relies on knowing the past p values of the speech samples
many systems set these past values to zero at the beginning of each pitch
cycle
pp
linear prediction can also be used for speech analysis rather than
for speech coding as shown in figure 
fc figure 
instead of transmitting the coefficients a sub k
they are used to determine the formant positions and bandwidths
we saw above that the polynomial
lb
eq
a sub  z sup  a sub  z sup  a sub p z sup p 
en
le
when factored into a product of secondorder terms gives the formant
characteristics as well as the spectral compensation term
factoring is equivalent to finding the complex roots of the polynomial
and this is fairly demanding computationally em especially if done at
a high rate  consequently peakpicking algorithms are sometimes
used instead  the absolute value of the polynomial gives the
frequency spectrum of the vocal tract filter and the formants
appear as peaks em just as they do in cepstrally smoothed speech
see chapter 
pp
the chief deficiency in the linear predictive method whether it
is used for speech coding or for speech analysis is that em like a series
synthesizer em it
implements an allpole model of the vocal tract
we mentioned in chapter  that this is rather simplistic
especially for nasalized sounds which involve a cavity in parallel
with the oral one  some research has been done on incorporating zeros
into a linear predictive model but it complicates the problem of
calculating the parameters enormously  for most purposes people seem
to be able to live with the limitations of the allpole model
sh   linear predictive analysis
pp
the key problem in linear predictive coding is to determine the values
of the coefficients a sub   a sub p
if the error signal is to be transmitted on a samplebysample basis
as it is in adaptive differential pulse code modulation then it can be most
economically encoded if its mean power is as small as possible
thus the coefficients are chosen to minimize
lb
eq
sum en sup 
en
le
over some period of time
the period of time used is related to the frame rate at which the
coefficients are transmitted or stored although there is no need
to make it exactly the same as one frame interval  as mentioned above
the frame size
is usually chosen to be in the region of  to  msec  some
schemes minimize the error signal over as few as  samples
corresponding to  msec at a  khz sampling rate  others take
longer up to  samples  msec
pp
however if the error signal is to be considered as impulsive and
parametrized by its frequency and amplitude before transmission
or if the coefficients a sub k are to be used for spectral calculations
then it is not immediately obvious how the coefficients should be
calculated
in fact it is still best to choose them to minimize the above sum
this is at least plausible for an impulsive excitation will have a
rather small mean power em most of the samples are zero
it can be justified theoretically in terms of
ul
spectral whitening
for it can be shown that minimizing the meansquared error
produces an error signal whose spectrum is maximally flat
now the only two waveforms whose spectra are absolutely flat
are a single impulse and white noise  hence if
the speech is voiced minimizing the meansquared error
will lead to an error signal which is as nearly impulsive
as possible  provided the timeframe for minimizing is short enough
the impulse will correspond to a single excitation pulse
if the speech is unvoiced minimization will lead to an error
signal which is as nearly white noise as possible
pp
how does one choose the linear predictive coefficients to minimize
the meansquared error  the total squared prediction error is
lb
eq
msum from n en sup sum from n
xn sum from k to p a sub k x sub nk  sup  
en
le
leaving the range of summation unspecified for the moment
to minimize m by choice of the coefficients a sub j differentiate
with respect to each of them and set the resulting derivatives
to zero
lb
eq
dm over da sub j  sum from n xnjxn
sum from k to p a sub k xnk
en
le
so
lb
eq
sum from k to p a sub k  sum from n xnjxnk
sum from n xnxnjjp
en
le
pp
this is a set of p linear equations for the p unknowns a sub  
a sub p
solving it is equivalent to inverting a p times p matrix
this job must be repeated at the frame rate and so if
realtime operation is desired quite a lot of calculation is needed
rh the autocorrelation method
so far the range of the nsummation has been left open  the
coefficients of the matrix equation have the form
lb
eq
sum from n xnjxnk
en
le
if a doublyinfinite summation were made with xn being defined
as zero whenever n we could make use of the fact that
sp
ce
eq
sum from n infinity to infinity xnjxnk
sum from n infinity to infinity xnjxnk
sum from n infinity to infinity xnxnjk
en
sp
to simplify the matrix equation  this just states that the
autocorrelation of an infinite sequence depends only on the lag at which
it is computed and not on absolute time
pp
defining rm as the
autocorrelation at lag m that is
lb
eq
rm sum from n xnxnm
en
le
the matrix equation becomes
lb
ne
nf
eq
ra sub  ra sub  ra sub  r
en
eq
ra sub  ra sub  ra sub  r
en
eq
ra sub  ra sub  ra sub  r
en
eq
etc
en
fi
le
an elegant method due to durbin and levinson exists for solving this
special system of equations  it requires much less computational
effort than is generally needed for symmetric matrix equations
pp
of course an infinite range of summation can not be used in
practice  for one thing the power spectrum is changing and
only the data from a short timeframe should be used for
a realistic estimate of the optimum linear predictive coefficients
hence a windowing procedure
lb
eq
xn sup  w sub n xn
en
le
is used to reduce the signal to zero outside a finite range of
interest  windows were discussed in chapter  from the
point of view of fourier analysis of speech signals and the same
sort of considerations apply to choosing a window for linear
prediction
pp
this is known as the
ul
autocorrelation method
of computing prediction parameters  typically a window of
 to  samples is used for analysis of one frame of speech
rh algorithm for the autocorrelation method
the algorithm for obtaining linear prediction coefficients
by the autocorrelation method is quite simple  it is
straightforward to compute the matrix coefficients
rm from the speech samples and window coefficients
the durbinlevinson method of solving matrix equations operates
directly on this rvector to produce the coefficient vector a sub k
the complete procedure is given as procedure  and is shown
diagrammatically in figure 
fc figure 
rf
fi
na
nh
ul
const
n p
ul
type
svec 
ul
array
n
ul
of
real
cvec 
ul
array
p
ul
of
real
sp
ul
procedure
autocorrelationsignal vec window svec
ul
var
coeff cvec
sp
computes linear prediction coefficients by autocorrelation method
in coeffp
sp
ul
var
r temp
ul
array
p
ul
of
real
n n ij p e real
sp
ul
begin
window the signal
inn
ul
for
n
ul
to
n
ul
do
signaln  signalnwindown
sp
compute autocorrelation vector
br
ul
for
i
ul
to
p
ul
do begin
inn
ri  
br
ul
for
n
ul
to
ni
ul
do
ri  ri  signalnsignalni
inn
ul
end
sp
solve the matrix equation by the durbinlevinson method
br
e  r
br
coeff  re
br
ul
for
i
ul
to
p
ul
do begin
inn
e  coefficoeffie
br
coeffi  ri
br
ul
for
j
ul
to
i
ul
do
coeffi  coeffi  rijcoeffj
br
coeffi  coeffie
br
ul
for
j
ul
to
i
ul
do
tempj  coeffj  coefficoeffij
br
ul
for
j
ul
to
i
ul
do
coeffj  tempj
inn
ul
end
inn
ul
end
nf
fg procedure   pascal algorithm for the autocorrelation method
pp
this algorithm is not quite as efficient as it might be for some
multiplications are repeated during the calculation of the
autocorrelation vector  blankinship  shows how
the number of multiplications can be reduced by about half

blankinship 

pp
if the algorithm is performed in fixedpoint arithmetic
as it often is in practice because of speed considerations
some scaling must be done  the maximum and minimum values of
the windowed signal can be determined within the window
calculation loop and one extra pass over the vector will
suffice to scale it to maximum significance
incidentally if all sample values are the same the procedure
cannot produce a solution because e becomes zero and this
can easily be checked when scaling
pp
the absolute value of the rvector has no significance and since
r is always the greatest element this can be set to the largest
fixedpoint number and the other rs scaled down appropriately
after they have been calculated
these scaling operations are shown as dashed boxes in figure 
e decreases monotonically
as the computation proceeds so it is safe to initialize it to r
without extra scaling  the remainder of the scaling is straightforward
with the linear prediction coefficients a sub k appearing as fractions
rh the covariance method
one of the advantages of linear predictive methods that was
promised earlier was that it allows us to escape from
the problem of windowing  to do this we must abandon the
requirement that the coefficients of the matrix equation have
the symmetry property of autocorrelations  instead suppose
that the range of nsummation uses a fixed number of
elements say n starting at nh to estimate the prediction
coefficients between sample number h and sample number hn
pp
this leads to the matrix equation
lb
eq
sum from k to p a sub k sum from nh to hn xnjxnk 
sum from nh to hn xnxnjjp
en
le
alternatively we could write
lb
eq
sum from k to p a sub k  q sub jk sup hq sub j sup h
jp
en
le
where
lb
eq
q sub jk sup hsum from nh to hn xnjxnk
en
le
note that some values of xn outside the range  h    n    hn  are
required  these are shown diagrammatically in figure 
fc figure 
pp
now  q sub jk sup h  q sub kj sup h  so the equation has
a diagonally symmetric matrix and in fact the matrix q sup h can
be shown to be positive semidefinite em and is almost always positive
definite in practice  advantage can be taken of these facts
to provide a computationally efficient method for solving the
equation  according to a result called choleskys theorem a
positive definite symmetric matrix q can be factored into the form
q    ll sup t where l is a lower triangular matrix
this leads to an efficient
solution algorithm
pp
this method of computing prediction coefficients has become known
as the
ul
covariance method
it does not use windowing of the speech signal and can give accurate
estimates of the prediction coefficients with a smaller analysis
frame than the autocorrelation method  typically  to  speech samples
might be used to estimate the coefficients and they are recalculated
every  to  samples
rh algorithm for the covariance method
an algorithm for the covariance method is given in procedure 
rf
fi
na
nh
ul
const
n p
ul
type
svec 
ul
array
pn
ul
of
real
cvec 
ul
array
p
ul
of
real
sp
ul
procedure
covariancesignal svec
ul
var
coeff cvec
sp
computes linear prediction coefficients by covariance method
in coeffp
sp
ul
var
q
ul
array
pp
ul
of
real
n n ijr p x real
sp
ul
begin
calculate uppertriangular covariance matrix in q
inn
ul
for
i
ul
to
p
ul
do
inn
ul
for
ji
ul
to
p
ul
do begin
inn
qij
br
ul
for
n
ul
to
n
ul
do
inn
qij  qij  signalnisignalnj
inn
inn
ul
end
inn
sp
calculate the square root of q
br
ul
for
r
ul
to
p
ul
do
inn
ul
begin
inn
ul
for
i
ul
to
r
ul
do
inn
ul
for
j
ul
to
i
ul
do
inn
qir  qir  qjiqjr
inn
ul
for
j
ul
to
r
ul
do
inn
ul
begin
inn
x  qjr
br
qjr  qjrqji
br
qrr  qrr  qjrx
inn
ul
end
inn
inn
inn
ul
end
inn
sp
calculate coeffp
br
ul
for
r
ul
to
p
ul
do
inn
ul
for
i
ul
to
r
ul
do
qr  qr  qirqi
inn
ul
for
r
ul
to
p
ul
do
qr  qrqrr
br
ul
for
rp
ul
downto

ul
do
inn
ul
for
ir
ul
to
p
ul
do
qr  qr  qriqi
inn
ul
for
r
ul
to
p
ul
do
coeffr  qr
inn
ul
end
nf
fg procedure   pascal algorithm for the covariance method
and is shown diagrammatically in figure 
fc figure 
the algorithm shown is not terribly efficient from a computation
and storage point of view although it is workable  for one thing
it uses the obvious method for computing the covariance matrix
by calculating
eq
q sub  sup h 
en
eq
q sub  sup h   
en
eq
q sub p sup h 
en
eq
q sub  sup h  
en
in turn which repeats most of the multiplications p times em not
an efficient procedure  a simple alternative is to precompute the necessary
multiplications and store them in a  nh times p diagonally symmetric
table but even apart from the extra storage required for this the number
of additions which must be performed subsequently to give the qs is far
larger than necessary  it is possible however to write a procedure which is
both time and spaceefficient witten 

witten  algorithms for linear prediction

pp
the scaling problem is rather more tricky for the covariance
method than for the autocorrelation method  the xvector
should be scaled initially in the same way as before but now there
are p diagonal elements of the covariance matrix any of which could
be the greatest element  of course
lb
eq
q sub jk    max  q sub   q sub    q sub pp 
en
le
but despite the considerable communality in the summands of the diagonal
elements there are no
ul
a priori
bounds on the ratios between them
pp
the only way to scale the q matrix properly is to calculate each of its p
diagonal elements and use the greatest as a scaling factor
alternatively the fact that
lb
eq
q sub jk    n times max x sub n sup  
en
le
can be used to give a bound for scaling purposes however this
is usually a rather conservative bound and as n is often around  several
bits of significance will be lost
pp
scaling difficulties do not cease when q has been determined  it is possible
to show that the elements of the lowertriangular matrix l which represents
the square root of q are actually
ul
unbounded
in fact there is a slightly different variant of the cholesky decomposition
algorithm which guarantees bounded coefficients but suffers from the
disadvantage that it requires square roots to be taken martin
ul
et al


martin peters wilkinson 

however experience with the method indicates that it is rare for the elements
of l to exceed  times the maximum element of q and the possibility of
occasional failure to adjust the coefficients may be tolerable in a practical
linear prediction system
rh comparison of autocorrelation and covariance analysis
there are various factors which should be taken into account when
deciding whether to use the autocorrelation or covariance method for linear
predictive analysis  furthermore there is a rather different technique
called the lattice method which will be discussed shortly
the autocorrelation method involves windowing which means that in
practice a rather longer stretch of speech should be used
for analysis  we have illustrated this by setting n in the
autocorrelation algorithm and  in the covariance one
offsetting the extra calculation that this entails is the
fact that the durbinlevinson method of inverting a matrix is much more
efficient than cholesky decomposition  in practice this means
that similar amounts of computation are needed for each method em a
detailed comparison is made in witten 

witten  algorithms for linear prediction

pp
a factor which weighs against the covariance method is the
difficulty of scaling intermediate quantities within the algorithm
the autocorrelation method can be implemented quite satisfactorily
in fixedpoint arithmetic and this makes it more suitable for
hardware implementation  furthermore serious instabilities sometimes
arise with the covariance method whereas it can be shown that
the autocorrelation one is always stable  nevertheless the approximations
inherent in the windowing operation and the smearing effect of taking a
larger number of sample points mean that covariancemethod coefficients
tend to represent the speech more accurately if they can be obtained
pp
one way of using the covariance method which has proved to be rather
satisfactory in practice is to synchronize the analysis frame with
the beginning of a pitch period when the excitation is strongest
pitch synchronous techniques were discussed in chapter  in the context
of discrete fourier transformation of speech  the snag of course is that
pitch peaks do not occur uniformly in time and furthermore it is difficult
to estimate their locations precisely
sh   linear predictive synthesis
pp
if the linear predictive coefficients and the error signal are available
it is easy to regenerate the original speech by
lb
eq
xnen sum from k to p a sub k xnk 
en
le
if the error signal is parametrized into the sound source type
voiced or unvoiced amplitude and pitch if voiced it can be
regenerated by an impulse repeated at the appropriate pitch
frequency if voiced or white noise if unvoiced
pp
however it may be that the filter represented by the coefficients a sub k is
unstable causing the output speech signal to oscillate wildly
in fact it is only possible for the covariance method to produce an
unstable filter and not the autocorrelation method em although even
with the latter truncation of the a sub ks for transmission may turn
a stable filter into an unstable one  furthermore the coefficients
a sub k are not suitable candidates for quantization because small
changes in them can have a dramatic effect on the characteristics of
the synthesis filter
pp
both of these problems can be solved by using a different set of numbers
called
ul
reflection coefficients
for quantization and transmission  thus for example in figures 
and  these reflection coefficients could be derived at the
transmitter quantized and used by the receiver to reproduce
the speech waveform  they can be related to reflection and transmission
parameters at the junctions of an acoustic tube model of the vocal tract
hence the name  procedure  shows an algorithm for calculating the
reflection coefficients from the filter coefficients a sub k
rf
fi
na
nh
ul
const
p
ul
type
cvec 
ul
array
p
ul
of
real
sp
ul
procedure
reflectioncoeff cvec
ul
var
refl cvec
sp
computes reflection coefficients in reflp corresponding
to linear prediction coefficients in coeffp
sp
ul
var
temp cvec  i m p
sp
ul
begin
inn
ul
for
mp
ul
downto

ul
do begin
inn
reflm  coeffm
br
ul
for
i
ul
to
m
ul
do
tempi  coeffi
br
ul
for
i
ul
to
m
ul
do
tin
coeffi 
tin
coeffi  reflmtempmi    reflmreflm
inn
ul
end
inn
ul
end
nf
mt 
procedure   pascal algorithm for producing reflection coefficients
from filter coefficients
te
pp
although we will not go into the theoretical details here
reflection coefficients are bounded by  for stable filters
and hence form a useful test for stability  having a limited
range makes them easy to quantize for transmission and in fact
they behave better under quantization than do the filter coefficients
one could resynthesize speech from reflection coefficients by first
converting them to filter coefficients and using the synthesis
method described above  however it is natural to seek a singlestage
procedure which can regenerate speech directly from reflection
coefficients
pp
such a procedure does exist and is called a
ul
lattice filter
figure  shows one form of lattice for speech synthesis
fc figure 
the error signal whether transmitted or synthesized
enters at the upper lefthand corner passes along the top forward
signal path being modified on the way to give the output signal
at the righthand side
then it passes back through a chain of delays along the bottom
backward path and is used to modify subsequent forward signals
finally it is discarded at the lower lefthand corner
pp
there are p stages in the lattice structure of figure  where p is the
order of the linear predictive filter
each stage involves two multiplications by the appropriate
reflection coefficients one by the backward signal em the
result of which is added into the forward path em and the other by
the forward signal em the result of which is subtracted from the
backward path  thus the number of multiplications is twice
the order of the filter and hence twice as many as for the
realization using coefficients a sub k  if the labour necessary
to turn the reflection coefficients into a sub ks is included
the computational load becomes the same  moreover since the
reflection coefficients need fewer quantization bits than the a sub ks
for a given speech quality the word lengths are smaller in the
lattice realization
pp
the advantages of the lattice method of synthesis over direct evaluation
of the prediction using filter coefficients a sub k then are
lb
np
the reflection coefficients are used directly
np
the stability of the filter is obvious from the reflection coefficient
values
np
the system is more tolerant to quantization errors in fixedpoint
implementations
le
although it may seem unlikely that an unstable filter would be produced
by linear predictive analysis instability is in fact a real problem
in nonlattice implementations  for example
coefficients are often interpolated at the receiver to allow longer
frame times and smooth over sudden transitions and it is quite likely that
an unstable configuration is obtained when interpolating filter coefficients
between two stable configurations
this cannot happen with reflection coefficients however because a
necessary and sufficient condition for stability is that all
coefficients lie in the interval 
sh   lattice filtering
pp
lattice filters are an important new method of linear predictive
ul
analysis
as well as synthesis and so
it is worth considering the theory behind them a little further
rh theory of the lattice synthesis filter
figure  shows a single stage of the synthesis lattice given earlier
fc figure 
there are two signals at each side of the lattice and the ztransforms
of these have been labelled x sup  and x sup  at the lefthand side
and y sup  and y sup  at the righthand side
the direction of signal flow is forwards along the upper positive path
and backwards along the lower negative one
pp
the signal flows show that the following two relationships hold
lb
eq
y sup   x sup   k z sup  y sup  
en
for the forward upper path
br
eq
x sup    ky sup   z sup  y sup  
en
hwufor the backward lower path
le
rearranging the first equation yields
lb
eq
x sup    y sup   k z sup  y sup  
en
le
and so we can describe the function of the lattice by a single matrix
equation
lb
ne
eq
left  matrix ccol x sup  above x sup  right  
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol y sup  above y sup  right   
en
le
it would be nice to be able to
call this an inputoutput equation but it is not
for the input signals to the lattice stage are x sup  and y sup 
and the outputs are x sup  and y sup 
we have written it in this form because it allows a multistage lattice to
be described by cascading these matrix equations
pp
a singlestage lattice filter has y sup  and y sup  connected together
forming its output call this x sub output while the input is x sup 
x sub input
hence the input is related to the output by
lb
eq
left  matrix ccol x sub input above sq  right   
 left  matrix ccol  above k ccol k z sup 
above z sup  right 
 left  matrix ccol x sub output above x sub output right   
en
le
so
lb
eq
x sub input     k z sup  x sub output 
en
le
or
lb
eq
x sub output over x sub input   over  k sub  z sup   
en
le
the symbol sq is used here and elsewhere
to indicate an unimportant element of a vector
or matrix  this certainly has the form of a linear predictive
synthesis filter which is
lb
eq
xz over ez   over  sum from k to p a sub k
z sup k  over a sub  z sup   
en
when p
le
pp
the behaviour of a secondorder lattice filter shown in figure 
can be described by
lb
ne
eq
left  matrix ccol x sub  sup  above x sub  sup  right   
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol x sub  sup  above x sub  sup  right 
en
sp
ne
eq
left  matrix ccol x sub  sup  above x sub  sup  right   
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol x sub  sup  above x sub  sup  right 
en
le
with
lb
ne
eq
x sub  sup  x sub input
en
br
eq
x sub  sup   x sub  sup   x sub output 
en
le
fc figure 
x sub  sup  and x sub  sup  can be eliminated by substituting the
second equation into the first which yields
lb
eq
left  matrix ccol x sub input above sq  right   mark 
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol x sub output above x sub output right 
en
sp
sp
eq
lineup   left  matrix ccol k sub  k sub  z sup  above sq 
ccol  k sub  z sup  k sub  z sup  above sq  right 
 left  matrix ccol x sub output above x sub output right   
en
le
this leads to an inputoutput relationship
lb
eq
x sub output over x sub input   
 over k sub  k sub  z sup  k sub  z sup   
en
le
which has the required form namely
lb
eq
 over  sum from k to p a sub k z sup k   p
en
le
when
lb
eq
a sub  k sub  k sub  
en
br
eq
a sub  k sub 
en
le
pp
a thirdorder filter is described by
lb
eq
left  matrix ccol x sub input above sq  right   
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol  above k sub   ccol k sub  z sup 
above z sup  right 
 left  matrix ccol x sub output above x sub output right   
en
le
and brave souls can verify that this gives an inputoutput
relationship
lb
eq
x sub output over x sub input    
 over k sub  k sub   k sub  k sub   z sup  
k sub  k sub  k sub   k sub   z sup   k sub  z sup    
en
le
it is fairly obvious that a pth order lattice filter will give the
required allpole pth order synthesis form
lb
eq
 over   sum from k to p a sub k z sup k   
en
le
pp
we have not shown that the algorithm given in procedure  for producing
reflection coefficients from filter coefficients gives those values
for k sub i which are necessary to make the lattice filter equivalent
to the ordinary synthesis filter  however this is the case and it is
easy to verify by hand for the first second and thirdorder cases
rh different lattice configurations
the lattice filters of figures   and  have two multipliers
per section
this is called a twomultiplier configuration
however there are other configurations which achieve
the same effect but require different numbers of multiplies
figure  shows onemultiplier and fourmultiplier configurations
along with the familiar twomultiplier one
fc figure 
it is easy to verify that the three configurations can be modelled in
matrix terms by
lb
ne

left  matrix ccol x sup  above x sup  right    
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol y sup  above y sup  right 
twomultiplier configuration
sp
sp
ne

left  matrix ccol x sup  above x sup  right    
left  k over k right  sup  
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol y sup  above y sup  right 
onemultiplier configuration
sp
sp
ne

left  matrix ccol x sup  above x sup  right    
 over k sup  sup  
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol y sup  above y sup  right 
fourmultiplier configuration
le
each of the three has the same frequencydomain response although
a different constant factor is involved in each case
the effect of this can be annulled by performing a single multiply
operation on the output of a complete lattice chain
the multiplier has the form
lb
eq
left    k sub p over   k sub p 
  k sub p over   k sub p 
  k sub  over   k sub  right  sup 
en
sp
le
for singlemultiplier lattices and
lb
eq
left   over   k sub p sup  
 over   k sub p sup  
 over   k sub  sup  right  sup 
en
le
for fourmultiplier lattices where the reflection coefficients
in the lattice are k sub p k sub p  k sub 
pp
there are important differences between these three configurations
if multiplication is timeconsuming the onemultiplier model has obvious
computational advantages over the other two methods
however the fourmultiplier structure behaves substantially better
in finite wordlength implementations  it is easy to show that with this
configuration
lb
eq
x sup   sup   y sup   sup    
x sup   sup   z sup  y sup   sup  
en
le
em a relationship which suggests that the energy in the
the input signals namely  x sup  and y sup   is preserved in the output
signals  x sup  and y sup 
notice that care must be taken with the ztransforms since squaring is a
nonlinear operation  z sup  y sup   sup   means the square of
the previous value of  y sup   which is not the same
as  z sup  y sup   sup 
pp
it has been shown gray and markel  that the fourmultiplier
configuration has some stability properties which are not shared by other
digital filter structures

gray markel  normalized digital filter structure

when a linear predictive filter is used for synthesis the parameters
of the filter em the kparameters in the case of lattice filters
and the aparameters in the case of direct ones em change with time
it is usually rather difficult to guarantee stability in the case of
timevarying filter parameters but some guarantees can be made for a
chain of fourmultiplier lattices  furthermore if the input is a
discrete delta function the cumulative energies at each stage of the
lattice are the same and so maximum dynamic range will be achieved
for the whole filter if each section is implemented with the same
word size
rh lattice analysis
it is quite easy to construct a filter which is inverse to
a singlestage lattice
the structure of figure a does the job
ignore for a moment
the dashed lines connecting figure a and b  its matrix transfer
function is
fc figure 
lb
ne

left  matrix ccol y sup  above y sup  right  
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol x sup  above x sup  right 
analysis lattice figure a
le
notice that this is exactly the same as the transfer function of the
synthesis lattice of figure  which is reproduced
in figure b except that the xs and ys are reversed
lb
ne

left  matrix ccol x sup  above x sup  right  
left  matrix ccol  above k ccol kz sup  above z sup  right 
 left  matrix ccol y sup  above y sup  right 
synthesis lattice figure b
le
or in other words
lb
ne

left  matrix ccol y sup  above y sup  right    
left  matrix ccol  above k ccol kz sup  above z sup 
right  sup 
 left  matrix ccol x sup  above x sup  right 
synthesis lattice figure b
le
hence if the filters of figures a and b were connected together
as shown by the dashed lines they
would cancel each other out and the overall transfer would be unity
lb
ne
eq
left  matrix ccol  above k ccol kz sup  above z sup 
right  
left  matrix ccol  above k ccol kz sup  above z sup 
right  sup    
left  matrix ccol  above  ccol  above  right   
en
le
actually such a connection is not possible in physical terms
for although the upper paths can be joined together the lower ones can not
the righthand lower point of figure a is an
ul
output
terminal and so is the lefthand lower one of figure b  however
there is no need to envisage a physical connection of the lower paths
it is sufficient for cancellation just to assume that the signals at both
of the points turn out to be the same
pp
and they do
the general case of a pstage analysis lattice
connected to a pstage synthesis
lattice is shown in figure 
fc figure 
notice that the forward and backward paths are connected together at both
of the extreme ends of the system
it is not difficult to show that under these
conditions the signal at the lower righthand
terminal of the analysis chain will equal that at the lower lefthand
terminal of the synthesis chain even though they are not connected
provided the upper terminals are connected together as shown by the dashed
line
of course the reflection coefficients  k sub  k sub  
k sub p  in the analysis lattice must equal those in the synthesis
lattice and as figure  shows the order is reversed in the synthesis
lattice
successive analysis and synthesis sections pair off working from
the middle outwards  at each stage the sections cancel each other out
giving a unit transfer function as demonstrated above
rh estimating reflection coefficients
as stated earlier in this chapter the key problem in linear prediction is to
determine the values of the predictive coefficients em in this case the
reflection coefficients
if this is done correctly we have shown using procedure  that
the the synthesis part of figure  performs the same calculation that
a conventional directform linear predictive synthesizer would and hence
the signal that excites it em that is the signal represented by the
dashed line em must be the prediction residual or error signal discussed
earlier  the system is effectively the same as the highorder adaptive
differential pulse code modulation one of figure 
pp
one of the most interesting features of the lattice structure for
analysis filters is that calculation of suitable values for the
reflection coefficients can be done locally at each stage of the lattice
for example consider the ith section of the analysis lattice in
figure   it is possible to determine a suitable value of k sub i
simply by performing a calculation on the inputs to the ith
section ie x sup  and x sup  in figure 
no longer need the complicated global optimization technique of matrix
inversion be used as in the autocorrelation and covariance methods discussed
earlier
pp
a suitable value for k in the single lattice section of figure  is
lb
eq
k   e x sup  n x sup  n over
 e x sup  n sup   e x sup  n sup    sup   
en
le
that is the statistical correlation between x sup  n and
x sup  n
here x sup  n and x sup  n represent the input signals to the
upper and lower paths recall that x sup  and x sup 
are their ztransforms
x sup  n is just x sup  n delayed by one time unit that is
the output of the z sup  box in the figure
pp
the criterion of optimality for the autocorrelation and covariance methods
was that the prediction error that is the signal which emerges from
the righthand end of the upper path of a lattice analysis filter
should be minimized in a meansquare sense
the reflection coefficients obtained from the above formula do not necessarily
satisfy any such global minimization criterion
nevertheless they do keep the error signal small and have been used with
success in speech analysis systems
pp
it is easy to minimize the output from either the upper or the lower path
of the lattice filter at each stage  for example the ztransform of the
upper output is given by
lb
eq
y sup   x sup   k z sup  x sup  
en
le
or
lb
eq
y sup  n  x sup  n  k x sup  n 
en
le
hence
lb
eq
ey sup  n sup      ex sup  n sup   
kex sup  n x sup  n   k sup  e x sup  n sup   
en
le
where e stands for expected value and this reaches a minimum when the
derivative with respect to k becomes zero
lb
eq
ex sup  n x sup  n   kex sup  n sup    
en
le
that is when
lb
eq
k   ex sup  n x sup  n  over ex sup  n sup  
  
en
le
a similar calculation shows that the output of the lower path is minimized
when
lb
eq
k   ex sup  n x sup  n  over ex sup  n sup  
  
en
le
unfortunately either of these expressions can exceed  leading to an
unstable filter
the value of k cited earlier is the geometric mean of these two
expressions and since it is a correlation coefficient must be less than 
pp
another possibility is to minimize the expected value of the sum of the
squares of the upper and lower outputs
lb
eq
y sup  n sup   y sup  n sup    
k sup  x sup  n sup   kx sup  n x sup  n 
k sup  x sup  n sup  
en
le
taking expected values and setting the derivative with respect to k to zero
leads to
lb
eq
k   ex sup  n x sup  n  over
 half  ex sup  n sup   x sup  n sup   
en
le
this also is guaranteed to be less than  and has given good results
in speech analysis systems
pp
figure  shows the implementation of a single section of an analysis
lattice
fc figure 
the signals x sup  n and x sup  n are fed to a
correlator which produces a suitable value for k
this value is used to calculate the output of the lattice section
and hence the input to the next lattice section
the reflection coefficient needs to be lowpass filtered because it will
only be transmitted to the synthesizer occasionally say every  msec and so a
shortterm average is required
pp
one implementation of the correlator is shown in figure  kang 

kang 

fc figure 
this calculates the value of k given by the last equation above and does it
by summing and differencing the two
signals x sup  n and x sup  n squaring the results to give
lb
eq
x sup  n sup   x sup  n mark  x sup  n x sup  n sup 
 x sup  n sup   x sup  n x sup  n x sup  n sup 
 
en
le
and summing and differencing these to yield
lb
eq
lineup x sup  n sup   x sup  n sup  
x sup  n x sup  n  
en
le
sp
before these are divided to give the final coefficient k they are
individually lowpass filtered
while some rather complex schemes have been proposed
based upon kalman filter theory eg matsui
ul
et al


matsui nakajima suzuki omura 

a simple exponential weighted past average has been found to be
satisfactory  this has ztransform
lb
eq
 over    z sup   
en
le
that is in the time domain
lb
eq
yn    over   yn   over   yn  
en
le
this filter exponentially averages past sample values
with a timeconstant of  sampling intervals
em that is  msec at an  khz sampling rate
sh   pitch estimation
pp
it is sometimes useful to think of linear prediction as a kind of
curvefitting technique
figure  illustrates how four samples of a speech signal can predict
the next one
fc figure 
in essence a curve is drawn through four points
to predict the position of the fifth and only the prediction error
is actually transmitted  now if the order of linear prediction
is high enough at least  and if the coefficients are chosen
correctly the prediction will closely model the resonances of the
vocal tract  thus the error will actually be zero except at pitch
pulses
pp
figure  shows a segment of voiced speech together with the prediction
error often called the prediction residual
fc figure 
it is apparent that the
error is indeed small except at pitch pulses
this suggests that a good way to determine the pitch period is to examine
the error signal perhaps by looking at its autocorrelation function
as with all pitch detection methods one must be
careful  spurious peaks can occur especially in nasal sounds when
the allpole model provided by linear prediction fails  continuity
constraints which use previous values of pitch period when determining
which peak to accept as a new pitch impulse can eliminate many of these
spurious peaks  unvoiced speech should produce an error signal with no
prominent peaks and this needs to be detected
voiced fricatives are a difficult case  peaks should be present
but the general noise level of the error signal will be greater than
it is in
purely voiced speech
such considerations have been taken into account in a practical pitch
estimation system based upon this technique markel 

markel  sift

pp
this method of pitch detection highlights another advantage of the lattice
analysis technique  when using autocorrelation or covariance analysis to
determine the filter or reflection coefficients the error signal is not
normally produced  it can of course be found by taking the speech samples
which constitute the current frame and running them through an analysis
filter whose parameters are those determined by the analysis but this
is a computationally demanding exercise for the filter must run at the
speech sampling rate say  khz instead of at the frame rate say  hz
usually pitch is estimated by other methods like those discussed in
chapter  when using autocorrelation or covariance linear prediction
however we have seen above that with the lattice method the error
signal is produced as a byproduct  it appears at the righthand end
of the  upper path of the lattice chain  thus it is already available
for use in determining pitch periods
sh   parameter coding for linear predictive storage or transmission
pp
in this section the coding requirements of linear predictive parameters
will be examined  the parameters that need to be stored or transmitted
are
lb
np
pitch
np
voicedunvoiced flag
np
overall amplitude level
np
filter coefficients or reflection coefficients
le
the first three are parameters of the excitation source
they can be derived directly from the error signal as indicated above if
it is generated as it is in lattice implementations or by other
methods if no error signal is calculated
the filter or reflection coefficients are of course the main product
of linear predictive analysis
pp
it is generally agreed that around  levels logarithmically spaced
are needed to represent pitch for telephone quality speech
the voicedunvoiced indication requires one bit but since pitch is
irrelevant in unvoiced speech it can be coded as one of the pitch
levels  for example with bit coding of pitch the value  can be
reserved to indicate unvoiced speech with values  indicating the
pitch of voiced speech
the overall gain has not been discussed above  it is simply the average
amplitude of the error signal  five bits on a logarithmic scale
are sufficient to represent it
pp
filter coefficients are not very amenable to quantization  at least
 bits are required for each one  however reflection coefficients
are better behaved and  bits each seems adequate  the number of
coefficients that must be stored or transmitted is the same as the
order of the linear prediction   is commonly used for lowquality
speech with as many as  for higher qualities
pp
these figures give around  bitsframe for a th order system using
filter coefficients and around  bitsframe for a th order system
using reflection coefficients  frame lengths vary between  msec
and  msec depending on the quality desired  thus for  msec frames
the data rates work out at around  bits using filter coefficients
and  bits using reflection coefficients
pp
substantially lower data rates can be achieved by more careful
coding of parameters  in  the us government defined a standard
coding scheme for pole linear prediction with a data rate of
 bits em conveniently chosen as one of the
commonlyused rates for serial data transmission
this standard called lpc tackles the difficult problem of
protection against transmission errors fussell
ul
et al


fussell boudra abzug cowing 

pp
whenever data rates are reduced redundancy inherent in the signal is
necessarily lost and so the effect of transmission errors becomes
greatly magnified
for example a single corrupted sample in pcm transmission of speech
will probably not be noticed and even a short burst of errors will be
perceived as a click which can readily be distinguished from the speech
however any error in lpc transmission will last for one entire
frame em say  msec em and worse still it will be integrated into the
speech signal and not easily discriminated from it by the listeners brain
a single corruption may for example change a voiced frame into an
unvoiced one or vice versa  even if it affects only 
a reflection coefficient it will change the resonance characteristics
of that frame and change them in a way that does not simply sound like
superimposed noise
pp
table  shows the lpc coding scheme
rf
ini
ta i i i
nr x wvoiced sounds
nr x wunvoiced sounds
ul
hnxuvoiced soundshnxuunvoiced sounds
sp
pitchvoicing pitch levels hamming
hw uand gray coded
energylogarithmically coded
k sub coded by table lookup
k sub coded by table lookup
k sub 
k sub 
k sub 
k sub 
k sub 
k sub 
k sub 
k sub 
synchronizationalternating  pattern
error detectionhwu
correction
hwuwuhwuwu
sp
hwuhwu
sp
ta i i i i i i i i i i i i
frame rate  hz  msec frames
in 
fg table   bit requirements for each parameter in lpc coding scheme
different coding is used for voiced and unvoiced frames
only four reflection coefficients are transmitted for unvoiced frames
because it has been determined that no perceptible increase in speech quality
occurs when more are used
the bits saved are more fruitfully employed to provide error detection
and correction for the other parameters
seven bits are used for pitch and the voicedunvoiced flag and they are
redundant in that only  possible pitch values are
allowed
most transmission errors in this field will be detected by the receiver
which can then use an estimate of pitch based on previous values and
discard the erroneous one  pitch values are also gray coded so that
even if errors are not detected there is a good chance that an adjacent
pitch value is read instead
different numbers of bits are allocated to the various reflection
coefficients  experience shows that the lowernumbered ones contribute
most highly to intelligibility and so these are quantized most finely
in addition a table lookup operation is performed on the code
generated for the first two providing a nonlinear quantization which is
chosen to minimize the error on a statistical basis
pp
with  bitsframe and  msec frames lpc requires a  bits
data rate  even lower rates have been used successfully for lowerquality
speech  the speak n spell toy described in chapter  has an
average data rate of  bits  rates as low as  bits have
been achieved kang and coulter  by pattern recognition techniques operating
on the reflection coefficients  however the speech quality is not good

kang coulter 

sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
most recent books on digital signal processing contain some information
on linear prediction see oppenheim and schafer  rabiner and gold 
and rabiner and schafer  all referenced at the end of chapter 
lb nn
atal

ds a atal bs
as a  and hanauer sl
ds d 
ds t speech analysis and synthesis by linear prediction of the acoustic wave
ds j jasa
ds v 
ds p 
nr p 
ds o august
nr t 
nr a 
nr o 
  journalarticle
inn
this paper is of historical importance because it introduced the idea
of linear prediction to the speech processing community
inn
makhoul

ds a makhoul ji
ds d 
ds k 
ds t linear prediction a tutorial review
ds j proc ieee
ds v 
ds n 
ds p 
nr p 
ds o april
nr t 
nr a 
nr o 
  journalarticle
inn
an interesting informative and readable survey of linear prediction
inn
markel

ds a markel jd
as a  and gray ah
ds d 
ds t linear prediction of speech
ds i springer verlag
ds c berlin
nr t 
nr a 
nr o 
  book
inn
this is the only book which is entirely devoted to linear prediction of speech
it is an essential reference work for those interested in the subject
inn
wiener

ds a wiener n
ds d 
ds t extrapolation interpolation and smoothing of stationary time series
ds i mit press
ds c cambridge massachusetts
nr t 
nr a 
nr o 
  book
inn
linear prediction is often thought of as a relatively new technique
but it is only its application to speech processing that is novel
wiener develops all of the basic mathematics used in linear prediction
of speech except the lattice filter structure
inn
le nn
eq
delim 
en
ch   joining segments of speech
ds rt joining segments of speech
ds cx principles of computer speech
pp
the obvious way to provide speech output from computers
is to select the basic acoustic units to be used record them
and generate utterances by concatenating together appropriate segments
from this prestored inventory
the crucial question then becomes what are the basic units
should they be whole sentences words syllables or phonemes
pp
there are several tradeoffs to be considered here
the larger the units the more utterances have to be stored
it is not so much the length of individual utterances that is of concern
but rather their variety which tends to increase exponentially instead
of linearly with the size of the basic unit  numbers provide an
easy example  there are  sup  digit telephone numbers and it is
certainly infeasible to record each one individually
note that as storage technology improves the limitation is becoming
more and more one of recording the utterances in the first place rather
than finding somewhere to store them
at a pcm data rate of  kbits a  mbyte disk can hold over  hours
of continuous speech
with linear predictive coding at  kbits it holds  of a
megasecond em well over a week  and this is a hour day week
which corresponds to a working month and continuous speech em without
pauses em which probably requires another factor of five for
production by a person
setting up a recording session to fill the disk would be a formidable
task indeed
furthermore the use of videodisks em which will be common domestic items
by the end of the decade em could increase these figures by a factor of 
pp
the word seems to be a sensiblysized basic unit
many applications use a rather limited vocabulary em  words
for the airline reservation system described in chapter 
even at pcm data rates this will consume less than  mbyte of
storage
unfortunately coarticulation and prosodic factors now come into play
pp
real speech is connected em there are few gaps between words
coarticulation where sounds are affected by those on either side
naturally operates across word boundaries
and the time constants of coarticulation are associated with the
mechanics of the vocal tract and hence measure tens or hundreds
of msec  thus the effects straddle several pitch periods  hz pitch
has  msec period and cannot be simulated by simple interpolation of the
speech waveform
pp
prosodic features em notably pitch and rhythm em span much longer
stretches of speech than single words  as far as most speech output
applications are concerned they operate at the utterance level of
a single sentencesized information unit  they cannot be
accomodated if speech waveforms of individual words of
the utterance are stored
for it is rarely feasible to alter the fundamental
frequency or duration of a time waveform without changing all the formant
resonances as well
however both wordtoword coarticulation and the essential features
of rhythm and intonation can be incorporated if the stored words are
coded in sourcefilter form
pp
for more general applications of speech output the limitations of
word storage soon become apparent  although peoples daily
vocabularies are not large most words have a variety
of inflected forms which need to be treated separately if a strict
policy is adopted of word storage  for instance in this book
there are  words and   different ones counting
inflected forms
in chapter  alone there are  words and   different ones
pp
it seems crazy to treat a simple inflection like s or its voiced
counterpart z as in inflectionc
ul
sc

as a totally different word from the base form
but once you consider storing roots and endings separately
it becomes apparent
that there is a vast number of different endings and it is difficult to know
where to draw the line  it is natural to think instead of simply
using the syllable as the basic unit
pp
a generous estimate of the number of different syllables in english is 
at three a second only about an
hours storage is required for them all  but waveform storage
will certainly not do
although coarticulation effects between words are needed to make
speech sound fluent coarticulation between syllables is necessary
for it even to be
ul
comprehensible
adopting a sourcefilter form of representation is essential as is
some scheme of interpolation between syllables which simulates
coarticulation
unfortunately a great deal of acoustic action occurs at syllable
boundaries em stops are exploded the sound source changes
between voicing and frication and so on  it may be more appropriate
to consider inverse syllables comprising a vowelconsonantvowel sequence
instead of consonantvowelconsonant
these have jokingly been dubbed lisibles
pp
there is again some considerable practical difficulty in creating
an inventory of syllables or lisibles
now it is not so much the recording that is impractical but
the editing needed to ensure that the cuts between syllables are made
at exactly the right point  as units get smaller the exact
placement of the boundaries becomes ever more critical and several thousand
sensitive editing jobs is no easy task
pp
since quite general effects of coarticulation must be accomodated
with syllable synthesis there will not necessarily be significant
deterioration if smaller demisyllable units are employed
this reduces the segment inventory to an estimated  entries
and the tedious job of editing each one individually becomes at
least feasible if not enviable
alternatively the segment inventory could be created by artificial
means involving cutandtry experiments with resonance parameters
pp
the ultimate in economy of inventory size of course is to use
phonemes as the basic unit  this makes the most critical
part of the task interpolation between units rather than their
construction or recording  with only about  phonemes
in english each one can be examined in many different contexts to
ascertain the best data to store
there is no need to record them directly from a human voice em it
would be difficult anyway for most cannot be produced in isolation
in fact a phoneme is an abstract unit not a particular sound
recall the discussion of phonology in chapter  and so it is
most appropriate that data be abstracted from several different
realizations rather than an exact record made of any one
pp
if information is stored about phonological units of
speech em phonemes em the difficult task of phonologicaltophonetic
conversion must necessarily be performed automatically
allophones are created by altering the transitions between units
and to a lesser extent by modifying the central parts of the units
themselves
the rules for making transitions will have a big effect on the
quality of the resulting speech
instead of trying to perform this task automatically by a computer
program the allophones themselves could be stored  this will
ease the job of generating transitions between segments but
will certainly not eliminate it
the total number of allophones will depend on the narrowness of the
transcription system   is typical and it is unlikely to exceed
one or two hundred  in any case there will not be a storage problem
however now the burden of producing an allophonic transcription
has been transferred to the person who codes the utterance prior
to synthesizing it  if he is skilful and patient he should
be able to coax the system into producing fairly understandable
speech but the effort required for this on a perutterance basis
should not be underestimated
rf
nr x wsentences  
nr x w  
nr x wdepends on  
nr x wgeneralized or  
nr x wnatural speech  
nr x wauthor of segment
nr x nxunxunxunxunxunxu
nr x nlnx
in nxu
ta nxu nxu nxu nxu nxu
size ofstoragesource ofprincipal
utterancemethodutteranceburden is
inventoryinventoryplaced on
hilnxuul

sentencesdepends onwaveform ornatural speechrecording artist
applicationsourcefilterstorage medium
parameters

wordsdepends onsourcefilternatural speechrecording artist
applicationparametersand editor
storage medium

syllablessourcefilternatural speechrecording editor
  lisiblesparameters

demisourcefilternatural speechrecording editor
  syllablesparametersor artificiallyor inventory
generatedcompiler

phonemesgeneralizedartificiallyauthor of segment
parametersgeneratedconcatenation
program

allophonesgeneralized orartificiallycoder of
sourcefiltergenerated orsynthesized
parametersnatural speechutterances
hilnxuul
in 
ta i i i i i i i i i i i i
fg table   some issues relevant to choice of basic unit
pp
table  summarizes in broad brushstrokes the issues which relate to the
choice of basic unit for concatenation
the sections which follow provide more detail about the different
methods of joining segments of speech together
only segmental aspects are considered for the important problems of
prosody will be treated in the next chapter
all of the methods rely to some extent on the acoustic properties of speech
and as smaller basic units are considered the role of speech acoustics
becomes more important
it is impossible in a book like this to give a detailed account of acoustic
phonetics for it would take several volumes
what i aim to do in the following pages is to highlight some salient features
which are relevant to segment concatenation without attempting to be
complete
sh   word concatenation
pp
for general speech output word concatenation is an inherently limited
technique because of the large number of phonetically different words
despite this fact it is at present the most widelyused synthesis
method and is likely to remain so for several years
we have seen that the primary problems are wordtoword
coarticulation and prosody and both can be overcome at least to a useful
approximation by coding the words in sourcefilter form
rh timedomain techniques
nevertheless a surprising number of applications simply store
the time waveform coded usually by one of the techniques described in
chapter 
from an implementation point of view there are many advantages to this
speech quality can easily be controlled by selecting a suitable sampling
rate and coding scheme
a naturalsounding voice is guaranteed male or female as desired
the equipment required is minimal em a digitaltoanalogue
converter and postsampling filter will do for synthesis if
pcm coding is used and
dpcm adpcm and delta modulation decoders are not much more complicated
pp
from a speech point of view the resulting utterances can never be made
convincingly fluent
we discussed the early experiments of stowe and hampton 
at the beginning of chapter 

stowe hampton 

a major drawback to word concatenation in the
analogue domain is the introduction of clicks and other interference
between words  it is difficult to prevent the time waveform transitions
from adding extraneous sounds
this poses no problem with digital storage however for the waveforms
can be edited accurately prior to storage so that they start
and finish at an exactly
zero level
rather the lack of fluency stems from the absence of proper control
of coarticulation and prosody
pp
but this is not necessarily a serious drawback if the application is
a sufficiently limited one  complete invariant utterances can be
stored as one unit  often they must contain datadependent
slotfillers as in
lb
this flight makes em stops
le
and
lb
flight number em leaves em at em  arrives in em at em
le
taken from the airline reservation system of chapter 
levinson and shipley 

levinson shipley 

then each slotfilling word is recorded in an intonation consistent
both with its position in the template utterance and with the
intonation of that utterance
this could be done by embedding the word in the utterance
for recording and excising it by digital editing before storage
it would be dangerous to try to take into account coarticulation effects
for the coarticulation could not be made consistent with both the
several slotfillers and the single template
this could be overcome if several versions of the template were stored
but then the scheme becomes subject to combinatorial explosion
if there is more than one slot in a single utterance
but it is not really necessary for the lack of fluency will probably
be interpreted by a benevolent listener as an attempt to convey the
information as clearly as possible
pp
difficulties will occur if the same slotfiller is used in different
contexts  for instance the first gap in each of the sentences above
contains a number yet the intonation of that number is different
many systems simply ignore this problem
then one does notice anomalies if one is attentive  the words come
as it were from different mouths without fluency
however the problem is not necessarily acute  if it is two or more
versions of each slotfiller can be recorded one for each context
pp
as an example consider the synthesis of digit telephone numbers
like   if one version only of each digit is stored
it should be recorded in a level tone of voice  a pause should be
inserted after the third digit of the synthetic number to accord
with common elocution  the result will certainly be unnatural although
it should be clear and intelligible
any pitch errors in the recordings will make certain numbers
audibly anomalous
at the other extreme  single digits could be stored one version of
each digit for each position in the number  the recording will be
tedious and errorprone and the synthetic utterances will still not
be fluent em for coarticulation is ignored em but instead
unnaturally clearly enunciated  a compromise is to record only
three versions of each digit one for any of the
five positions
nr x wul
nr x nx
nr x m
zxhnxuzxhnxuhnxuzhnxuzxhnxuzxhnxuc
zxhnxuhnxuvnxulnxuulvnxu 
another one for the third position
hnxuhnxuzxhnxuzhnxuhnxuc
hnxuhnxuhnxuvnxulnxuulvnxu 
and the last for the final position
hnxuhnxuhnxuzhnxuhnxuc
hnxuhnxuzxhnxuvnxulnxuulvnxu 
the first version will be in a level voice the second an
incomplete rising tone and the third a final dropping pitch
rh joining formantcoded words
the limitations of the timedomain method are lack of
fluency caused by unnatural transitions between words and the
combinatorial explosion created by recording slotfillers several times
in different contexts
both of these problems can be alleviated by storing formant tracks
concatenating them with suitable interpolation and applying a complete
pitch contour suitable for the whole utterance
but one can still not generate conversational speech for natural speech
rhythms cause nonlinear warpings of the time axis which cannot reasonably
be imitated by this method
pp
solving problems often creates others
as we saw in chapter  it is not easy to obtain reliable formant tracks
automatically  yet handediting of formant parameters adds a whole new
dimension to the problem of vocabulary construction for it is
an exceedingly tiresome and timeconsuming task
even after such tweaking resynthesized utterances will be degraded
considerably from the original for the sourcefilter model is by no means
a perfect one
a hardware or realtime software formant synthesizer must be added
to the system presenting design problems and creating extra cost
should a serial or parallel synthesizer be used em the latter offers
potentially better speech especially in nasal sounds but requires
additional parameters namely formant amplitudes to be estimated
finally as we will see in the next chapter it is not an easy matter to
generate a suitable pitch contour and apply it to the utterance
pp
strangely enough the interpolation itself does not present any great
difficulty for there is not enough information in the formantcoded
words to make possible sophisticated coarticulation
the need for interpolation is most pressing when one word ends with
a voiced sound and the next begins with one
if either the end of the first or the beginning of the second word
or both is unvoiced unnatural formant transitions do not matter
for they will not be heard
actually this is only strictly true for fricative transitions  if
the juncture is aspirated then formants will be perceived in the
aspiration  however
ul
h
is the only fully aspirated sound in english
and it is relatively uncommon
it is not absolutely necessary to interpolate the fricative filter resonance
because smooth transitions from one fricative sound to another are rare
in natural speech
pp
hence unless both sides of the junction are voiced no interpolation
is needed  simple abuttal of the stored parameter tracks will do
note that this is
ul
not
the same as joining time waveforms for the synthesizer
will automatically ensure a relatively smooth transition from one
segment to another because of energy storage in the filters
a new set of resonance parameters for the formantcoded words will be stored
every  or  msec see chapter  and so the transition will automatically
be smoothed over this time period
pp
for voicedtovoiced transitions some interpolation is needed
an overlap period of duration say  msec is established and
the resonance parameters in the final  msec of the first word are
averaged with those in the first  msec of the second
the average is weighted with the first words formants dominating
at the beginning and their effect progressively dying out
in favour of the second word
pp
more sophisticated than a simple average is to weight the components
according to how rapidly they are changing
if the spectral change in one word is much greater than that in the
other we might expect that this will dominate the transition
a simple measure of spectral derivative at any given time can be found
by adding the magnitude of the discrepancies in each formant frequency
between one sample and the next
the spectral change in the transition region can be obtained by summing
the spectral derivatives at each sample in the region
such a measure can perhaps be made more accurate by taking into
account the relative importance of the formants but will probably
never be more than a rough and ready yardstick
at any rate it can be used to load the average in favour of the
dominant side of the junction
pp
much more important for naturalness of the speech are the effects
of rhythm and intonation discussed in the next chapter
pp
such a scheme has been implemented and tested on em guess what em digit
telephone numbers rabiner
ul
et al


rabiner schafer flanagan 

significant improvement at the  level of statistical
significance in peoples
ability to recall numbers was found for this method over direct
abuttal of either natural or synthetic versions of the digits
although the method seemed on balance to produce utterances that were
recalled less accurately than completely natural spoken
telephone numbers the difference was not significant at the  level
the system was also used to generate wiring instructions by computer
directly from the connection list as described in chapter 
as noted there synthetic speech was actually preferred to natural speech
in the noisy environment of the production line
rh joining linear predictive coded words
because obtaining accurate formant tracks for natural utterances
by fourier transform methods is difficult it is worth considering
the use of linear prediction as the sourcefilter model
actually formant resonances can be extracted from linear predictive
coefficients quite easily but there is no need to do this because
the reflection coefficients themselves are quite suitable
for interpolation
pp
a slightly different interpolation scheme from that described in the
previous section has been reported olive 

olive 

the reflection coefficients were spliced during an overlap region of
only  msec
more interestingly attempts were made to suppress the plosive bursts
of stop sounds in cases where they were followed by another stop at
the beginning of the next word
this is a common coarticulation occurring for instance in the phrase
stop burst  in running speech the plosion on the
ul
p
of stop is
normally suppressed because it is followed by another stop
this is a particularly striking case because the place of articulation
of the two stops
ul
p
and
ul
b
is the same  complete suppression is not as likely
to happen in stop gap for example although it may occur
here is an instance of how extra information could improve the
quality of the synthetic transitions considerably
however automatically identifying the place of articulation of stops is
a difficult job of a complexity far above what is appropriate for
simply joining words stored in sourcefilter form
pp
another innovation was introduced into the transition between two
vowel sounds when the second word began with an accented syllable
a glottal stop was placed at the juncture
although the glottal stop was not described in chapter  it is a sound
used in many dialects of english  it frequently occurs
in the utterance uhuh meaning no  here it
ul
is
used to separate two vowel sounds but in fact this is not particularly
common in most dialects
one could say the apple the orange the onion with a neutral vowel
in the to rhyme with c
ul
ac
bove and a glottal stop as separator
but it is much more usual to rhyme the with he and introduce a
ul
y
between the words
similarly even speakers who do not normally pronounce an
ul
r
at the
end of words will introduce one in bigger apple rather than
using a glottal stop
note that it would be wrong to put an
ul
r
in the apple even
for speakers who usually terminate the and bigger with the same sound
such effects occur at a high level of processing and are practically
impossible to simulate with wordinterpolation rules
hence the expedient of introducing a glottal stop is a good one although
it is certainly unnatural
sh   concatenating whole or partial syllables
pp
the use of segments larger than a single phoneme or allophone but smaller
than a word as the basic unit for speech synthesis has an interesting
history
it has long been realized that transitions between phonemes are
extremely sensitive and critical components of speech and thus are
essential for successful synthesis
consider the unvoiced stop sounds
ul
p t
and
ul
k
their central portion is actually silence  try saying a word like
butter with a very long
ul
tc
  hence
in this case it is
ul
only
the transitional information which can distinguish these sounds from
each other
pp
sound segments which comprise the transition from the centre of one phoneme
to the centre of the next are called
ul
dyads
or
ul
diphones
the possibility of using them as the basic units for concatenation
was first mooted in the mid s
the idea is attractive because there is relatively little spectral
movement in the central socalled steadystate portion of many
phonemes em in the extreme case of unvoiced stops there is not only
no spectral movement but no spectrum at all in the steady state
at that time the resonance synthesizer was in its infancy and
so recorded segments of live speech were used  the early experiments
met with little success because of the technical difficulties
of joining analogue waveforms and inevitable discrepancies between
the steadystate parts of a phoneme recorded in different contexts em not
to mention the problems of coarticulation and prosody which effectively
preclude the use of waveform concatenation at such a low level
pp
in the mid s with the growing use of resonance synthesizers
it became possible to generate diphones by copying resonance parameters
manually from a spectrogram and improving the result by trial and error
it was not feasible to extract formant frequencies automatically from real
speech though because the fast fourier transform was not yet widely
known and the computational burden of slow fourier transformation was
prohibitive
for example a project at ibm stored manuallyderived parameter tracks
for diphones identified by pairs of phoneme names dixon and maxey 

dixon maxey 

to generate a synthetic utterance it was coded in
phonetic form and used to access
the diphone table to give a set of parameter tracks for the complete
utterance  note that this is the first system we have encountered
whose input is a phonetic transcription which relates to an inventory
of truly synthetic character  all previous schemes used recordings of
live speech albeit processed in some form
since the inventory was synthetic there was no difficulty in ensuring
that discontinuities did not arise between segments beginning and ending with
the same phoneme  thus interpolation was irrelevant and the synthesis
procedure concentrated on prosodic questions  the resulting speech
was reported to be quite impressive
pp
strictly speaking diphones are not demisyllables but phoneme pairs
in the simplest case they happen to be similar for two primary diphones
characterize a consonantvowelconsonant syllable
there is an advantage to using demisyllables rather than diphones as the basic
unit for many syllables begin or end with complicated consonant clusters
which are not easy to produce convincingly by diphone
concatenation
but they are not easy to produce by handediting resonance parameters
either
now that speech analysis methods have been developed and refined
resonance parameters or linear predictive coefficients
can be extracted automatically
from natural utterances and there has been a resurgence of interest in
syllabic and demisyllabic synthesis methods  the wheel has turned
full circle from segments of natural speech to handtailored parameters
and back again
pp
the advantage of storing demisyllables over syllables or lisibles from
the point of view of storage capacity has already been pointed out
perhaps  demisyllables as opposed to  syllables
but it is probably not too significant with the continuing decline
of storage costs
the requirements are of the order of  kbyte versus  mbyte
for  bits linear predictive coding and the latter could
almost be accomodated today em  em on a stateoftheart
readonly memory chip
a bigger advantage comes from rhythmic considerations
as we will see in the next chapter the rhythms of fluent speech cause
dramatic variations in syllable duration but these seem to affect
the vowel and closing consonant cluster much more than the initial consonant
cluster  thus if a demisyllable is deemed to begin shortly say  msec
after onset of the vowel when the formant structure has settled down
the bulk of the vowel and the closing consonant cluster will form a
single demisyllable  the opening cluster of the next syllable will lie
in the next demisyllable  then differential lengthening can be applied
to that part of the syllable which tends to be stretched in live speech
pp
one system for demisyllable concatenation has produced excellent results
for monosyllabic english words lovins and fujimura 

lovins fujimura 

complex wordfinal consonant clusters are excluded from the inventory by
using syllable affixes
ul
s z t
and
ul
d
these are attached to the
syllabic core as a separate exercise macchi and nigro 

macchi nigro 

prosodic rather than segmental considerations are likely to prove the major
limiting factor when this scheme is extended to running speech
pp
monosyllabic words spoken in isolation are coded as linear predictive
reflection coefficients and segmented by digital editing into the initial
consonant cluster and the vocalic nucleus plus final cluster
the cut is made  msec into the vowel as suggested above
this minimizes the difficulty of interpolation when concatenating
segments for there is ample voicing on either side of the juncture
the reflection coefficients should not differ radically because the
vowel is the same in each demisyllable
a  msec overlap is used with the usual linear interpolation
an alternative smoothing rule applies when the second segment has
a nasal or glide after the vowel  in this case anticipatory coarticulation
occurs affecting even the early part of the vowel  for example a vowel
is frequently nasalized when followed by a nasal sound em even in english
where nasalization is not a distinctive feature in vowels see chapter 
under these circumstances the overlap area is moved forward in time so
that the colouration applies throughout almost the whole vowel
sh   phoneme synthesis
pp
acoustic phonetics is the study of how the acoustic
signal relates to the phonetic sequence which was spoken or heard
people em especially engineers em often ask how could phonetics not
be acoustic  in fact it can be articulatory auditory or linguistic
phonological for example and we have touched on the first and last
in chapter 
the invention of the sound spectrograph in the late s was an
event of colossal significance for acoustic phonetics for it somehow
seemed to make the intricacies of speech visible
this was thought to be a greater advance than actually turned
out  historicallyminded readers should refer to potter
ul
et al

for an enthusiastic contemporary appraisal of the invention  a

potter kopp green 

result of several years of research at haskins laboratories in new york
during the s was a set of minimal rules for synthesizing speech
which showed how stylized formant patterns could generate cues for
identifying vowels and particularly consonants
liberman  liberman
ul
et al


liberman  some results of research on speech perception


liberman ingemann lisker delattre cooper 

pp
these were to form the basis of many speech synthesisbyrule computer
programs in the ensuing decades  such programs take as input a
phonetic transcription of the utterance and generate a spoken version
of it  the transcription may be broad or narrow depending on the
system  experience has shown that the haskins rules really are
minimal and the success of a synthesisbyrule program depends on
a vast collection of minutia each seemingly insignificant in isolation
but whose effects combine to influence the speech quality dramatically
the best current systems produce clearly understandable
speech which is nevertheless something of a strain to listen to for
long periods
however many are not good and some are execrable
in recent times commercial influences have unfortunately restricted
the free exchange of results and programs between academic researchers
thus slowing down progress
research attention has turned to prosodic factors
which are certainly less well understood than segmental ones and
to synthesis from plain english text rather than from phonetic transcriptions
pp
the remainder of this chapter describes the techniques of segmental
synthesis  first it is necessary to introduce some
elements of acoustic phonetics
it may be worth rereading chapter  at this point to refresh
your memory about the classification of speech sounds
sh   acoustic characterization of phonemes
pp
shortly after the invention of the sound spectrograph an inverse
instrument was developed called the pattern playback synthesizer
this took as input a spectrogram either in its original form or
painted by hand
an optical arrangment was used to modulate the amplitude of some
fifty harmonicallyrelated oscillators by the lightness or darkness
of each point on the frequency axis of the spectrogram
as it was drawn past the playing head sound was produced which
had approximately the frequency components shown on the spectrogram
although the fundamental frequency was constant
pp
this device allowed the complicated
acoustic effects seen on a spectrogram see for example figures  and 
to be replayed in either original or simplified form
hence the features which are important for perception of the different sounds
could be isolated  the procedure was to copy from an actual spectrogram
the features which were most prominent visually and then to make further
changes by trial and error until the result was judged to have
reasonable intelligibility when replayed
pp
for the purpose of acoustic characterization of particular phonemes
it is useful to consider the central steadystate part separately from
transitions into and out of the segment
the steadystate part is that sound which is heard when the phoneme
is prolonged  the term phoneme is being used in a rather loose sense
here  it is more appropriate to think of a sound segment rather than
the abstract unit which forms the basis of phonological classification
and this is the terminology i will adopt
pp
the essential auditory characteristics of some sound segments are inherent in
their steady states
if a vowel for example is spoken and prolonged it can readily be
identified by listening to any part of the utterance
this is not true for diphthongs  if you say i very slowly and freeze
your vocal tract posture at any time the resulting steadystate sound
will not be sufficient to identify the diphthong  rather it will be
a vowel somewhere between
ul
aa
in had or
ul
ar
in hard and
ul
ee
in heed
neither is it true for glides for prolonging
ul
w
in want or
ul
y
in you results in vowels resembling respectively
ul
u
hood or
ul
ee
heed
fricatives voiced or unvoiced can be identified from the steady state
but stops can not for theirs is silent or em in the case
of voiced stops em something close to it
pp
segments which are identifiable from their steady state are easy to synthesize
the difficulty lies with the others for it must be the transitions which
carry the information  thus transitions are an essential part of speech
and perhaps the term is unfortunate for it calls to mind an unimportant
bridge between one segment and the next
it is tempting to use the words continuant and noncontinuant to distinguish
the two categories unfortunately they are used by phoneticians in a different
sense
we will call them steadystate and transient segments  the latter term
is not particularly appropriate for even sounds in this class
ul
can
be prolonged  the point is that the identifying information is in the
transitions rather than the steady state
rf
nr x wexcitation
nr x wformant resonance
nr x wfricative
nr x wfrequencies hz
nr x wresonance hz
nr x niiiiiwnx
nr x nlnx
in nxu
ta n i i i i i
hnxuexcitationhnxuformant resonancehnxufricative
hnxufrequencies hzc
hnxuresonance hz
lnxuul
sp
nr x wvoicing
fiuhfrthehnxuvoicing
fiafrbudhnxuvoicing
fiefrheadhnxuvoicing
fiifrhidhnxuvoicing
fiofrhodhnxuvoicing
fiufrhoodhnxuvoicing
fiaafrhadhnxuvoicing
fieefrheedhnxuvoicing
fierfrheardhnxuvoicing
fiarfrhardhnxuvoicing
fiawfrhoardhnxuvoicing
fiuufrfoodhnxuvoicing
nr x waspiration
fihfrhehnxuaspiration
nr x wfrication
nr x wfrication and voicing
fisfrsinhnxufrication
fizfrzedhnxufrication and voicing
fishfrshinhnxufrication
fizhfrvisionhnxufrication and voicing
fiffrfinhnxufrication
fivfrvathnxufrication and voicing
fithfrthinhnxufrication
fidhfrthathnxufrication and voicing
lnxuul
ta i i i i i i i i i i i i
in 
fg table   resonance synthesizer parameters for steadystate sounds
rh steadystate segments
table  shows appropriate values for the resonance parameters and
excitation sources of a resonance synthesizer for steadystate
segments only
there are several points to note about it
firstly all the frequencies involved obviously depend upon the
speaker em the size of his vocal tract his accent and speaking habits
the values given are nominal ones for a male speaker with a dialect of
british english called received pronunciation rp em for it is what
used to be received on the wireless in the old days
before the british broadcasting corporation
adopted a policy of more informal more regional speech
female speakers have formant frequencies approximately  higher
than male ones
secondly the third formant is relatively unimportant for vowel
identification it is
the first and second that give the vowels their character
thirdly formant values for
ul
h
are not given for they would be meaningless
although it is certainly a steadystate sound
ul
h
changes radically
in context  if you say had heed hud and so on and freeze
your vocal tract posture on the initial
ul
h
you will find it
already configured for the following vowel em an excellent
example of anticipatory coarticulation
fourthly amplitude values do play some part in identification
particularly for fricatives
ul
th
is the weakest sound closely followed by
ul
f
with
ul
s
and
ul
sh
the
strongest  it is necessary to get a reasonable mix of excitation in
the voiced fricatives the voicing amplitude is considerably less than
in vowels  finally there are other sounds that might be considered
steady state ones  you can probably identify
ul
m n
and
ul
ng
just by
their steady states  however the difference is not particularly
strong it is the transitional parts which discriminate most effectively
between these sounds  the steady state of
ul
r
is quite distinctive too
for most speakers because the top of the tongue is curled back in a
socalled retroflex action and this causes a radical change in the
third formant resonance
rh transient segments
transient sounds include diphthongs glides
nasals voiced and unvoiced stops and affricates
the first two are relatively easy to characterize for they are
basically continuous gradual transitions from one vocal tract posture
to another em sort of dynamic vowels  diphthongs and glides are
similar to each other  in fact you could be transcribed as
a triphthong
ul
i e uu
except that in the initial posture the tongue
is even higher and the vocal tract correspondingly more constricted
than in
ul
i
hid em though not as constricted as in
ul
sh
both categories can be represented in terms of target formant
values on the understanding that these are not to be
interpreted as steady state configurations but strictly as
extreme values at the beginning or end of the formant motion for
transitions out of and into the segment respectively
pp
nasals have a steadystate portion comprising a strong nasal formant
at a fairly low frequency on account of the large size of the
combined nasal and oral cavity which is resonating
higher formants are relatively weak because of attenuation effects
transitions into and out of nasals are strongly nasalized
as indeed are adjacent vocalic segments with
the oral and nasal tract operating in parallel  as discussed in
chapter  this cannot be simulated on a series synthesizer
however extremely fast motions of the formants occur on account of
the binary switching action of the velum and it turns out that
fast formant transitions are sufficient to simulate nasals because
the speech perception mechanism is accustomed to hearing them only
in that context  contrast this with the extremely slow transitions
in diphthongs and glides
pp
stops form the most interesting category and research using the pattern
playback synthesizer was instrumental in providing adequate acoustic
characterizations for them  consider unvoiced stops
they each have three phases  transition in silent central portion
and transition out  there is a lot of action on the transition out
and many phoneticians would divide this part alone into several phases
first as the release occurs there is a small burst of fricative noise
say t t t  as in tuttut without producing any voicing
actually when used as an admonishment this is accompanied by
an ingressive inhaling airstream instead of the normal egressive
exhaling one used in english speech although some languages
do have ingressive sounds
in any case a short fricative somewhat resembling a tiny
ul
s
can be heard as the tongue leaves the roof of the mouth
frication is produced when the gap is very narrow and ceases
rapidly as it becomes wider
next when an unvoiced stop is released a significant amount of aspiration
follows the release
say pot tot cot with force and you will hear the
ul
hc
like
aspiration quite clearly
it doesnt always occur though for example you will hear little
aspiration when a fricative like
ul
s
precedes the stop in the
same syllable as in spot scot  the aspiration is a distinguishing
feature between white spot and the rather unlikely whites pot
it tends to increase as the emphasis on the syllable increases
and this in an example of a prosodic feature influencing segmental
characteristics  finally at the end of the segment
the aspiration em if any em will turn to voicing
pp
what has been described applies to
ul
all
unvoiced stops
what distinguishes one from another
the tiny fricative burst will be different because the noise is produced
at different places in the vocal tract em at the lips for
ul
p
tongue and front of palate for
ul
t
and tongue and back of palate for
ul
k
the most important difference however is the formant motion illuminated
by the last vestiges of voicing at closure and by both aspiration and the
onset of voicing at opening
each stop has target formant values which although
they cannot be heard during the stopped portion for there is no
sound there do affect the transitions in and out
an added complexity is that the target positions themselves vary to some
extent depending on the adjacent segments
if the stop is heavily aspirated the vocal posture will have almost
attained that for the following vowel before voicing begins but
the formant transitions will be perceived because they affect
the sound quality of aspiration
pp
the voiced stops
ul
b d
and
ul
g
are quite similar to their unvoiced analogues
ul
p t
and
ul
k
what distinguishes them from each other are the formant transitions to
target positions heard during closure and opening
they are distinguished from their unvoiced counterparts by the fact
that more voicing is present  it lingers on longer at closure
and begins earlier on opening  thus little or no aspiration appears
during the opening phase  if an unvoiced stop is uttered in a context
where aspiration is suppressed as in spot it is almost identical to the
corresponding voiced stop sbot  luckily no words in english require
us to make a distinction in such contexts
voicing sometimes pervades the entire stopped portion of a voiced stop
especially when it is surrounded by other voiced segments
when saying a word like baby slowly you can choose whether or not to
prolong voicing throughout the second
ul
b
if you do creating what is
called a voice bar in spectrograms
the sound escapes through the cheeks for
the lips are closed em try doing it for a very long time and your cheeks
will fill up with air
this severely attenuates highfrequency components and can
be simulated with a weak first formant at a low resonant frequency
rf
nr x wunvoiced stops    u
nr x n
nr x nxnxwaspiration burst context and emphasisdependentu
nr x nlnx
in nxu
ta nxu nxu
unvoiced stopsclosure early cessation of voicing
silent steady state
opening comprising
short fricative burst
aspiration burst context and emphasisdependent
onset of voicing
sp
voiced stopsclosure late cessation of voicing
steady state possibility of voice bar
opening comprising
prevoicing
short fricative burst
ta i i i i i i i i i i i i
in 
fg table   acoustic phases of stop consonants
pp
table  summarizes some of the acoustic phases of voiced and unvoiced
stops  there are many variations that have not been mentioned
nasal plosion good news occurs at the word boundary in this case
when the nasal formant pervades the
opening phase  stop bursts are suppressed when the next sound is a stop
too the burst on the
ul
p
of apt for example
it is difficult to distinguish a voiced stop from an unvoiced one
at the end of a word cab and cap if the speaker is trying to
make himself particularly clear he will put a short neutral vowel
after the voiced stop to emphasize its early onset of voicing
if he is italian he will probably do this anyway for it is the norm
in his own language
pp
finally we turn to affricates of which there are only two
in english
ul
ch
chin and
ul
j
djinn
they are very similar to the stops
ul
t
and
ul
d
followed by the fricatives
ul
sh
and
ul
zh
respectively and their acoustic characterization is similar to that
of the phoneme pair
ul
ch
has a closing phase a stopped phase and a long fricative burst
there is no aspiration
for the vocal cords are not involved
ul
j
is the same except that voicing extends further into the stopped
portion and the terminating fricative is also voiced
it may be pronounced with a voice bar if the preceding segment is voiced
adjunct
sh   speech synthesis by rule
pp
generation of speech by rules acting upon a phonetic transcription
was first investigated in the early s kelly and gerstman 

kelly gerstman 

most systems employ a hardware resonance synthesizer analogue or digital
series or parallel
to reduce the load on the computer which operates the rules
the speechbyrule program rather than the
synthesizer inevitably contributes by far the greater part of the
degradation in the resulting speech
although parallel synthesizers offer greater potential control over
the spectrum it is not clear to what extent a synthesis program can take
advantage of this  parameter tracks for a series synthesizer can
easily be converted into linear predictive coefficients and systems
which use a linear predictive synthesizer will probably become popular
in the near future
pp
the phrase synthesis by rule which is in common use does not
make it clear just what sort of features the rules are supposed to
accomodate and what information must be included explicitly in the
input transcription
early systems made no attempt to simulate prosodics
pitch and rhythm could be controlled but only by inserting
pitch specifiers and duration markers in the input
some kind of prosodic control was often incorporated later
but usually as a completely separate phase from segmental synthesis
this does not allow interaction effects such as the extra
aspiration for voiceless stops in accented syllables to be taken
into account easily
even systems which perform prosodic operations invariably need to have
prosodic specifications embedded explicitly in the input
pp
generating parameter tracks for a synthesizer from a phonetic transcription
is a process of data
ul
expansion
six bits are ample to specify a phoneme and a speaking rate of  phonemessec
leads to an input data rate of  bits
the data rate required to control the synthesizer will depend upon the number
of parameters and the rate at which they are sampled
but a typical figure is  kbits chapter 
hence there is something like a hundredfold data expansion
pp
figure  shows the parameter tracks for a series synthesizers rendering
of the utterance
ul
s i k s
fc figure 
there are eight parameters
you can see the onset of frication at the beginning and end parameter 
and the amplitude of voicing parameter  come on for the
ul
i
and off again before the
ul
k
the pitch parameter  is falling slowly throughout the utterance
these tracks are stylized  they come from a computer synthesisbyrule
program and not from a human utterance
with a parameter update rate of  msec the graphs can be represented
by  sets of eight parameter values a total of  values or  bits
if a bit representation is used for each value
contrast this with the input of only four phoneme segments or say  bits
rh a segmentbysegment system
a seminal paper appearing in  was the first comprehensive
description of a computerbased synthesisbyrule system
holmes
ul
et al


holmes mattingly shearme 

the same system is still in use and has been reimplemented in a more
portable form wright 

wright 

the inventory of sound segments
includes the phonemes listed in table  as well as diphthongs and
a second allophone of
ul
l
many british speakers use quite a different vocal posture for
pre and postvocalic
ul
lc
s called clear and dark
ul
lc
s
respectively  some phonemes are expanded into subphonemic
phases by the program  stops have three phases corresponding to
the closure silent steady state and opening
diphthongs have two phases  we will call individual phases and
singlephase phonemes segments for they are subject to exactly
the same transition rules
pp
parameter tracks are constructed out of linear pieces
consider a pair of adjacent segments in an utterance to be synthesized
each one has a steadystate portion and an internal transition
the internal transition of one phoneme is dubbed external
as far as the other is concerned
this is important because instead of each segment being responsible
for its own internal transition one of the pair is identified
as dominant and it controls the duration of both transitions em its
internal one and its external the others internal one
for example in figure  the segment
ul
sh
dominates
ul
ee
and so it
governs the duration of both transitions shown
fc figure 
note that each
segment contributes as many as three linear pieces to the parameter track
pp
the notion of domination is similar to that discussed earlier for
word concatenation
the difference is that for word concatenation the dominant segment was
determined by computing the spectral derivative over the transition
region whereas for synthesisbyrule
segments are ranked according to a static precedence
and the higherranking segment dominates
segments of stop consonants have the highest rank and also
the greatest spectral derivative while fricatives nasals glides
and vowels follow in that order
pp
the concatenation procedure is controlled by a table which associates
 quantities with each segment  they are
lb
ni
rank
ni
  overall durations for stressed and unstressed occurrences
ni
  transition durations for internal and external transitions of
formant frequencies and amplitudes
ni
  target parameter values amplitudes and frequencies of three
formant resonances plus fricative information
ni
  quantities which specify how to calculate boundary values for
formant frequencies two for each formant except the third
which has only one
ni
  quantities which specify how to calculate boundary values for
amplitudes
le
this table is rather large  there are  segments in all remember
that many phonemes are represented by more than one segment
and so it has  entries  the system was an offline one which ran on
what was then em  em a large computer
pp
the advantage of such a large table of rules is the
flexibility it affords
notice that transition durations are specified independently for
formant frequency and amplitude parameters em this permits
fine control which is particularly useful for stops
for each parameter the boundary value between segments is calculated
using a fixed contribution from the dominant one
and a proportion of the steady state value of the other
pp
it is possible that the two transition durations which are
calculated for a segment actually exceed the overall duration specified
for it  in this case the steadystate target values will be approached
but not actually attained simulating a situation where coarticulation
effects prevent a target value from being reached
rh an eventbased system
the synthesis system described above in common with many others takes
an uncompromisingly segmentbysegment view of speech
the next phoneme is read perhaps split into a few segments and
these are synthesized one by one with due attention being paid
to transitions between them
some later work has taken a more syllabic view
mattingly  urges a return to syllables for both practical and
theoretical reasons

mattingly  syllable synthesis

transitional effects are particularly strong
within a syllable and comparatively weak but by no means negligible
from one syllable to the next  from a theoretical viewpoint
there are much stronger phonetic restrictions on phoneme sequences
than there are on syllable sequences  pretty well any syllable can
follow another although whether the pair makes sense is
a different matter but the linguistically
acceptable phoneme sequences are only a fraction
of those formed by combining phonemes in all
possible ways
hill  argues against what be calls the segmental assumption
that progress through the utterance should be made one segment at a time
and recommends a description of speech based upon perceptually relevant
events

hill  a program structure for eventbased speech synthesis by rules

this framework is interesting because it provides an opportunity for prosodic
considerations to be treated as an integral part of the synthesis
process
pp
the phonetic segments and other information that specify an utterance
can be regarded as a list of events which describes it
at a relatively high level
synthesisbyrule is the act of taking this list and elaborating on it
to produce lowerlevel events which are realized by the vocal tract
or acoustically simulated by a resonance synthesizer to give a speech
waveform
in articulatory terms an event might be begin tongue motion towards
upper teeth with a given effort while in resonance terms it could be
begin second formant transition towards  hz at a given rate
these two examples are
ul
not
intended to describe the same event  a tongue motion causes much more
than the transition of a single formant  coarticulation
issues such as stop burst suppression and nasal plosion should
be easier to imitate within an eventbased scheme than a segmenttosegment
one
pp
the isp system witten and abbess  is eventbased

witten abbess 

the key to its operation is the
ul
synthesis list
to prepare an utterance for synthesis the lexical items which specify
it are joined into a linked list  figure  shows the start of
the list created for
lb

ul
dh i z  i z  d zh aa k s  h aa u s
le
this is jacks house the       are
prosodic markers which will be discussed in the next chapter
fc figure 
next the rhythm and pitch assignment routines
augment the list with syllable boundaries phoneme
cluster identifiers and duration and pitch specifications
then it is passed to the segmental synthesis routine
which chains events into the appropriate places and as it
proceeds removes the no longer useful elements phoneme names
pitch specifiers etc which originally constituted the synthesis list
finally an interruptdriven speech synthesizer handler removes
events from the list as they become due and uses them to control
the hardware synthesizer
pp
by adopting the synthesis list as a uniform data structure for
holding utterances at every stage of processing the problems of storage
allocation and garbage collection are minimized
each list element has a forward pointer and five data words the first
indicating what type of element it is
lexical items which may appear in the input are
lb
ni
end of utterance    
ni
intonation indicator  
ni
rhythm indicator  
ni
word boundary   
ni
syllable boundary 
ni
phoneme segment
c
ul
ar b ng c

ni
explicit duration or pitch information
le
several of these have to do with prosodic features em a prime
advantage of the structure is that it does not create an artificial
division between segmentals and prosody
syllable boundaries and duration and pitch information are optional
they will normally be computed by isp but the user can override them in the
input in a natural way
the actual characters which identify lexical items are not fixed
but are taken from the rule table
pp
as synthesis
proceeds new elements are chained in to the synthesis list
for segmental purposes three types of event are defined em
target events increment events and aspiration events
with each event is associated a time at which the event becomes due
for a target event a parameter number target parameter value
and timeincrement are specified
when it becomes due motion of the parameter towards the
target is begun  if no other event for that parameter intervenes
the target value will be reached after the given timeincrement
however another target event for the parameter may change its motion
before the target has been attained
increment events contain a parameter number a parameter increment
and a timeincrement  the fixed increment is added to the parameter value
throughout the time specified  this provides an easy way to make a
fricative burst during the opening phase of a stop consonant
aspiration events switch the mode of excitation from voicing to aspiration
for a given period of time  thus the aspirated part of unvoiced stops
can be accomodated in a natural manner by changing the mode of excitation
for the duration of the aspiration
rf
nr x wexcitation
nr x wformant resonance
nr x wfricative
nr x wtype
nr x wfrequencies hz
nr x wresonance hz
nr x iiiiiiwlong vowel
nr x nlnx
in nxu
ta i i i i i i
hnxuexcitationhnxuformant resonancehnxufricativehnxutype
hnxufrequencies hzhnxuresonance hz
lnxuul
sp
nr x wvoicing
nr x wvowel
fiuhfrhnxuvoicingc
hnxuvowel
fiafrhnxuvoicinghnxuvowel
fiefrhnxuvoicinghnxuvowel
fiifrhnxuvoicinghnxuvowel
fiofrhnxuvoicinghnxuvowel
fiufrhnxuvoicinghnxuvowel
fiaafrhnxuvoicinghnxuvowel
nr x wlong vowel
fieefrhnxuvoicinghnxulong vowel
fierfrhnxuvoicinghnxulong vowel
fiarfrhnxuvoicinghnxulong vowel
fiawfrhnxuvoicinghnxulong vowel
fiuufrhnxuvoicinghnxulong vowel
nr x waspiration
nr x wh
fihfrhnxuaspirationhnxuh
nr x wvoicing
nr x wglide
firfrhnxuvoicing hnxuglide
fiwfrhnxuvoicinghnxuglide
filfrhnxuvoicinghnxuglide
fiyfrhnxuvoicinghnxuglide
nr x wnasal
fimfrhnxuvoicinghnxunasal
nr x wnone
nr x wstop
fibfrhnxunonehnxustop
fipfrhnxunonehnxustop
nr x wvoicing
nr x wnasal
finfrhnxuvoicinghnxunasal
nr x wnone
nr x wstop
fidfrhnxunonehnxustop
fitfrhnxunonehnxustop
nr x wvoicing
nr x wnasal
fingfrhnxuvoicinghnxunasal
nr x wnone
nr x wstop
figfrhnxunonehnxustop
fikfrhnxunonehnxustop
nr x wfrication
nr x wvoice  fric
nr x wfricative
fisfrhnxufricationhnxufricative
fizfrhnxuvoice  frichnxufricative
fishfrhnxufricationhnxufricative
fizhfrhnxuvoice  frichnxufricative
fiffrhnxufricationhnxufricative
fivfrhnxuvoice  frichnxufricative
fithfrhnxufricationhnxufricative
fidhfrhnxuvoice  frichnxufricative
lnxuul
ta i i i i i i i i i i i i
in 
fg table   rule table for an eventbased synthesisbyrule program
pp
now the rule table which is shown in table 
holds simple target positions for each phoneme segment as well as
the segment type  the latter is used to trigger events by computer
procedures which have access to the context of the segment
in principle this allows considerably more sophistication to be
introduced than does a simple segmentbysegment approach
rf
nr x iiwpreceding consonant in this syllable suppress burst if fricativeu
nr x nlnx
in nxu
ta i i
fricative bursts on stops
aspiration bursts on unvoiced stops affected by
preceding consonant in this syllable suppress burst if fricative
following consonant suppress burst if another stop introduce
nasal plosion if a nasal
prosodics increase burst if syllable is stressed
voice bar on voiced stops in intervocalic position
postvoicing on terminating voiced stops if syllable is stressed
anticipatory coarticulation for fihfr
vowel colouring when a nasal or glide follows
ta i i i i i i i i i i i i
in 
fg table   some coarticulation effects
pp
for example table  summarizes some of the subtleties of the
speech production process which have been mentioned earlier in this
chapter  most of them are contextdependent with the prosodic
context whether two segments are in the same syllable whether a
syllable is stressed playing a significant role  a scheme where
datadependent demons fire on particular patterns in a linked list
seems to be a sensible approach towards incorporating such rules
rh discussion
there are two opposing trends in speech synthesis by rule
on the one hand larger and larger segment inventories can be used
containing more and more allophones explicitly
this is the approach of the votrax soundsegment synthesizer
discussed in chapter 
it puts an increasing burden on the person who codes the utterances
for synthesis although as we shall see computer programs can assist with
this task
on the other hand the segment inventory can be kept small perhaps
comprising just the logical phonemes as in the isp system
this places the onus on the computer program to accomodate allophonic variations
and to do so it must take account of the segmental and prosodic
context of each phoneme
an eventbased approach seems to give the best chance of incorporating
contextual modification whilst avoiding undesired interactions
pp
the second trend brings synthesis closer to the articulatory process
of speech production  in fact an eventbased system would be
an ideal way of implementing an articulatory model for speech synthesis
by rule  it would be much more satisfying to have the rule table
contain articulatory target positions instead of resonance ones
with events like begin tongue motion towards upper teeth with a given
effort  the problem is that hard data on articulatory postures and
constraints is much more difficult to gather than resonance information
pp
an interesting question that relates to articulation is whether formant
motion can be simulated adequately by a small number of linear pieces
the segmentbysegment system described above had as many as nine
pieces for a single phoneme for some phonemes had three phases
and each one contributes up to three pieces transition in
steady state and transition out
another system used curves of decaying exponential
form which ensured that all transitions started rapidly towards
the target position but slowed down as it was approached rabiner  

rabiner  speech synthesis by rule bell system technical j


rabiner  a model for synthesizing speech by rule

the timeconstant of decay was stored with each segment in the rule
table  the rhythm of the synthetic speech was controlled at this level
for the next segment was begun when all the formants had attained
values sufficiently close to the current targets
this is a poor model of the human speech production process where rhythm
is dictated at a relatively high level and the next phoneme is not
simply started when the current one happens to end
nevertheless the algorithm produced smooth continuous formant motions
not unlike those found in spectrograms
pp
there is however by no means universal agreement on decaying exponential formant
motions  lawrence  divided segments into checked and free
categories corresponding roughly to consonants and vowels and postulated
ul
increasing
exponential transitions into checked segments and decaying transitions into
free ones

lawrence 

this is a reasonable supposition if you consider the mechanics of
articulation  the speed of movement of the tongue for example is likely
to increase until it is physically stopped by reaching the roof of the
mouth
when moving away from a checked posture into a free one the transition will
be rapid at first but slow down to approach the target asymptotically
governed by proprioceptive feedback
pp
the only thing that seems to be agreed is that the formant tracks should
certainly
ul
not
be piecewise linear  however in the face of
conflicting opinions as to whether exponentials should be decaying
or increasing piecewise linear motions seem to be a reasonable
compromise  it is likely that the precise shape of formant
tracks is unimportant so long as the gross features are imitated
correctly
nevertheless this is a question which an articulatory model
could help to answer
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
there are unfortunately few books to recommend on the subject of
joining segments of speech
the references form a representative and moderately comprehensive bibliography
here is some relevant background reading in linguistics
lb nn
fry

ds a fry dbeditor
ds d 
ds t acoustic phonetics
ds i cambridge univ press
ds c cambridge england
nr t 
nr a 
nr o 
  book
inn
this book of readings contains many classic papers on acoustic phonetics
published from 
it covers much of the history of the subject and is intended
primarily for students of linguistics
inn
lehiste

ds a lehiste ieditor
ds d 
ds t readings in acoustic phonetics
ds i mit press
ds c cambridge massachusetts
nr t 
nr a 
nr o 
  book
inn
another basic collection of references which covers much the same ground
as fry  above
inn
sivertsen

ds a sivertsen e
ds d 
ds k 
ds t segment inventories for speech synthesis
ds j language and speech
ds v 
ds p 
nr p 
nr t 
nr a 
nr o 
  journalarticle
inn
this is a careful early study of the quantitative implications of using
phonemes demisyllables syllables and words as the basic building
blocks for speech synthesis
inn
le nn
eq
delim 
en
ch   prosodic features in speech synthesis
ds rt prosodic features
ds cx principles of computer speech
pp
prosodic features are those which characterize an utterance as a whole
rather than having a local influence on individual sound segments
for speech output from computers an utterance usually comprises a
single unit of information which stretches over several words em a clause
or sentence  in natural speech an utterance can be very much longer but
it will be broken into prosodic units which are again roughly the size of a
clause or sentence  these prosodic units are certainly closely related
to each other  for example the pitch contour used when introducing a new
topic is usually different from those employed to develop it subsequently
however for the purposes of synthesis the successive prosodic units can
be treated independently and information about pitch contours to be used
will have to be specified in the input for each one
the independence between them is not complete however and
lowerlevel contextual effects such as interpolation of pitch between
the end of one prosodic unit and the start of the next must still be
imitated
pp
prosodic features were introduced briefly in chapter 
variations in voice dynamics occur in three dimensions  pitch of the voice
time and amplitude
these dimensions are inextricably twined together in living speech
variations in voice quality are much less important for the factual
kind of speech usually sought in voice response applications
although they can play a considerable in conveying emotions
for a discussion of the acoustic manifestations of emotion in speech
see williams and stevens 

williams stevens 

pp
the distinction between prosodic and segmental effects is a traditional one
but it becomes rather fuzzy when examined in detail
it is analogous to the distinction between hardware and
software in computer science  although useful from some points of view
the borderline becomes blurred as one gets closer to actual systems em with
microcode interrupts memory management and the like
at a trivial level prosodics
cannot exist without segmentals for there must be some vehicle to carry the
prosodic contrasts
timing em a prosodic feature em is actually realized by the durations of
individual segments  pauses are tantamount to silent segments
pp
while pitch may seem to be relatively independent of segmentals em and
this view is reinforced by the success of the sourcefilter model
which separates the frequency of the
excitation source from the filter characteristics em there
are some subtle phonetic effects of pitch
it has been observed that it drops on the transition into certain
consonants and rises again on the transition out haggard
ul
et al


haggard ambler callow 

this can be explained in terms of variations in pressure from the
lungs on the vocal cords ladefoged 

ladefoged 

briefly the increase in mouth pressure which occurs during some consonants
causes a reduction in the pressure difference across the vocal cords
and in the rate of flow of air between them
this results in a decrease in their frequency of vibration
when the constriction is released there is a temporary increase in the air
flow which increases the pitch again
the phenomenon is called microintonation
it is particularly noticeable in voiced stops but also occurs in voiced
fricatives and unvoiced stops
simulation of the effect in synthesisbyrule has often been found to give
noticeable improvements in the speech quality
pp
loudness also has a segmental role  for example we noted in the last chapter
that amplitude values play a small part in identification of fricatives
in fact loudness is a very
ul
weak
prosodic feature  it contributes little to the perception of stress
even for shouting the distinction from normal speech is as much in the voice
quality as in amplitude
ul
per se
it is not necessary to consider varying loudness on a prosodic basis
in most speech synthesis systems
pp
the above examples show how prosodic features have segmental influences
as well
the converse is also true  some segmental features have a prosodic effect
the last chapter described how stress is associated with increased aspiration
of syllableinitial unvoiced stops  furthermore stressed syllables
are articulated with greater effort than unstressed ones and hence the formant
transitions are more likely to attain their target values
under circumstances which would otherwise cause them to fall short
in unstressed syllables extreme vowels like
ul
ee aa uuc

tend to more centralized sounds
like
ul
i uh u
respectively
although all british english vowels
ul
can
appear in unstressed syllables they often become reduced into a
centralized form
consider the following examples
lb
ni
diplomat 
ul
d i p l uh m aa t
ni
diplomacy 
ul
d i p l uh u m uh s i
ni
diplomatic 
ul
d i p l uh m aa t i k
le
the vowel of the second syllable is reduced to
ul
uh
in diplomat and diplomatic whereas the root form diploma and also
diplomacy has a diphthong
c
ul
uh uc

there  the third syllable has an
ul
aa
in diplomat and diplomatic which is reduced to
ul
uh
in diplomacy
in these cases the reduction is shown explicitly in the phonetic transcription
but in more marginal examples where it is less extreme it will not be
pp
i have tried to emphasize in previous chapters that prosodic features are
important in speech synthesis
there is something very basic about them
rhythm is an essential part of all bodily activity em of breathing
walking working and playing em and so it pervades speech too
mothers and babies communicate effectively using intonation alone
some experiments have indicated that the language environment of
an infant affects his babbling at an early age before he has effective
segmental control
there is no doubt that tone of voice plays a large part in human
communication
pp
however early attempts at synthesis did not pay too
much attention to prosodics perhaps because it was thought sufficient to get the
meaning across by providing clear segmentals
as artificial speech grows more widespread however it is becoming
apparent that its acceptability to users and hence its ultimate
success depends to a large extent on incorporating naturalsounding
prosodics  flat arhythmic speech may be comprehensible in short stretches
but it strains the concentration in significant discourse and people
are not usually prepared to listen to it
unfortunately current commercial speech output systems do not really tackle
prosodic questions which indicates our present rather inadequate
state of knowledge
pp
the importance of prosodics for automatic speech
ul
recognition
is beginning to be appreciated too  some research projects
have attended to the automatic identification of points of stress
in the hope that the clear articulation of stressed syllables can be used
to provide anchor points in an unknown utterance for example see lea
ul
et al


lea medress skinner 

pp
but prosodics and segmentals are closely intertwined
i have chosen to
treat them in separate chapters in order to split the material up into
manageable chunks rather than to enforce a deep division between them
it is also true that synthesis of prosodic features is an uncharted and
controversial area which gives this chapter rather a different
flavour from the last
it is hard to be as definite about alternative strategies
and methods as you can for segment concatenation
in order to make the treatment as concrete and downtoearth as possible
i will describe in some detail two example projects in prosodic synthesis
the first treats the problem of transferring pitch from one utterance to
another while the second considers how artificial timing and pitch can be
assigned to synthetic speech
these examples illustrate quite different problems and are reasonably
representative of current research activity
other systems are described by mattingly  rabiner
ul
et al
  before

mattingly 


rabiner levitt rosenberg 

looking at the two examples we will discuss
a feature which is certainly prosodic but does not appear in the
list given earlier em stress
sh   stress
pp
stress is an everyday notion and when
listening to natural speech people can usually agree on which syllables
are stressed  but it is difficult to characterize in acoustic terms
from the speakers point of view a stressed syllable is produced by
pushing more air out of the lungs  for a listener the points of stress
are obvious
you may think that stressed syllables are louder than the others  however
instrumental studies show that this is not necessarily nor even usually
so eg lehiste and peterson 

lehiste peterson 

stressed syllables frequently have a longer vowel than unstressed
ones but this is by no means universally true em if you say little
or bigger you will find that the vowel in the first stressed syllable
is short and shows little sign of lengthening as you increase the emphasis
moreover experiments using bisyllabic nonsense words have indicated
that some people consistently judge the
ul
shorter
syllable to be stressed in the absence of other clues morton and jassem


morton jassem 

pitch often helps to indicate stress
it is not that stressed syllables are always higher or lowerpitched
than neighbouring ones or even that they are uttered with a rising or
falling pitch  it is the
ul
rate of change
of pitch that tends to be greater
for stressed syllables  a sharp rise or fall
or a reversal of direction helps to give emphasis
pp
stress is acoustically manifested in timing and pitch
and to a much lesser extent in loudness
however it is a rather subtle feature and does
ul
not
correspond simply to duration increases or pitch rises
it seems that listeners unconsciously put together all the clues
that are present in an utterance in order to deduce which syllables are
stressed
it may be that speech is perceived by a listener with reference to how
he would have produced it himself and that this is how he detects which syllables
were given greater vocal effort
pp
the situation is confused by the fact that certain syllables in words are
often said in ordinary language to be stressed on account of their
position in the word  for example the words
diplomat diplomacy and diplomatic have stress on the first
second and third syllables respectively
but here we are talking about the word itself rather than
any particular utterance of it  the stress is really
ul
latent
in the indicated syllables and only made manifest upon uttering them
and then to a greater or lesser degree depending on exactly how
they are uttered
pp
some linguists draw a careful distinction between salient syllables
accented syllables and stressed syllables
although the words are sometimes used differently by different authorities
i will not adopt a precise terminology here
but it is as well to be aware of the subtle distinctions involved
the term salience is applied to actual utterances and salient
syllables are those that are perceived as being more prominent than their
neighbours
accent is the potential for salience as marked for example
in a dictionary or lexicon
thus the discussion of the diplo words above is about accent
stress is an articulatory phenomenon associated with increased
muscular activity
usually syllables which are perceived as salient were produced with stress
but in shouting for example all syllables can be stressed em even
nonsalient ones
furthermore accented syllables may not be salient
for instance the first syllable of the word very is accented
that is potentially salient but in a sentence as uttered it may or may not be
salient  one can say
lb
c
ul
hes
very good
le
with salience on he and possibly good or
lb
hes
ul
very
good
le
with salience on the first syllable of very and possibly good
pp
nonstandard stress patterns are frequently used to bring out contrasts
words like a and the are normally unstressed but can be stressed
in contexts where ambiguity has arisen
thus factors which operate at a much higher level than the phonetic structure
of the utterance must be taken into account when deciding where stress
should be assigned  these include syntactic and semantic considerations
as well as the attitude of the speaker and the likely attitude of
the listener to the material being spoken
for example i might say
lb
anna
ul
and
nikki should go
le
with emphasis on the and purely because i was aware that my listener
might quibble about the expense of sending them both
clearly some notation is needed to communicate to the synthesis process
how the utterance is supposed to be rendered
sh   transferring pitch from one utterance to another
pp
for speech stored in sourcefilter form and concatenated on a
slotfilling basis it would be useful to
have stored typical pitch contours which can be applied to the
synthetic utterances
from a practical point of view it is important to be able to generate
naturalsounding pitch for highquality artificial speech
although several algorithms for creating completely synthetic contours
have been proposed em and we will examine one later in this chapter em
they are unsuitable for highquality speech
they are generally designed for use with synthesisbyrule from phonetics
and the rather poor quality of articulation does not encourage the
development of excellent pitch assignment procedures  with speech
synthesized by rule there is generally an emphasis on keeping the
data storage requirements to a minimum and so it is not appropriate
to store complete contours
moreover if speech is entered in textual
form as phoneme strings it is natural to attach pitch information as markers
in the text rather than by entering a complete and detailed contour
pp
the picture is rather different for concatenated segments of natural speech
in the airline reservation system with utterances formed from templates like
lb
flight number em leaves em at em  arrives in em at em 
le
it is attractive to store the pitch contour of one complete instance of the
utterance and apply it to all synthetic versions
pp
there is an enormous literature on the anatomy of intonation and much of it
rests upon the notion of a pitch contour as a descriptive aid to analysis
underlying this is the assumption usually unstated that a contour can be
discussed independently of the particular stream of words that manifests it
that a single contour can somehow be bound to any sentence or phrase or
clause to produce an acceptable utterance  but the contour and its binding
are generally described only at the grossest level the details being left
unspecified
pp
there are phonetic influences on pitch em the characteristic lowering
during certain consonants was mentioned above em and these are
not normally considered as part of intonation
such effects will certainly spoil attempts to store contours extracted
from living speech and apply them to different utterances but the impairment
may not be too great for pitch is only one of many segmental clues to
consonant identification
pp
in the system mentioned earlier which generated digit telephone numbers
by concatenating formantcoded words a single natural pitch contour
was applied to all utterances
it was taken to match as well as possible the general shape of the
contours measured in naturallyspoken telephone numbers  however this is a very
restricted environment for telephone numbers exhibit almost no variety in
the configuration of stressed and unstressed syllables em
the only digit which is not a monosyllable is seven
significant problems arise when more general utterances are considered
pp
suppose the pitch contour of one utterance the source
is to be transferred to another the target
assume that the utterances are encoded in sourcefilter form
either as parameter tracks for a formant synthesizer or as linear predictive
coefficients
then there are no technical obstacles to combining pitch and segmentals
the source must be available as a complete utterance while the target
may be formed by concatenating smaller units such as words
pp
for definiteness we will consider utterances of the form
lb
the price is em dollars and em cents
le
where the slots are filled by numbers less than 
and of the form
lb
the price is em cents
le
the domain of prices encompasses a wide range of syllable
configurations
there are between one and five syllables in each variable part
if the numbers are restricted to be less than 
the sentences have a constant pragmatic semantic and syntactic structure
as in the vast majority of reallife situations
minimal phonetic distinctions between utterances do not occur
pp
pitch transfer is complicated by the fact that values of the source pitch
are only known during the voiced parts of the utterance
although it would certainly be possible to extrapolate pitch
over unvoiced parts this would introduce some artificiality into
the otherwise completely natural contours
let us assume therefore that the pitch contour
of the voiced nucleus of each syllable in the source is applied to the
corresponding syllable nucleus in the target
pp
the primary factors which might tend to inhibit successful transfer
are
lb
np
different numbers of syllables in the utterances
np
variations in the pattern of stressed and unstressed syllables
np
different syllable durations
np
pitch discontinuities
np
phonetic differences between the utterances
le
rh syllabification
it is essential to take into account the syllable structures
of the utterances so that pitch is transferred between
corresponding syllables rather than over the utterance
as a whole
fortunately syllable boundaries can be detected automatically
with a fair degree of accuracy especially if the speech is carefully
enunciated
it is worth considering briefly how this can be done even though it takes
us off the main topic of synthesis and into speech analysis
pp
a procedure developed by mermelstein 
involves integrating the spectral energy
at each point in the utterance

mermelstein  automatic segmentation of speech into syllabic units

first the low  hz and high  hz ends are filtered out
with  dboctave cutoffs
the resulting energy signal is smoothed
by a  hz lowpass filter giving a socalled loudness
function
all this can be accomplished with simple recursive digital filters
pp
then the loudness function is compared with its convex hull
the convex hull is the shape a piece of elastic would assume if
stretched over the top of the loudness function and anchored down at
both ends as illustrated in figure 
fc figure 
the point of maximum difference between the hull and loudness function
is taken to be a tentative syllable
boundary
the hull is recomputed but anchored to the actual loudness function
at the tentative boundary
and the points of maximum hullloudness difference in each of the
two halves  are selected as further tentative
boundaries
the procedure continues recursively until the maximum hullloudness
difference with the hull anchored at each tentative boundary
falls below a certain minimum say  db
pp
at this stage the number of tentative boundaries will greatly exceed
the actual number of syllables by a factor of around 
many of the extraneous boundaries are eliminated by the following
constraints
lb
np
if two boundaries lie within a certain time of each other
say  msec one of them is discarded
np
if the maximum loudness within a tentative syllable falls too
far short of the overall maximum for the utterance
more than  db one boundary is discarded
le
the question of which boundary to discard can be decided by
examining the voicing continuity of the utterance
if possible voicing across a syllable boundary should be avoided
otherwise the boundary with the smallest hullloudness
difference should be rejected
rf
nr x wboundaries moved slightly to correspond better with voicing
nr x nlnx
in nxu
ta i i
lnxuul
sp
total syllable count
boundaries missed by algorithm
extra boundaries inserted by algorithm
boundaries moved slightly to correspond better with voicing

sp
total errors
lnxuul
ta i i i i i i i i i i i i
in 
fg table   success of the syllable segmentation procedure
pp
table  illustrates the success of this syllabification
procedure in a particular example
segmentation is performed with less than  of extraneous
boundaries being inserted
and much less than  of actual boundaries being missed
these figures are rather sensitive to the values of the
three thresholds
the values were chosen to err on the side
of overzealous syllabification because all the boundaries need to be checked
by ear and eye and it is easier to delete
a boundary by hand than to insert one at an appropriate place
it may well be that with careful optimization of thresholds
better figures could be
achieved
rh stressed and unstressed syllables
if the source and target utterances have the same number of
syllables and the same pattern of stressed and unstressed syllables
pitch can simply be transferred from a syllable in the source
to the corresponding one in the target
but if the pattern differs em even though the
number of syllables may be the same as in eleven and seventeen em
then a onetoone mapping will conflict with the stress points
and certainly sound unnatural
hence an attempt should be made to ensure that the pitch is mapped in a
plausible way
pp
the syllables of each utterance can be classified as stressed
and unstressed
this distinction could be made automatically by
inspection of the pitch contour within the domain of utterances used
and possibly even in general lea
ul
et al


lea medress skinner 

however in many cases it is expedient to perform the job by hand
in our example the sentences have fixed carrier parts and
variable number parts
the stressed carrier syllables namely
lb
 price  dol  cents
le
can be marked as such by hand
to facilitate proper alignment between the source and target
this marking would be difficult to do automatically
because it would be hard to distinguish the carrier from the numbers
pp
even after classifying the syllables as carrier stressed
stressed and unstressed alignment still presents problems
because the configuration of syllables in the variable parts
of the utterances may differ
syllables in the source which have no
correspondence in the target can be ignored
the pitch track of
the source syllable can be replicated for each
additional syllable in corresponding
position in the target
of course a stressed syllable should be selected for copying
if the unmatched target syllable is stressed
and similarly for unstressed ones
it is rather dangerous to copy exactly a part of a pitch
contour for the ear is very sensitive to the juxtaposition of
identically intoned segments of speech em especially when the segment is stressed
to avoid this whenever a stressed syllable is replicated the
pitch values should be decreased by say  on the second copy
it sometimes happens that a single stressed syllable in the source
needs to cover a stressedunstressed pair in the target  in
this case the first part of the source pitch track can be used
for the stressed syllable and the remainder for the
unstressed one
pp
the example of figure  will help to make these rules clear
fc figure 
note that the marking alone is done by hand
the detailed mapping decisions can be left to the computer
the rules were derived intuitively and do not have any sound theoretical
basis
they are intended to give reasonable results in the majority of cases
pp
figure  shows the result of transferring the pitch from the price is ten
cents to the price is seventyseven cents
fc figure 
the syllable boundaries which are marked were determined automatically
the use of the last  of the
ten contour to cover the first en syllable and its replication
to serve the ty syllable can be seen
however the em proportion is applied to the source contour
and the linear distortion described next upsets the proportion in the
target utterance
the contour of the second seven can be seen to be a
replication of that of the first one lowered by 
notice that the pitch extraction procedure has introduced an artifact into the final
part of one of the cents contours by doubling the pitch
rh stretching and squashing
the pitch contour over a source syllable nucleus must be stretched
or squashed to match the duration
of the target nucleus
it is difficult to see how anything other than linear stretching
and squashing could be done without considerably increasing the
complexity of the procedure
the gross nonlinearities will have been accounted for
by the syllable alignment process and so simple linear timedistortion
should not cause too much degradation
rh pitch discontinuities
sudden jumps in pitch during voiced speech sound peculiar
although they can in fact be produced naturally by yodelling
people frequently burst into laughter on hearing them in synthetic speech
it is particularly important to avoid this diverting effect in
voice response applications
for the listeners attention is instantly directed
away from what is said to the voice that speaks
pp
discontinuities can arise in the pitchtransfer procedure either by a
voicedunvoicedvoiced transition between syllables mapping on to
a voicedvoiced transition in the target
or by voicing continuity being broken when the syllable
alignment procedure drops or replicates a syllable
there are several ways in which at least some of the possibilities can
be avoided
for example one could hold unstressed syllables at a constant pitch
whose value coincides with either the end of the previous
syllables contour or the beginning of the next syllables contour
depending on which transition is voiced
alternatively the policy of reserving the trailing part
of a stressed syllable in the source to cover an unmatched following
unstressed syllable in the target could be generalized to allow use of the leading 
of the next stressed syllables contour instead
if that maintained voicing continuity
a third solution is simply to merge the pitch contours
at a discontinuity by mixing the average pitch value at the break
with the pitch contour on either side of it in a proportion which
increases linearly from the edges of the domain of influence to the discontinuity
figure  shows the effect of this merging
when the pitch contour of the price is seven cents
is transferred to the price is eleven cents
fc figure 
of course the
interpolated part will not necessarily be linear
rh results of an experiment on pitch transfer
some experiments have been conducted to evaluate the performance
of this pitch transfer method on the kind of utterances discussed above
witten 

witten  on transferring pitch from one utterance to another

first the source and target sentences
were chosen to be lexically identical that is the same words were spoken
for this experiment alone
expert judges were employed
each sentence was recorded twice by the same person
and pitch was transferred from copy a
to copy b and vice versa  also the originals were resynthesized from their linear
predictive coefficients with their own pitch contours
although all four often sounded extremely similar sometimes the pitch
contours of originals a and b were quite different
and in these cases it was immediately obvious to the ear that two of
the four utterances shared the same intonation
which was different to that shared by the other two
pp
experienced researchers in speech analysissynthesis served as
judges
in order to make the test as stringent as possible it was explained
to them exactly what had been done
except that the order of the utterances in each quadruple was kept secret
they were asked to identify which two of the four sentences did not have their
original contours
and were allowed to listen to each quadruple as often as they liked
on occasion they were prepared to identify only one or even none
of the sentences as artificial
pp
the result was that an utterance with pitch transferred
from another lexically identical one is indistinguishable from
a resynthesized version of the original even to a skilled ear
to be more precise this hypothesis
could not be rejected even at the  level of statistical significance  this
gave confidence in the transfer procedure
however one particular judge was quite successful at identifying the bogus contours
and he attributed his success to the fact that
on occasion the segmental durations did not accord with the
pitch contour
this casts a shadow of suspicion on the linear stretching and
squashing mechanism
pp
the second experiment examined pitch transfers between utterances having only one variable part
each the price is  cents to test the transfer
method under relatively controlled conditions
ten sentences of the form
lb
the price is em cents
le
were selected to cover
a wide range of syllable structures
each one was regenerated with pitch transferred from each of
the other nine
and these nine versions were paired with the original resynthesized
with its natural pitch
the  times  resulting pairs were recorded on tape in random order
pp
five males and five females with widely differing occupations
secretaries teachers academics and students served as judges
written instructions explained that the tape contained pairs of
sentences which were lexically identical but had a slight difference
in tone of voice and that the subjects were to judge which of
each pair sounded most natural and intelligible  the
response form gave the price associated with each pair em
a preliminary experiment had shown that there was never
any difficulty in identifying this em and a column for decision
with each decision the subjects recorded their confidence in the decision
subjects could rest at any time during the test which lasted for about
 minutes but they were not permitted to hear any pair a second time
pp
defining a success to be a choice of the utterance with
natural pitch as the best of a pair
the overall success rate was about 
if choices were random one would of course expect only a  success rate
and the figure obtained was significantly different from this
almost half the choices were correct and made with high confidence
highconfidence but incorrect choices accounted for a quarter of the
judgements
pp
to investigate structural effects in the pitch transfer process
low confidence decisions were ignored to eliminate noise and the others
lumped together and tabulated by source and target utterance
the number of stressed and unstressed syllables does not appear to play
an important part in determining whether a particular utterance is an
easy target
for example it proved to be particularly difficult to tell
eq
delim 
en
natural from transferred contours with utterances  and 
eq
delim 
en
in fact the results showed no better than random discrimination for them
even though the decisions in which listeners expressed little confidence
had been discarded
hence it seems that the syllable alignment procedure and the policy
of replication were successful
pp
eq
delim 
en
the worst target scores were for utterances  and 
eq
delim 
en
both of these contained large unbroken voiced periods
in the variable part em almost twice as long as the next longest
voiced period
the first has an unstressed syllable followed by
a stressed one with no break in voicing
involving in a natural contour
a fast but continuous climb in pitch over the juncture
and it is not surprising that it proved to be the most difficult target
a more sophisticated smoothing algorithm than the
one used may be worth investigating
pp
in a third experiment sentences with two variable parts were used to check
that the results of the second experiment extended to more complex
utterances
the overall success rate was  significantly different from chance
however a breakdown of the results by source and target utterance
showed that there was one contour for the utterance
the price is  dollars and  cents which exhibited very successful
transfer subjects identifying the transferredpitch utterances at only
a chance level
pp
finally transfers of pitch from utterances with two variable parts
to those with one variable part were tested
pitch contours were transferred to sentences with the same cents
figure but no dollars part for example
the price is five dollars and thirteen cents
to
the price is thirteen cents  the
contour was simply copied between the corresponding
syllables so that no adjustment needed to be made
for different syllable structures
the overall score was  successes in  judgements em
the same percentage as in the second experiment
pp
to summarize the results of these four experiments
lb
np
even accomplished linguists cannot distinguish an utterance from one with
pitch transferred from a different recording of it
np
when the utterance contained only one variable part embedded in a
carrier sentence
lay listeners identified the original correctly in  of cases
over a wide variety of syllable structures  this
figure differs significantly from the chance value of 
np
lay listeners identified the original confidently and correctly in
 of cases confidently but incorrectly in  of cases
np
the greatest hindrance to successful transfer was the presence of
a long uninterrupted period of voicing in the target utterance
np
the performance of the method deteriorates as the number
of variable parts in the utterances increases
np
some utterances seemed to serve better than others as the pitch source for
transfer although this was not correlated with complexity of syllable structure
np
even when the utterance contained two variable parts
there was one source utterance whose pitch contour was
transferred to all the others so successfully that listeners could not identify
the original
le
pp
the fact that only  of originals in the second experiment were
spotted by lay listeners in a stringent
pairedcomparison test em many of them being identified without confidence em
does encourage the use of the procedure for generating stereotyped
but different utterances of high quality in voiceresponse systems
the experiments indicate that although different syllable patterns
can be handled satisfactorily by this procedure
long voiced periods should be avoided if possible when designing
the message set
and that if individual utterances must contain multiple variable parts
the source utterance should be chosen with the aid of listening tests
sh   assigning timing and pitch to synthetic speech
pp
the pitch transfer method can give good results within a fairly narrow
domain of application
but like any speech output technique which treats complete utterances
as a single unit with provision for a small number of slotfillers to
accomodate datadependent messages it becomes unmanageable in more general
situations with a large variety of utterances
as with segmental synthesis it becomes necessary to consider methods
which use a textual rather than an acousticallybased representation
of the prosodic features
pp
this raises a problem with prosodics that was not there for segmentals  how
ul
can
prosodic features be written in text form
the standard phonetic transcription method does not give much help with
notation for prosodics  it does provide a diacritical mark to indicate
stress but this is by no means enough information for synthesis
furthermore texttospeech procedures described in the next chapter
promise to allow segmentals to be specified by an ordinary orthographic
representation of the utterance but we have seen that considerable
intelligence is required to derive prosodic features from text
more than mere intelligence may be needed  this is underlined by a paper
bolinger 
delightfully entitled
accent is predictable em if youre a mind reader

bolinger  accent is predictable em if youre a mind reader

pp
if synthetic speech is to be used as a computer output medium rather
than as an experimental tool for linguistic research it is important
that the method of specifying utterances is natural and easy to learn
prosodic features must be communicated to the computer in a manner
considerably simpler than individual duration and pitch specifications
for each phoneme as was required in early synthesisbyrule systems
fortunately a notation has been developed for conveying some of the
prosodic features of utterances as a byproduct of the linguistically
important task of classifying the intonation contours used in
conversational english halliday 

halliday 

this system has even been used to help foreigners speak english
halliday  em which emphasizes the fact that it was designed for use
by laymen not just linguists

halliday  course in spoken english intonation

pp
here are examples of the way utterances can be conveyed to the isp
speech synthesis system which was described in the previous chapter
the notation is based upon hallidays
lb
ni

ul
  aw t uhm aa t i k  s i n th uh s i s  uh v  s p ee t sh
ni

ul
  f r uh m  uh  f uhn e t i k  r e p r uh z e nt e i sh uh n
le
automatic synthesis of speech from a phonetic representation  three
levels of stress are distinguished  tonic or sentence stress
marked by  before the syllable foot stress marked by 
and unstressed syllables
the notion of a foot controls the rhythm of the speech in a way that
will be described shortly
a fourth level of stress is indicated on a segmental basis when a syllable
contains a reduced vowel
pp
utterances are divided by punctuation into
ul
tone groups
which are the basic prosodic unit em there are two in the example
the shape of the pitch contour is governed by a numeral at the start of
each tone group
crude control over pauses is achieved by punctuation marks  full stop for
example signals a pause while comma does not
longer pauses can be obtained by several full stops as in   the
 character stands for a socalled silent stress or breath point
word boundaries are marked by two spaces between phonemes
as mentioned in the previous chapter syllable boundaries and explicit
pitch and duration specifiers can also be included in the input
if they are not the isp system will attempt to compute them
rh rhythm
our understanding of speech rhythm knows many laws but little order
in the mid s there was a spate of publications reporting new data
on segmental duration in various contexts and there is a growing
awareness that segmental duration is influenced by a great many factors
ranging from the structure of a discourse through semantic and syntactic
attributes of the utterances their phonemic and phonetic makeup
right down to physiological constraints
these multifarious influences are ably documented and reviewed by
klatt 

klatt  linguistic uses of segment duration in english

what seems to be lacking in this work is a conceptual framework on to
which new information about segmental duration can be nailed
pp
one startingpoint for imitating the rhythm of english speech is the
hypothesis of regularly recurring stresses
these stresses are primarily
ul
rhythmic
ones and should be distinguished from the tonic stress mentioned above which
is primarily an
ul
intonational
one
rhythmic stresses are marked in the transcription by a 
the stretch between one and the next is called a foot
and the hypothesis above is often referred to as that of isochronous feet
isochronous means of equal time
there is considerable controversy about this hypothesis
it is most popular among british linguists and it must be admitted
amongst those who work by introspection and intuition and do not actually
ul
measure
things
although the question of isochrony of feet has long been debated there
seems to be general agreement
em even amongst american linguists em
that there is at least a tendency towards
equal spacing of foot boundaries
however little is known about the strength of this tendency and the extent
of deviations from it see hill
ul
et al
 for an attempt
to quantify it em and there is even evidence to suggest that it may in part
be a
ul
perceptual
phenomenon lehiste 

hill jassem witten 


lehiste 

on this basic point as on many others the designer of a prosodic synthesis
strategy must needs make assumptions which cannot be properly justified
pp
from a pragmatic point of view there are two advantages to basing
a synthesis strategy on this hypothesis
firstly it provides a way to represent the many influences of higherlevel
processes like syntax and semantics on rhythm using a simple notation which
fits naturally into the phonetic utterance representation
and which people find quite easy to understand and generate
secondly it tends to produce a heavily accentuated but not unnatural
speech rhythm which can easily be moderated into a more acceptable rhythm
by departing from isochrony in a controlled manner
pp
the isp procedure does not make feet exactly isochronous
it starts with a standard foot time and attempts to fit the syllables of the
foot into this time
if doing so would result in certain syllables having less than a preset minimum
duration the isochrony constraint is relaxed and the foot is expanded
there is no preset
ul
maximum
syllable length
however when the durations of individual phoneme postures are adjusted
to realize the calculated syllable durations
limits are imposed on the amount by which individual phonemes can be expanded
or contracted
thus a hierarchy of limits exists
pp
the rate of talking is determined by the standard foot time
if this time is short many feet will be forced to have durations longer than
the standard and the speech will be less isochronous
this seems to accord with common human experience
if the standard time is longer however the minimum syllable limit
will always be exceeded and the speech will be completely isochronous
if it is too long the abovementioned limits to phoneme expansion will
come into play and again partially destroy the isochrony
pp
it has often been observed that the final foot of an utterance tends to be
longer than others as does the tonic foot em that which bears the
major stress
this is easy to accomodate simply by making the target duration
longer for these feet
rh from feet to syllables
a foot is a succession of syllables one or more
and it is obvious that since there are more syllables in some feet than
in others some syllables must occupy less time than others in order to preserve
the tendency towards isochrony of feet
pp
however the duration of a foot is not divided evenly between its constituent
syllables  the syllables have a definite rhythm of their own which seems
to be governed by
lb
np
the nature of the salient that is the first syllable of the foot
np
the presence of word boundaries within the foot
le
a salient syllable tends to be long either if it contains one of
a class of socalled long vowels or if there is a cluster of two or more
consonants following the vowel
the pattern of syllables and word boundaries governs the rhythm of the foot
and table  shows the possibilities for one two and threesyllable feet
this theory of speech rhythm is due to abercrombie 

abercrombie  syllable quantity and enclitics in english

rf
nr x wthreesyllable feet  u
nr x wsalshort  u
nr x wweak   u
nr x wweak      u
nr x wfiit s inconfrceivable    u
nr x wsyllable rhythm
nr x nxnxnxnxnxnxnx
nr x nlnx
in nxu
ta nxu nxu nxu nxu nxu
ul
syllable patternexamplehnxusyllable rhythm
sp
onesyllable feetsalientfigoodfr show
weakfi goodfrbye
sp
twosyllable feetsallongweakficentrefr forward
salshortweakfiatomfr bomb
salient  weakfitea forfr two
sp
threesyllable feetsalient  weak weakfione for thefr road
fiits inconfrceivable
sallongweak weakfiafter thefr war
salshortweak weakfimiddle tofr top
sallongweakweakfinobodyfr knows
salshortweakweakfianythingfr more
sp
 denotes a word boundary
 is an optional word boundary
ta i i i i i i i i i i i i
fg table   syllable patterns and rhythms
pp
a foot may have the rhythmical characteristics of a twosyllable foot
while having only one syllable if the first place in it is filled by a
silent stress marked by 
this is shown in the second onesyllable example of
table 
a similar effect may occur with two and threesyllable feet
although examples are not given in the table
feet of four and five syllables em with or without a silent stress em are
considerably rarer
pp
syllabification em splitting an utterance into syllables em is a job
which had to be done for the pitchtransfer procedure described earlier
and the nature of syllable rhythms calls for it here too
even though the utterance is now specified phonetically instead of
acoustically the same basic principle applies
syllables normally coincide with peaks of sonority
where sonority measures the inherent loudness of a sound relative to
other sounds of the same duration and pitch
however difficult cases exist where it seems to be unclear how many syllables
there are in a word  ladefoged  discusses this problem with examples
such as real realistic and reality  furthermore

ladefoged 

care must be taken to avoid counting two syllables in a word like sky
because of its two peaks of sonority em for the stop
ul
k
has lower
sonority than the fricative
ul
s
pp
three levels of notional sonority are enough for syllabification
dividing phoneme segments into
ul
sonorants
glides and nasals
ul
obstruents
stops and fricatives and vowels a general syllable has the form
lb
eq
obstruent sup   sonorant sup   vowel sup   sonorant sup  
obstruent sup   
en
le
where  means repetition that is occurrence zero or more times
this sidesteps the sky problem by giving fricatives the same
sonority as stops
it is easy to use the above structure to count the number
of syllables in a given utterance by counting the sonority
peaks
pp
however what is required is an indication of syllable
ul
boundaries
as well as a syllable count
for slow conversational speech these can be approximated as follows
word divisions obviously form syllable boundaries as should
foot markers em but it may be wise not to assume that the latter do if the
utterance has been prepared by someone with little knowledge of linguistics
syllable boundaries should be made to coincide with sonority minima
as an
ul
ad hoc
pragmatic
rule if only one segment has the minimum sonority the boundary is placed
before it
if there are two segments each with the minimum sonority it is placed between
them while for three or more it is placed after the first two
pp
these rules produce obviously acceptable divisions in many cases
today ashtray taxfree with perhaps unexpected positioning of the
boundary in others inspire department
actually people do differ in placement of syllable boundaries
abercrombie 

abercrombie 

rh from syllables to segments
the theory of isochronous feet with the caveats noted earlier
and that of syllable rhythms provide a way of producing durations for
individual syllables  but where are these durations supposed to be measured
there is a beat point or tapping point near the beginning of each syllable
this is the place where a listener will tap if asked to give one tap to each
syllable it has been investigated experimentally by allen 

allen  location of rhythmic stress beats in english one

it is not necessarily at the very beginning of the syllable
for example in straight the tapping point is certainly after the
ul
s
and the stopped part of the
ul
t
pp
another factor which relates to the division of the syllable duration
amongst phonetic segments is the oftenobserved fact that the length of the
vocalic nucleus is a strong clue to the degree of voicing of the terminating
cluster lehiste 

lehiste  suprasegmentals

if you say in pairs words like cap cab cat cad tack tag
you will find that the vowel in the first word of each pair is significantly
shorter than that in the second
in fact the major difference between such pairs is the vowel length
not the final consonant
pp
such effects can be taken into account by considering a syllable to comprise
an initial consonant cluster followed by a vocalic nucleus and a final
consonant cluster
any of these elements can be missing em the most unusual case where the
nucleus is absent occurs for example in socalled syllabic
ul
nc
s
as in renderings of button pudding which might be written
buttn puddn
however it is convenient to modify the definition of the nucleus
so as to rule out the possibility of it being empty
using the characterization of the syllable given above the clusters can
be defined as
lb
ni
initial cluster  obstruentud sonorantud
ni
nucleus  vowelud sonorantud
ni
final cluster  obstruentud
le
sonorants are included in the nucleus so that it is always present
even in the case of a syllabic consonant
pp
then rules can be used to divide the syllable duration between the
initial cluster nucleus and final cluster
these must distinguish between situations where the terminating cluster
is voiced or unvoiced so that the characteristic differences in vowel lengths
can be accomodated
pp
finally the cluster durations must be apportioned amongst their constituent
phonetic segments  there is little published data on which to base this
two simple schemes which have been used in isp are described in
witten  and witten  smith 

witten  a flexible scheme for assigning timing and pitch to synthetic speech


witten smith  synthesizing british english rhythm

rh pitch
there are two basically different ways of looking at the pitch of an
utterance
one is to imagine pitch
ul
levels
attached to individual syllables
this has been popular amongst american linguists and some people
have even gone so far as to associate pitch levels with levels of
stress
the second approach is to consider pitch
ul
contours
as we did earlier when examining how to transfer pitch from one utterance
to another
this seems to be easier for the person who transcribes the utterances
to produce for the information required is much less detailed than levels
attached to each syllable  some indication needs to be given of how
the contour is to be bound to the utterance and in the notation introduced above
the most prominent or tonic syllable is indicated in the transcription
pp
hallidays  classification identifies five different primary intonation
contours each hinging on the tonic syllable

halliday  course in spoken english intonation

these are sketched in figure  in the style of halliday
fc figure 
several secondary contours which are variations on the primary ones
are defined as well
however this classification scheme is intended for consumption by people
who bring to the problem a wealth of prior knowledge of speech and years
of experience with it  it captures only the gross features
of the infinite variety of pitch contours found in living speech
in a sense the classification is
ul
phonological
rather than
ul
phonetic
for it attempts to distinguish the features which make a logical difference
to the listener instead of the acoustic details of the pitch contours
pp
it is necessary to take these contours and subject them to a sort of
phonologicaltophonetic embellishment before applying them in synthetic
speech
for example the stretches with constant pitch which precede the tonic
syllable in tone groups   and  sound
most unnatural when synthesized em for pitch is hardly ever
exactly constant in living speech
some pretonic pitch variation is necessary
and this can be made to emphasize the salient syllable
of each foot  a lilting effect which reaches a peak at each foot
boundary and drops rather faster at the beginning of the foot than it
rises at the end sounds more natural  the magnitude of this inflection
can be altered slightly to add interest but a considerable increase in it
produces a semantic change by making the utterance sound more emphatic
it is a major problem to pin down exactly the turning points of pitch in
the fallingrising and risingfalling contours  and  in figure 
and even deciding on precise values for the pitch frequencies involved is not
always easy
pp
the aim of the pitch assignment method of isp is to allow the person
or program which originates a spoken message to exercise a great deal
of control over its intonation without having to concern himself with
foot or syllable structure  the message to be spoken must be broken down
into tone groups
which correspond roughly to hallidays tone groups
each one comprises a
ul
tonic
of one or more feet which is optionally preceded by a
ul
pretonic
also with a number of feet  it is advantageous to allow a tone group
boundary to occur in the middle of a foot whereas hallidays scheme
insists that it occurs at a foot boundary
the first foot of the tonic the
ul
tonic foot
is marked by an asterisk at the beginning
it is on the first syllable of this foot em the
tonic or nuclear
syllable em that the major stress of the tone group occurs
if there is no asterisk in a tone group
isp takes the final foot as the tonic
since this is the most common case
pp
the pitch contour on a tone group is specified by an array of ten numbers
of course the system cannot generate all conceivable contours for a tone
group but the definitions of the ten specifiable quantities have been
chosen to give a useful range of contours
if necessary more precise control over the pitch of an utterance can
be achieved by making the tone groups smaller
pp
the overall pitch movement is controlled by specifying the pitch at three
places  the beginning of the tone group the beginning of the tonic syllable
and the end of the tone group
provision is made for an abrupt pitch break at the start of the tonic
syllable in order to simulate tone groups  and  and to a lesser
extent tone groups  and 
the pitch is interpolated linearly over the first part of the
tone group up to the tonic syllable and over the last part from there to
the end except that it is possible to specify a nonlinearity on the tonic
syllable for emphasis as shown in figure 
fc figure 
pp
on this basic shape are superimposed two finer pitch patterns
one of these is an initializationcontinuation option which allows
the pitch to rise or fall independently on the initial and final feet
to specified values without affecting the contour on the rest
of the tone group figure 
fc figure 
the other is a foot pattern which is superimposed on each pretonic foot
to give the stressed syllables of the pretonic added prominence and avoid
the monotony of constant pitch
this is specified by a
ul
nonlinearity
parameter which distorts the contour on the foot at a predetermined
point along it
figure  shows the effect
fc figure 
pp
the ten quantities that define a pitch contour are summarized in
table  and shown diagrammatically in figure 
fc figure 
rf
nr x wh    u
nr x nxwfraction along foot of the nonlinearity position for the tonic footu
nr x nlnx
in nxu
ta nxu n
acontinuation from previous tone group
zero gives no continuation
nonzero gives pitch at start of tone group
bnotional pitch at start
cpitch range on whole of pretonic
ddeparture from linearity on each foot of pretonic
epitch change at start of tonic
fpitch range on tonic
gdeparture from linearity on tonic
hcontinuation to next tone group
zero gives no continuation
nonzero gives pitch at end of tone group
ifraction along foot of the nonlinearity position for pretonic feet
jfraction along foot of the nonlinearity position for the tonic foot
ta i i i i i i i i i i i i
in 
fg table   the quantities that define a pitch contour
pp
the intention of this parametric method of specifying contours
is that the parameters should be easily derivable from semantic variables
like emphasis novelty of idea surprise uncertainty incompleteness
here we really are getting into controversial unresearched areas
roughly speaking parameters d and g control emphasis g by itself
controls novelty and surprise and h and the relative sizes of e and f
control uncertainty and incompleteness
certain parameters notably i and j are defined because although they
do not appear to correspond to semantic distinctions we do not yet know
how to generate them automatically
rf
nr x iiiiiiiiiiiw
nr x nlnx
in nxu
ta i i i i i i i i i i i
hallidays
tone groupabcdefghij
lnxuul
sp





lnxuul
ta i i i i i i i i i i i i
in 
fg table   pitch contour table for hallidays primary tone groups
pp
one basic requirement of the pitch assignment scheme was the ability to
generate contours which approximate hallidays five primary tone groups
values of the ten specifiable quantities are given in table  for each
tone group
all pitches are given in hz
a distinctly dipping pitch movement has been given to each pretonic foot
parameter d
to lend prominence to the salient syllables
sh   evaluating prosodic synthesis
pp
it is extraordinarily difficult to evaluate schemes for prosodic synthesis
and this is surely a large part of the reason why prosodics are among the
least advanced aspects of artificial speech
segmental synthesis can be tested by playing people minimal pairs of
words which differ in just one feature that is being investigated
for example one might experiment with pit bit tot dot
cot got to test the rules which discriminate unvoiced from voiced stops
there are standard wordlists for intelligibility tests which can be
used to compare systems too
no equivalent of such microlevel evaluation exists for prosodics
for they by definition have a holistic effect on utterances
they are most noticeable and most important in longish stretches of speech
even monotonous arhythmic speech will be intelligible in
sufficiently short samples provided the segmentals are good enough
but it is quite impossible to concentrate on such speech in quantity
some attempts at evaluation appear in ainsworth  and mchugh 
but these are primarily directed at assessing the success of pronunciation
rules which are discussed in the next chapter

ainsworth  performance of a speech synthesis system


mchugh  listener preference and comprehension tests

pp
one evaluation technique is to compare synthetic with natural versions
of utterances as was done in the pitch transfer experiment
the method described earlier used a sensitive pairedcomparison test
where subjects heard both versions in quick succession and were asked
to judge which was most natural and intelligible
this is quite a stringent test and one that may not be so useful
for inferior completely synthetic contours
it is essential to degrade the natural utterance so that it is
comparable segmentally to the synthetic one  this was done in the
experiment described by extracting its pitch and resynthesizing it
from linear predictive coefficients
pp
several other experiments could be undertaken to evaluate artificial
prosody
for example one could compare
lb
np
natural and artificial rhythms using artificial segmental synthesis
in both cases
np
natural and artificial pitch contours using artificial segmental synthesis
in both cases
np
natural and artificial pitch contours using segmentals extracted from
natural utterances
le
there are many other topics which have not yet been fully investigated
it would be interesting for example to define rules for generating speech
at different tempos
elisions where phonemes or even whole syllables are suppressed
occur in fast speech these have been analyzed by linguists
but not yet incorporated into synthetic models
it should be possible to simulate emotion by altering parameters such as
pitch range and mean pitch level but this seems exceptionally difficult
to evaluate  one situation where it would perhaps be possible to
measure emotion is in the reading of sports results em in fact a study
has already been made of intonation in soccer results bonnet 

bonnet 

even the synthesis of voices with different pitch ranges requires
investigation for as noted earlier it is difficult to place
precise frequency specifications on phonological contours such as
those sketched in figure 
clearly the topic of prosodic synthesis is a rich and potentially
rewarding area of research
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
there are quite a lot of books in the field of linguistics which
describe prosodic features
here is a small but representative sample from both sides of the atlantic
lb nn
abercrombie

ds a abercrombie d
ds d 
ds t studies in phonetics and linguistics
ds i oxford univ press
ds c london
nr t 
nr a 
nr o 
  book
inn
abercrombie is one of the leading english authorities on phonetics
and this is a collection of essays which he has written over the years
some of them treat prosodics explicitly and others show the influence
of verse structure on abercrombies thinking
inn
bolinger

ds a bolinger deditor
ds d 
ds t intonation
ds i penguin
ds c middlesex england
nr t 
nr a 
nr o 
  book
inn
a collection of papers that treat a wide variety of different aspects
of intonation in living speech
inn
crystal

ds a crystal d
ds d 
ds t prosodic systems and intonation in english
ds i cambridge univ press
nr t 
nr a 
nr o 
  book
inn
this book attempts to develop a theoretical basis for the study of british
english intonation
inn
gimson

ds a gimson ac
ds d 
ds t the linguistic relevance of stress in english
ds b phonetics and linguistics
ds e wejones and jlaver
ds p 
nr p 
ds i longmans
ds c london
nr t 
nr a 
nr o 
  articleinbook
inn
here is a careful discussion of what is meant by stress with much more
detail than has been possible in this chapter
inn
lehiste

ds a lehiste i
ds d 
ds t suprasegmentals
ds i mit press
ds c cambridge massachusetts
nr t 
nr a 
nr o 
  book
inn
this is a comprehensive study of suprasegmental phenomena in natural speech
it is divided into three major sections  quantity timing tonal features
pitch and stress
inn
pike

ds a pike kl
ds d 
ds t the intonation of american english
ds i univ of michigan press
ds c ann arbor michigan
nr t 
nr a 
nr o 
  book
inn
a classic although somewhat dated study
notice that it deals specifically with american english
inn
le nn
eq
delim 
en
ch   generating speech from text
ds rt generating speech from text
ds cx principles of computer speech
pp
in the preceding two chapters i have described how artificial speech
can be produced from a written phonetic representation with additional
markers indicating intonation contours points of major stress rhythm
and pauses
this representation is substantially the same as that used by linguists
when recording natural utterances
what we will discuss now are techniques for generating this information
or at least some of it from text
pp
figure  shows various levels of the speech synthesis process
fc figure 
starting from the top with plain text the first box splits it into
intonation units tone groups decides where the major emphases
tonic stresses should be placed
and further subdivides the tone group into rhythmic units feet
for intonation analysis it is necessary to decide on an interpretation
of the text which in turn as was emphasized at the beginning of the
previous chapter depends both on the semantics of what is being said and
on the attitude of the speaker to his material
the resulting representation will be at the level of hallidays notation
for utterances with the words still in english rather than phonetics
table  illustrates the utterance representation at the various levels
of the figure
rf
nr x wpitch and duration    wat  khz sampling rate a second utterance
nr x nlnx
in nxu
ta wpitch and duration    u wpause  u w msec   u
representationexample
lnxuul
sp
plain textautomatic synthesis of speech
from a phonetic representation
sp
text adorned with automatic synthesis of speech
prosodic markers from a phonetic representation
sp
phonetic text withfi  aw t uhm aa t i k  s i n th uh s i sfr
prosodic markersfiuh v  s p ee t shfr 
fi  f r uh m  uh  f uhn e t i kfr
fir e p r uh z e nt e i sh uh nfr 
sp
phonemes withpause msec
pitch and durationfiawfr msec hz
fitfr msec hz
fiuhfr msec hz
fimfr msec hz
fiaafr msec hz



sp
parameters for parameters each updated at a frame
formant or linearrate of  msec
predictive second utterance gives  frames
synthesizeror  data values
sp
acoustic waveat  khz sampling rate a second utterance
has  samples
lnxuul
ta i i i i i i i i i i i i
in 
fg table   utterance representations at various levels in speech synthesis
pp
the next job is to translate the plain text into a broad phonetic
transcription
this requires knowledge of lettertosound pronunciation
rules for the language under consideration
but much more is needed  the structure of each word must be examined for
prefixes and suffixes because they em especially the latter em have a
strong influence on pronunciation
this is called morphological analysis
actually it is also required for rhythmical purposes because prefixes
are frequently unstressed note that the word prefix is itself an
exception to this
thus the appealing segmentation of the overall problem shown in figure 
is not very accurate for the individual processes cannot be rigidly
separated as it implies  in fact we saw earlier how this intermixing of
levels occurs with prosodic and segmental features
nevertheless it is helpful to structure discussion of the problem by
separating levels as a first approximation
further influences on pronunciation come from the semantics and syntax
of the utterance em and both also play a part in intonation and rhythm analysis
the result of this second process is a phonetic representation still
adorned with prosodic markers
pp
now we move down from higherlevel intonation and rhythm considerations
to the details of the pitch contour and segment durations
this process was the subject of the previous chapter
the problems are twofold  to map an appropriate acoustic pitch contour
on to the utterance using tonic stress point and foot boundaries as
anchor points and to assign durations to segments using the
footemsyllableemclusteremsegment hierarchy
if it is accepted that the overall rhythm can be captured adequately by foot
markers this process does not interact with earlier ones
however many researchers do not believing instead that rhythm is
syntactically determined at a very detailed level
this will of course introduce strong interaction between the duration
assignment process and the levels above
klatt  puts it into his title em
vowel lengthening is syntactically determined in a connected discourse

klatt  vowel lengthening is syntactically determined

contrast this with the paper cited earlier bolinger  entitled
accent is predictable em if youre a mind reader

bolinger  accent is predictable em if youre a mind reader

noone would disagree that accent is an influential factor in vowel length
pp
notice incidentally that the representation of the result of the pitch
and duration assignment process in table  is inadequate for each segment
is shown as having just one pitch
in practice the pitch varies considerably throughout every segment
and can easily rise and fall on a single one  for example
lb
hes
ul
very
good
le
may have a risefall on the vowel of very
the linked eventlist datastructure of isp is much more suitable
than a textual string for utterance representation at this level
pp
the fourth and fifth processes of figure  have little interaction with
the first two which are the subject of this chapter  segmental
concatenation which was treated in chapter  is affected by prosodic
features like stress but a notation which indicates stressed syllables
like hallidays is sufficient to capture this influence
contextual modification of segments by which i mean
the coarticulation effects which govern allophones of phonemes
is included explicitly in the fourth process to emphasize that the upper levels
need only provide a broad phonemic transcription rather than a detailed
phonetic one
signal synthesis can be performed by either a formant synthesizer or a
linear predictive one discussed in chapters  and 
this will affect the details of the segmental concatenation process but should have no
impact at all on the upper levels
pp
figure  performs a useful function by summarizing where we have
been in earlier chapters em the lower three boxes em and introducing the
remaining problems that must be faced by a full texttospeech system
it also serves to illustrate an important point  that a speech output system
can demand that its utterances be entered in any of a wide range of
representations
thus one can enter at a low level with a digitized waveform or linear
predictive parameters or higher up with a phonetic representation
that includes detailed pitch and duration specification at the phoneme level
or with a phonetic text or plain text adorned with prosodic markers
or at the very top with plain text as it would appear in a book
a heavy price in naturalness and intelligibility is paid by moving up
ul
any
of these levels em and this is just as true at the top of the figure as
at the bottom
sh   deriving prosodic features
pp
if you really need to start with plain text
some very difficult problems present themselves
the text should be understood first of all and then decisions need to be
made about how it is to be interpreted
for an excellent speaker em like an actor em these decisions will be artistic
at least in part
they should certainly depend upon the opinion and attitude of the speaker
and his perception of the structure and progress of the dialogue
very little is known about this upper level of speech synthesis from text
in practice it is almost completely ignored em and the speech is at most
barely intelligible and certainly uncomfortable to listen to
hence anybody contemplating building or using a speech output system which
starts from something close to plain text should consider carefully whether some extra
semantic information can be coded into the initial utterances to help with
prosodic interpretation
only rarely is this impossible em and reading machines for the blind are
a prime example of a situation where arbitrary unannotated texts
must be read
rh intonation analysis
one distinction which a program can usefully try
to make is between basically rising
and basically falling pitch contours  it is often said that pitch rises on
a question and falls on a statement but if you listen to speech you will
find this to be a gross oversimplification  it normally
falls on statements certainly but it falls as often as it rises on questions
it is more accurate to say that pitch rises on yesno questions
and falls on other utterances although this rule is still only a rough guide
a simple test which operates lexically on the input text is to determine
whether a sentence is a question by looking at the 
punctuation mark at its end and then to examine the first word
if it is a whword like what which when why and also how
a falling contour is likely to fit
if not the question is probably a yesno one and the contour
should rise
such a crude rule will certainly not be very accurate
it fails for example when the whword is embedded in a phrase as in
at what time are you going but at least it provides a startingpoint
pp
an air of finality is given to an utterance when it bears a definite
fall in pitch dropping to a rather low value at the end
this should accompany the last intonation unit in an utterance
unless it is a yesno question
however a risefall contour such as hallidays tone group  figure 
can easily be used in utterancefinal position by one person
in a conversation em
although it would be unlikely to terminate the dialogue altogether
a new topic is frequently introduced by a fallrise contour em such as
hallidays tone group  em and this often begins a paragraph
pp
determining the type of pitch contour is only one part of
intonation assignment  there are really three separate problems
lb
np
dividing the utterance into tone groups
np
choosing the tonic syllable or major stress point of each one
np
assigning a pitch contour to each tone group
le
let us continue to use the halliday notation for intonation which was introduced
in simplified form in the previous chapter
moreover assume that the foot boundaries can be placed correctly em
this problem will be discussed in the next subsection
then a scheme which considers only the lexical form of the utterance
and does not attempt to understand it whatever that means is as follows
lb
np
place a tone group boundary at every punctuation mark
np
place the tonic at the first syllable of the last foot in a tone group
np
use contour  for the first tone group in a paragraph and contour 
elsewhere except for a yesno question which receives contour 
le
rf
nr x wfrom scarborough to whitbyw    from scarborough to whitby is a
nr x nlnx
in nxu
ta wfrom scarborough to whitbyu
plain texttext adorned with prosodic markers
lnxuul
sp
from scarborough to whitby is a  from scarborough to whitby is a
very pleasant journey with very pleasant journey with
very beautiful countryside very beautiful countryside 
in fact the yorkshire coast is  in fact the yorkshire coast is
lovelylovely
all along ex all along ex
cept the parts that are covered cept the parts that are covered
in caravans of course andin caravans of course and
if you go in spring if you go in spring
when the gorse is out  when the gorse is out
or in summer  or in summer
when the heathers out  when the heathers out
its really one of the most  its really one of the most
delightful areas in thedelightful areas in the
whole country whole country
sp
the moorland is  the moorland is
rather high up and rather high up and
fairly flat em a fairly flat a
sort of plateau sort of plateau 
at least  at least
it isnt really flat  it isnt really flat
when you get up on the top  when you get up on the top
its rolling moorland  its rolling moorland
cut across by steep valleys  but cut across by steep valleys but
seen from the coast its seen from the coast its 
up there on the moors and you up there on the moors and you
always think of it as a always think of it as a
kind of tableland kind of tableland
lnxuul
ta i i i i i i i i i i i i
in 
fg table   example of intonation and rhythm analysis from halliday 

halliday  course in spoken english intonation

pp
these extremely crude and simplistic rules are really the most that one can do
without subjecting the utterance to a complicated semantic analysis
in statistical terms they are actually remarkably effective
table  shows part of a spontaneous monologue which was transcribed by
halliday and appears in his teaching text on intonation
halliday  p 

halliday  course in spoken english intonation

among the prosodic markers are some that were not introduced in chapter 
firstly each tone group has secondary contours which are identified
by   for tone group  and so on
secondly the mark  is used to indicate a pause which disrupts
the speech rhythm
notice that its positioning belies the advice of the old elocutionists
br
ev
in 
lb
fi
a comma stops the voice while we may privately tell
ni
ul
one
a semicolon
ul
two
a colon
ul
threec
  and a period
ul
four
br
nr x wfionefr a semicolon fitwofr a colon fithreefr  and a period fifourwmasonfr 
ni
hnxumason 
nf
le
br
ev
thirdly compound tone groups such as  appear which contain
ul
two
tonic syllables
this differs from a simple concatenation of tone groups
with contours  and  in this case because the second is in some sense subsidiary to
the first
typically it forms an adjunct clause while the first clause gives the
main information  halliday provides many examples such as
lb
ni
jane goes shopping in town every friday
ni
 i met arthur on the train
le
but he does not comment on the
ul
acoustic
difference between a compound tone group and a concatenation of simple ones em
which is after all the information needed for synthesis
a final minor difference between hallidays scheme and that outlined earlier
is that he compels tone group boundaries to occur at the beginning
of a foot
rf
nr x iiwcomplete
nr x nlnx
in nxu
ta i i
excerpt incomplete
table passage
lnxuul
sp
number of tone groups
sp
number of boundaries correctly  
placed
sp
number of boundaries incorrectly 
placed
sp
number of tone groups having a  
tonic syllable at the beginning
of the final foot
sp
number of tone groups whose  
contours are correctly assigned
lnxuul
sp
number of compound tone groups  
sp
number of secondary intonation  
contours
lnxuul
ta i i i i i i i i i i i i
in 
fg table   success of simple intonation assignment rules
pp
applying the simple rules given above to the text of table  leads to
the results in the first column of table 
threequarters of the foot boundaries are flagged by
punctuation marks with no extraneous ones being included
 of tone groups have a tonic syllable at the start of the final foot
however the compound tone groups each have two tonic syllables
and of course only the second one is predicted by the finalfoot rule
assigning intonation contours on the extremely simple basis of using
contour  for the first tone group in a paragraph and contour  thereafter
also seems to work quite well  secondary contours such as  and 
have been mapped into the appropriate primary contour  in this case
for the present purpose and compound tone groups have been assigned the first
contour of the pair
the result is that  of contours are given correctly
pp
in order to give some idea of the reliability of these figures the results
for the whole passage transcribed by halliday em of which table  is an
excerpt em are shown in the second column of table   although it
looks as though the rules may have been slightly lucky with the excerpt
the general trends are the same with  to  of features being assigned
correctly
it could be argued though that the complete text is punctuated fairly liberally by
presentday standards so that the tonegroup boundary rule is unusually
successful
pp
these results are really astonishingly good considering the crudeness of
the rules  however they should be interpreted with caution
what is missed by the rules although appearing to comprise only
 to  of the features is certain to include the important
informationbearing and varietyproducing features that give the utterance
its liveliness and interest
it would be rash to assume that all tonegroup boundaries
all tonic positions and all intonation contours are equally
important for intelligibility and naturalness
it is much more likely that the rules predict a
default pattern while most information is borne by deviations from
them
to give an engineering analogy it may be as though the carrier waveform
of a modulated transmission is being simulated instead of the
informationbearing signal
certainly the utterance will if synthesized with intonation given by these
rules sound extremely dull and repetitive mainly because of the
overwhelming predominance of tone group  and the universal placement
of tonic stress on the final foot
pp
there are certainly many different ways to orate any particular text
and that given by halliday and reproduced in table  is only one possible
version
however it is fair to say that the default intonation discussed above
could only occur naturally under very unusual circumstances em such as
a petulant child unwilling and sulky having been forced to read aloud
this is hardly how we want our computers to speak
rh rhythm analysis
consider now how to decide where foot boundaries should be placed
in english text
clearly semantic considerations sometimes play a part in this em one could
say
lb
 is this train going to london
le
instead of the more usual
lb
 is this train going to london
le
in circumstances where the train might be going
ul
to
or
ul
from
london
such effects are ignored here although it is worth noting in passing that the
rogue words will often be marked by underscoring or italicizing
as in the previous sentence
if the text is liberally underlined semantic analysis may
be unnecessary for the purposes of rhythm
pp
a rough and ready rule for placing foot boundaries is to insert one before
each word which is not in a small closed set of function words
the set includes for example a and but for is the to
if a verb or adjective begins with a prefix the boundary should be moved
between it and the root em but not for a noun
this will give the distinction between
ul
conc
vert noun and conc
ul
vert
verb
ul
exc
tract and exc
ul
tract
and for many north american speakers
will help to distinguish
ul
inc
quiry from inc
ul
quire
however detecting prefixes by a simple splitting algorithm is dangerous
for example predate is a verb with stress on what appears to be a prefix
contrary to the rule while the pre in predator is not a prefix em at
least it is not pronounced as the prefix pre normally is
moreover polysyllabic words like diplomat diplomacy diplomatic
or telegraph telegraphy telegraphic cannot be handled on such a simple
basis
pp
in  a remarkable work on english sound structure was published
chomsky and halle  which proposes a system of rules to transform
english text into a phonetic representation in terms of distinctive features
with the aid of a lexicon

chomsky halle 

a great deal of attention is paid to stress and rules are given which
perform well in many tricky cases
pp
it uses the american system of levels of stress marking
socalled primary stress with a superscript  secondary stress with a
superscript  and so on
the superscripts are written on the vowel of the stressed
syllable  completely unstressed syllables receive no annotation
for example the sentence take johns blackboard eraser is written
lb
taudke joudhns blaudckboaudrd eraudser
le
in foot notation this utterance
is
lb
take johns blackboard eraser
le
it undoubtedly contains less information than the stresslevel version
for example the second syllable of blackboard and the first one of erase
are both unstressed although the rhythm rules given in chapter 
will cause them
to be treated differently because they occupy different places in the
syllable pattern of the foot
take johns and the second syllable of erase are all nontonic
footinitial syllables and hence are not distinguished in the notation
although the pitch contours schematized in figure  will give them different
intonations
pp
an indefinite number of levels of stress can be used  for example according
to the rules given by chomsky and halle the word sad in
lb
my friend cant help being shocked at anyone who would fail to consider
his sad plight
le
has level stress the final two words being annotated
as saudd pliudght
however only the first few levels are used regularly and
it is doubtful whether acoustic distinctions are made in speech
between the weaker ones
pp
chomsky and halle are concerned to distinguish between such utterances as
lb
ni
blaudck boaudrderaudser    board eraser that is black
ni
blaudckboaudrd eraudser     eraser for a blackboard
ni
blaudck boaudrd eraudser    eraser of a black board
le
and their stress assignment rules do indeed produce each version when
appropriate
in foot notation the distinctions can still be made
lb
ni
black boarderaser
ni
blackboard eraser
ni
black board eraser
le
pp
the rules operate on a grammatical derivation tree
of the text
for instance input for the three examples would be written
lb
ni
dnpudau black dau dnudnu boarddnu
dnu eraser dnudnudnpu
ni
dnudnudau black dau dnu board dnudnu dnu eraser dnudnu
ni
dnudnpudau black dau dnu board dnudnpu dnu eraser dnudnu
le
representing the trees shown in figure 
fc figure 
here n stands for a noun np for a noun phrase and a for an adjective
these categories appear explicitly as nodes in the tree
in the linearized textual representation they are used to label
brackets which represent the tree structure
an additional piece of information which is needed is the lexical entry for
eraser which would show that it has only one accented
that is potentially stressed syllable namely the second
pp
consider now how to account for stress in prefixed and
suffixed words and those polysyllabic ones with more than one potential
stress point
for these the morphological structure must appear in the input
pp
now
ul
morphemes
are welldefined minimal units of grammatical analysis from which a word
may be composed
for example  went  go  ed  is
a morphemic decomposition where ed denotes the
pasttense morpheme
this representation is not particularly suitable for speech synthesis
for the obvious reason that the result bears no phonetic resemblance to
the input
what is needed is a decomposition into
ul
morphs
which occur only when the lexical or phonetic representation of a word may
easily be segmented into parts
thus  wanting  want  ing  and  bigger  big  er  are
simultaneously morphic and morphemic decompositions
notice that in the second example a rule about final consonant doubling has
been applied at the lexical level although it is not needed in
a phonetic representation  this comes into the sphere
of easy segmentation
contrast this with  went  go  ed  which
is certainly not an easy segmentation and hence a
morphemic but not a morphic decomposition
but between these extremes there are some difficult
cases  specific  specify  ic  is probably morphic
as well as morphemic but it is not clear
that  galactic  galaxy  ic  is
pp
assuming that the input is given as a derivation tree with morphological
structure made explicit chomsky and halle present rules which assign stress
correctly in nearly all cases  for example their rules give
lb
ni
daudnu incident dnu  aldau  em  iudncideudntal
le
and if the stem is marked by  dsu  dsu  in prefixed words
they can deduce
lb
ni
dnu tele dsu graph dsudnuem  teudlegraudph
ni
dnudnu tele dsu graph dsudnu y dnuem  teleudgraphy
ni
daudnu tele dsu graph dsudnu ic dauem  teudlegraudphiudc
le
pp
there are two rules which account for the wordlevel stress
on such examples  the main stress
rule and the alternating stress rule
in essence the main stress rule emphasizes the last strong syllable
of a stem
a syllable is strong either if it contains one of a class of socalled
long vowels or if there is a cluster of two or more consonants
following the vowel otherwise it is weak
if you are exceptionally observant you will notice that this strongemweak
distinction has been used before when discussing the rhythm of feet in
syllables  thus the verb torment receives stress on the second syllable
for it is a strong one
a noun like torment is treated as being derived from the corresponding verb
and the rule assigns stress to the verb first and then modifies it for the noun
the second alternating stress rule gives some stress to alternate
syllables of polysyllabic words like formc
ul
alc
dec
ul
hydec

pp
it is quite easy to incorporate the wordlevel rules into a computer
program which uses feet rather than stress levels as the basis for prosodic
description
a foot boundary is simply placed before the primarystressed level syllable
except for function words which do not begin a foot
the other stress levels should be ignored
except that for slow deliberate speech secondary level stress is
mapped into a foot boundary too if it precedes the primary stress
there is also a rule which reduces vowels in unstressed
syllables
pp
the stress assignment rules can work on phonemic script as well as english
for example starting from the phonetic
form  dvu c
ul
aa s t o n i sh c
dvu
the stress assignment rules
produce  c
ul
aa s t oud n i sh c
  the
vowel reduction rule
generates  c
ul
uh s t oud n i sh c
  and
the foot conversion process
gives  c
ul
uh st o n i sh
this appears to provide a fairly reliable algorithm for foot boundary
placement
rh speech synthesis from concept
i argued earlier that in order to derive prosodic features
of an utterance from text it
is necessary to understand its role in the dialogue its semantics
its syntax and em as we have just seen em its morphological structure
this is a very tall order and the problem of natural language comprehension
by machine is a vast research area in its own right
however in many applications requiring speech output
utterances are generated by the computer from internally stored data
rather than being read aloud from preprepared text
then the problem of comprehending text may be evaded for
presumably the languagegeneration module can provide a semantic
syntactic and even morphological decomposition of the utterance
as well as some indication of its role in the dialogue
that is why it is necessary to say it
pp
this forms the basis of the appealing notion of speech synthesis from concept
it has some advantages over speech generation from text and in principle
should provide more naturalsounding speech
every word produced by the system can have a complete lexical entry which
shows its morphological decomposition and potential stress points
the full syntactic history of each utterance is known
the chomskyhalle rules described above can therefore be used to place
foot boundaries accurately without the need for a complex parsing program
and without the risk of having to make guesses about unknown words
pp
however it is not clear how to take advantage of any semantic information
which is available  ideally it should be possible to place tone group
boundaries and tonic stress points and assign intonation contours in
a naturalsounding way
but look again at the example text of table  and imagine that you have
at your disposal as much semantic information as is needed
it is
ul
still
far from obvious how the intonation features could be assigned
it is in the ultimate analysis interpretive and stylistic
ul
choices
that add variety and interest to speech
pp
take the problem of determining pitch contours for instance
some of them may be explicable
contour  on
lb
ni
except the parts that are covered in caravans of course
le
is due to its being a contrastive clause for it presents
essentially new information
similarly the succession
lb
ni
if you go in spring
ni
when the gorse is out
ni
or in summer
ni
when the heathers out
le
could be considered contrastive being in the subjunctive voice and
this could explain why contour s were used
but this is all conjecture and it is difficult to apply throughout the
passage
halliday  explains the contexts in which each tone group is typically
used but in an extremely highlevel manner which would be impossible
to embody directly in a computer program

halliday  course in spoken english intonation

at the other end of the spectrum computer systems for written
discourse production do not seem to provide the subtle information needed
to make intonation decisions see for example davey  for a fairly
complete description of such a system

davey 

pp
one project which uses such a method for generating speech has been
described young and fallside 

young fallside 

although some attention is paid to rhythm the intonation contours
which are generated are disappointingly repetitive and lacking in
richness
in fact very little semantic information is used to assign contours really
just that inferred by the crude punctuationdriven method described
earlier
pp
the higherlevel semantic problems associated with speech output were
studied some years go under the
title synthetic elocution vanderslice 

vanderslice 

a set of rules was generated and tested by hand on a sample passage
the first part of which is shown in table 
however no attempt was made to formalize the rules in a computer program
and indeed it was recognized that a number of important questions
such as the form of the semantic information assumed at the input
had been left unanswered
rf
nr x w  psychologist   wemphasis assigned because of antithesis with  
nr x nlnx
in nxu
ta w  psychologist   u
lnxuul
sp
human experience and human behaviour are accessible to
observation by everyone  the psychologist tries to bring
them under systematic study  what he perceives however
anyone can perceive for his task he requires no microscope
or electronic gear
sp
  wordcomments
lnxuul
sp
  humanspecial treatment because paragraphinitial
  humanaccent deleted because it echoes word 
  psychologistemphasis assigned because of antithesis with
everyone
  themanaphoric to human experience and human
behaviour
  systematicemphasis assigned because of contrast with
observation
  studyemphasis em text is ambiguous whether
observation is a kind of study that is
nonsystematic or an activity contrasting
with the entire concept of systematic study
  whatincrease in pitch for what he perceives
because it is not the subject
  heaccented although anaphoric to word 
because of antithesis with word 
  howeverdecrease in pitch because it is parenthetical
  anyoneemphasized by antithesis with word 
  perceiveunaccented because it echoes word 
perceives
  semicolon assigns falling intonation
  taskunaccented because it is anaphoric with
tries to bring them under systematic study
lnxuul
ta i i i i i i i i i i i i
in 
fg table   sample passage and comments pertinent to synthetic elocution
pp
the comments in the table which are selected and slightly edited versions
of those appearing in the original work vanderslice  are intended
as examples of the nature and subtlety of the prosodic influences which
were examined

vanderslice 

the concepts of accent and emphasis are used these relate to stress
but are not easy to define precisely in our tonegroup terminology
fortunately we do not need an exact characterization of them for the present
purpose
roughly speaking accent encompasses both footinitial stress and
tonic stress whereas emphasis is something more than this
typically being realized by the fallrise or risefall contours of
hallidays tone groups  and  figure 
pp
particular attention is paid to anaphora and antithesis amongst other things
the first term means the repetition of a word or phrase in the text
and is often applied to pronoun references
in the example the word human is repeated in the first few words
them in the second sentence refers to human experience and human
behaviour he in the third sentence is the previouslymentioned
psychologist and task is anaphoric with tries to bring them under
systematic study
other things being equal anaphoric references are unaccented
in our terms this means that they certainly do not receive tonic stress
and may not even receive foot stress
pp
antithesis is defined as the contrast of ideas expressed by parallelism of
strongly contrasting words or phrases and the second element taking part
in it is generally emphasized
psychologist in the passage is an antithesis of everyone
systematic and possibly study of observation
thus
lb
ni
 the psychologist
le
would probably receive intonation contour  since it is also introducing
a new actor while
lb
ni
tries to bring them under systemmatic study
le
could receive contour 
he and everyone are antithetical not only does the latter receive
emphasis but the former has its accent restored em for otherwise
it would have been removed because of anaphora with psychologist
hence it will certainly begin a foot possibly a tonic foot
pp
a factor that does not affect the sample passage is the accentuation
of unusual syllables of similar words to bring out a contrast
for example
lb
ni
he went
ul
outc
side not
ul
inc
side
le
although this may seem to be just another facet of antithesis
vanderslice points out that it is phonetic rather than structural
similarity that is contrasted
lb
ni
i said
ul
dec
plane not
ul
comc
plain
le
this introduces an interesting interplay between the phonetic and
prosodic levels
pp
anaphora and antithesis provide an ideal domain for speech synthesis from
concept
determining them from plain text is a very difficult problem
requiring a great deal of realworld knowledge
the first has received some attention in the field of natural language
understanding
finding pronoun referents is an important problem for language translation
for their gender is frequently distinguished in say french where it is not
in english
examples such as
lb
ni
i bought the wine sat on a table and drank it
ni
i bought the wine sat on a table and broke it
le
have been closely studied wilks  for if they were to be translated
into french the pronoun it would be rendered differently in each case
c
ul
le
vin
ul
la
table

wilks  an intelligent analyzer and understander of english

pp
in spoken language emphasis is used to indicate the referent of a pronoun
when it would not otherwise be obvious
vanderslice gives the example
lb
ni
bill saw john across the room and he ran over to him
ni
bill saw john across the room and
ul
he
ran over to
ul
him
le
where the emphasis reverses the pronoun referents
so that john did the running
he suggests accenting a personal pronoun whenever the true
antecedent is not the same as the unmarked or default one
unfortunately he does not elaborate on what is meant by unmarked
does it mean that the referent cannot be predicted from
knowledge of the words alone em as in the second example above
if so this is a clear candidate for speech synthesis from concept
for the distinction cannot be made from text 
sh   pronunciation
pp
english pronunciation is notoriously irregular
a poem by charivarius the pseudonym of a dutch high school teacher
and linguist gntrenite  surveys the problems in an amusing
way and is worth quoting in full
br
ev
in 
lb nnnnnnnnnnnnnnnn
ul
              the chaos
sp
ne
dearest creature in creation
studying english pronunciation
in n
i will teach you in my verse
sounds like corpse corps horse and worse
ne
in n
it will keep you susy busy
make your head with heat grow dizzy
in n
tear in eye your dress youll tear
so shall i  oh hear my prayer
ne
in n
pray console your loving poet
make my coat look new dear sew it
in n
just compare heart beard and heard
dies and diet lord and word
ne
in n
sword and sward retain and britain
mind the latter how its written
in n
made has not the sound of bade
say em said pay em paid laid but plaid
ne
in n
now i surely will not plague you
with such words as vague and ague
in n
but be careful how you speak
say break steak but bleak and streak
ne
in n
previous precious fuchsia via
pipe shipe recipe and choir
in n
cloven oven how and low
script receipt shoe poem toe
ne
in n
hear me say devoid of trickery
daughter laughter and terpsichore
in n
typhoid measles topsails aisles
exiles similes reviles
ne
in n
wholly holly signal signing
thames examining combining
in n
scholar vicar and cigar
solar mica war and far
ne
in n
desire em desirable admirable em admire
lumber plumber bier but brier
in n
chatham brougham renown but known
knowledge done but gone and tone
ne
in n
one anemone balmoral
kitchen lichen laundry laurel
in n
gertrude german wind and mind
scene melpemone mankind
ne
in n
tortoise turquoise chamoisleather
reading reading heathen heather
in n
this phonetic labyrinth
gives  moss gross brook brooch ninth plinth
ne
in n
billet does not end like ballet
bouquet wallet mallet chalet
in n
blood and flood are not like food
nor is mould like should and would
ne
in n
banquet is not nearly parquet
which is said to rime with darky
in n
viscous viscount load and broad
toward to forward to reward
ne
in n
and your pronunciations ok
when you say correctly  croquet
in n
rounded wounded grieve and sieve
friend and fiend alive and live
ne
in n
liberty library heave and heaven
rachel ache moustache eleven
we say hallowed but allowed
people leopard towed but vowed
in n
mark the difference moreover
between mover plover dover
ne
in n
leeches breeches wise precise
chalice but police and lice
in n
camel constable unstable
principle discipline label
ne
in n
petal penal and canal
wait surmise plait promise pal
in n
suit suite ruin circuit conduit
rime with  shirk it and beyond it
ne
in n
but it is not hard to tell
why its pall mall but pall mall
in n
muscle muscular goal and iron
timber climber bullion lion
ne
in n
worm and storm chaise chaos chair
senator spectator mayor
in n
ivy privy famous clamour
and enamour rime with hammer
ne
in n
pussy hussy and possess
desert but dessert address
in n
golf wolf countenants lieutenants
hoist in lieu of flags left pennants
ne
in n
river rival tomb bomb comb
doll and roll and some and home
in n
stranger does not rime with anger
neither does devour with clangour
ne
in n
soul but foul and gaunt but aunt
font front wont want grand and grant
in n
shoes goes does  now first say  finger
and then singer ginger linger
ne
in n
real zeal mauve gauze and gauge
marriage foliage mirage age
in n
query does not rime with very
nor does fury sound like bury
ne
in n
dost lost post and doth cloth loth
job job blossom bosom oath
in n
though the difference seems little
we say actual but victual
ne
in n
seat sweat chaste caste leigh eight height
put nut granite but unite
in n
reefer does not rime with deafer
feoffer does and zephyr heifer
ne
in n
dull bull geoffrey george ate late
hint pint senate but sedate
in n
scenic arabic pacific
science conscience scientific
ne
in n
tour but our and succour four
gas alas and arkansas
in n
sea idea guinea area
psalm maria but malaria
ne
in n
youth south southern cleanse and clean
doctrine turpentine marine
in n
compare alien with italian
dandelion with battalion
ne
in n
sally with ally yea ye
eye i ay aye whey key quay
say aver but ever fever
neither leisure skein receiver
in n
never guess em it is not safe
we say calves valves half but ralf
ne
in n
heron granary canary
crevice and device and eyrie
in n
face preface but efface
phlegm phlegmatic ass glass bass
ne
in n
large but target gin give verging
ought out joust and scour but scourging
in n
ear but earn and wear and tear
do not rime with here but ere
ne
in n
seven is right but so is even
hyphen roughen nephew stephen
in n
monkey donkey clerk and jerk
asp grasp wasp and cork and work
ne
in n
pronunciation em think of psyche 
is a paling stout and spikey
in n
wont it make you lose your wits
writing groats and saying groats
ne
in n
its a dark abyss or tunnel
strewn with stones like rowlock gunwale
in n
islington and isle of wight
housewife verdict and indict
ne
in n
dont you think so reader rather
saying lather bather father
in n
finally  which rimes with enough
though through plough cough hough or tough
ne
in n
hiccough has the sound of cup
my advice is  give it up
le nnnnnnnnnnnnnnnn
br
ev
rh lettertosound rules
despite such irregularities it is surprising how much can be done
with simple lettertosound rules
these specify phonetic equivalents of word fragments and single letters
the longest stored fragment which matches the current word is translated
and then the same strategy is adopted on the remainder of the word
table  shows some english fragments and their pronunciations
rf
nr x iwpronunciation  
nr x nlnx
in nxu
ta i
fragmentpronunciation
lnxuul
sp
pfipfr
phfiffr
phefif eefr
phesfif ee zfr
photfif uh u tfr
placefip l e i sfr
placifip l e i s ifr
plementfip l i m e n tfr
pliefip l aa i yfr
postfip uh u s tfr
ppfipfr
pplyfip l eefr
precioufip r e s uhfr
procedfip r uh u s ee dfr
properfip r o p uh rfr
provfip r uu vfr
purposefip er p uh sfr
pushfip u shfr
putfip u tfr
putsfip u t sfr
lnxuul
ta i i i i i i i i i i i i
in 
fg table   word fragments and their pronunciations
pp
it is sometimes important to specify that a rule applies only when
the fragment is matched at the beginning or end of a word
in the table  means that other fragments can precede or follow this
one
the  sign is used to separate suffixes from a word stem
as will be explained
shortly
pp
an advantage of the longeststring search strategy is that it is easy
to account for exceptions simply by incorporating them into the fragment
table
if they occur in the input the complete word will automatically be
matched first before any fragment of it is translated
the exception list of complete words can be surprisingly small for
quite respectable performance
table  shows the entire dictionary for an excellent early pronunciation
system written at bell laboratories mcilroy 

mcilroy 

some of the words are notorious exceptions in english while others are
included simply because the rules would run amok on them
notice that the exceptions are all quite short with only a few of them
having more than two syllables
rf
nr x iiiiii
nr x nlnx
in nxu
ta i i i i i
adoesntguestmeantreaderthose
alkalidoinghasmoreoverreferto
alwaysdonehavemrsaystoday
anydrhavingmrsseventomorrow
aprilearlyheardnatureshalltuesday
areearnhisnonesomeonetwo
aselevenimplynothingsomethingupon
becauseenableintonowherethanvery
beenengineisnuisancethatwater
beingetcislandofthewednesday
beloweveningjohnontheirwere
bodyeveryjulyoncethemwho
botheveryoneliveonetherewhom
busyfebruarylivedonlytherebywhose
copyfinallylivingoverthesewoman
dofridaymanypeopletheywomen
doesgasmaybereadthisyes
ta i i i i i i i i i i i i
in 
fg table   exception table for a simple pronunciation program
pp
special action has to be taken with final es
these lengthen and alter the quality
of the preceding vowel so that bit becomes bite and so on
unfortunately if the word has a suffix the e must be detected even though
it is no longer final as in lonely and it is even dropped sometimes
biting em otherwise these would be pronounced lonelly bitting
to make matters worse the suffix may be another word  we do not
want kiteflying to have an extra syllable which rhymes with deaf
although simple procedures can be developed to take care of common
word endings like ly ness d it is difficult to decompose
compound words like wisecrack and bumblebee reliably em but this must
be done if they are not to be articulated with three syllables instead of two
of course there are exceptions to the final e rule
many common words some done livedvu disobey the rule by not
lengthening the main vowel while in other rarer ones anemone
catastrophe epitome the final e is actually pronounced
there are also some complete anomalies fete
pp
mcilroys  system is a superb example of a robust program which takes
a pragmatic approach to these problems accepting that they will never be
fully solved and which is careful to degrade
gracefully when stumped

mcilroy 

the pronunciation of each word is found by a succession of increasingly
desperate trials
lb
np
replace upper by lowercase letters strip punctuation and try again
np
remove final s replace final ie by y and try again
np
reject a word without a vowel
np
repeatedly mark any suffixes with 
np
mark with  probable morph divisions in compound words
np
mark potential long vowels indicated by e
and long vowels elsewhere in the word
np
mark voiced medial s as in busy usual
replace final s if stripped
np
scanning the word from left to right apply lettertosound rules
to word fragments
np
when all else fails spell the word punctuation and all
burp on letters for which no spelling rule exists
le
rf
nr x w mentwreplace final ie by ywexcept when no vowel would remain in  
nr x nlnx
in nxu
ta w mentu wreplace final ie by yu
suffixactionnotes and exceptions
lnxuul
sp
sstrip off final sexcept in context us
strip off final 
iereplace final ie by y
ereplace final e by ewhen it is the only vowel in a word
long e

 ableplace suffix mark asexcept when no vowel would remain in
 ablyshownthe rest of the word
e  d
e  n
e  r
e  ry
e  st
e  y
 ful
 ing
 less
 ly
 ment
 ness
 or

 icplace suffix mark as
 icalshown and terminate
e final e processing
lnxuul
ta i i i i i i i i i i i i
in 
fg table   rules for detecting suffixes for final e processing
pp
table  shows the suffixes which the program recognizes with some comments
on their processing
multiple suffixes are detected and marked in words like
forcefully and spitefulness
this allows silent es to be spotted even when they occur far back in a
word
notice that the suffix marks are available to the wordfragment
rules of table  and are frequently used by them
pp
the program has some
ul
ad hoc
rules for dealing with compound words like racetrack houseboat
these are applied as well as normal suffix splitting so that multiple
decompositions like pacemaker can be accomplished
the rules look for short letter sequences which do not
usually appear in monomorphemic words
it is impossible however to detect every morph boundary
by such rules and the program inevitably makes mistakes
examples of boundaries which go undetected are
edgeways fencepost horseback largemouth wherein
while boundaries are incorrectly inserted into complementary
malevolent proletariat pamela
pp
we now seem to have presented two opposing points of view on the pronunciation
problem
charivarius the dutch poet shows that an enormous number of
exceptional words exist whereas mcilroys program makes do with a tiny
exception dictionary
these views can be reconciled by noting that most of charivarius words
are relatively uncommon
mcilroy tested his program against the  most frequent words in a large
corpus kucera and francis 
and found that  were pronounced correctly if word frequencies were
taken into account

kucera francis 

the notion of correctness is of course a rather subjective one  however
he estimated that on the remaining words the success rate was only 
pp
the system is particularly impressive in that it is prepared to say
anything  if used for example on source programs in a highlevel
computer language it will say the keywords and pronouncable
identifiers spell the other identifiers and even give the names of special
symbols like    correctly
rh morphological analysis
the use of lettertosound rules provides a cheap and fast technique
for pronunciation em the fragment table and exception dictionary for the
program described above occupy only  kbyte of storage and can easily
be kept in solidstate readonly memory
it produces reasonable results if careful attention is paid to rules
for suffixsplitting
however it is inherently limited because it is not possible in general
to detect compound words by simple rules which operate on the lexical
structure of the word
pp
compounds can only be found reliably by using a morph dictionary
this gives the added advantage that syntactic information
can be stored with the morphs to assist with rhythm assignment according
to the chomskyhalle theory
however it was noted earlier that morphs unlike the grammaticallydetermined
morphemes are not very well defined from a linguistic point of view
some morphemic decompositions are obviously not morphic because the
constituents do not in any way resemble the final word
while others where the word is simply a concatenation
of its components are clearly morphic
between these extremes lies a hazy region where what one considers
to be a morph depends upon how complex one is prepared to make the
concatenation rules
the following description draws on techniques used in a project at mit
in which a morphbased pronunciation system has been implemented
lee  allen 

lee 


allen  synthesis of speech from unrestricted text

pp
estimates of the number of morphs in english vary from  to 
although these seem to be very large numbers they are considerably less
than the number of words in the language
for example websters
ul
new collegiate dictionary
th edition contains about  entries
if all forms of the words were included this number would probably
double
pp
there are several classes of morphs with restrictions on the combinations
that occur
a general word has prefixes a root and suffixes as shown in figure 
only the root is mandatory
fc figure 
suffixes usually perform a grammatical role affecting the
conjugation of a verb or declension of a noun or transforming one
part of speech into another
al can make a noun into an adjective while ness performs the reverse
transformation  other
suffixes such as dom or ship only apply to certain parts of
speech nouns in this case but do not change the grammatical
role of the word  such suffixes and all prefixes alter the meaning
of a word
pp
some root morphs cannot combine with other morphs but always stand
alone em for instance this
others called free morphs can either occur on their own or combine
with further morphs to form a word
thus the root house can be joined on either side by another root
such as boat
or by a suffix such as ing
a third type of root morph is one which
ul
must
combine with another morph like crimin ceive
pp
even with a morph dictionary decomposing a word into a sequence
of morphs is not a trivial operation
the process of lexical concatenation often results in a
minor change in the constituents
how big this change is allowed to be governs the morph system being used
for example allen  gives three concatenation rules  a
final e can be omitted as in
ta i
lb
ni
give  ingem  giving
le
the last consonant of the root can be doubled as in
lb
ni
bid  ingem  bidding
le
or a final y can change to an i as in
lb
ni
handy  capem  handicap

allen  synthesis of speech from unrestricted text

le
if these are the only rules permitted the morph dictionary will
have to include multiple versions of some suffixes
for example the plural morpheme s needs to be represented both by
s and es to account for
lb
ni
pea  sem  peas
le
and
lb
ni
baby  esem  babies  using the y em i rule
le
this would not be necessary if a  y em ie  rule were included too
similarly the morpheme ic will include morphs
ic and c the latter to cope with
lb
ni
specify  cem  specific    using the y em i rule
le
furthermore nonmorphemic roots such as galact need to be included because
the concatenation rules do not capture the transformation
lb
ni
galaxy  icem  galactic
le
there is clearly a tradeoff between the size of the morph dictionary
and the complexity of the concatenation rules
pp
since a texttospeech system is presented with alreadyconcatenated
morphs it must be prepared to reverse the effects of the concatenation
rules to deduce the constituents of a word
when two morphs combine with any of the three rules given above
the changes in spelling occur only in the lefthand one
therefore the word is best scanned in a righttoleft direction to
split off the morphs starting with suffixes as mcilroys program does
if the procedure fails at any point one of the three rules is
hypothesized its effect is undone and splitting continues
for example consider the word
lb
ni
grasshoppersem  grass  hop  er  s
le
lee 

lee 

the s is detected first then er these are both stored in
the dictionary as suffixes
the remainder grasshopp cannot be decomposed and does not appear
in the dictionary
so each of the rules above is hypothesized in turn and the
result investigated  the y em i rule is obviously not
applicable  when
the finalconsonantdoubling rule is considered the sequence
grasshop is investigated
shop could be split off this but then the unknown morph gras
would result
the alternative to remove hop leaves a remainder grass which
ul
is
a free morph as desired
thus a unique and correct decomposition is obtained
notice that the procedure would fail if for example grass had
been inadvertently omitted from the dictionary
pp
sometimes several seemingly valid decompositions present themselves
allen 

allen  synthesis of speech from unrestricted text

for example
lb
ni
scarcityem  scar  city
ni
em  scarce  ity  using finale deletion
ni
em  scar  cite  y  using finale deletion
ni
restingem  rest  ing
ni
em  re  sting
ni
bidingem  bide  ing  using finale deletion
ni
em  bid  ing
ni
unionizedem  un  ion  ize  d
ni
em  union  ize  d
ni
windingem  winddnu  ing
ni
em  winddvu  ing
le
the last distinction is important because the pronunciation of wind
depends on whether it is a noun or a verb
ta i i i i i i i i i i i i
pp
several sources of information can be used to resolve these ambiguities
the word structure of figure  together with the division of root
morphs into bound and free ones may eliminate some possibilities
certain letter sequences such as rp do not appear at the beginning
of a word or morph and others never occur at the end
knowledge of these sequences can reject some unacceptable
decompositions em or perhaps more importantly can enable intelligent guesses
to be made in cases where a constituent morph has been omitted from the
dictionary
the grammatical function of suffixes allows suffix sequences to be
checked for compatibility
the syntax of the sentence together with suffix knowledge can
rule out other combinations
semantic knowledge will occasionally be necessary as in the unionized
and winding examples above em compare a winding road with a winding
blow
finally allen  suggests that a preference structure on composition
rules can be used to resolve ambiguity

allen  synthesis of speech from unrestricted text

pp
once the morphological structure has been determined
the rest of the pronunciation
process is relatively easy
a phonetic transcription of each morph may be stored in the morph dictionary
or else lettertosound rules can be used on individual morphs
these are likely to be quite successful because finale processing can be
now be done with confidence  there are no hidden final es in the middle
of morphs
in either case the resulting phonetic transcriptions of the individual morphs
must be concatenated to give the transcription of the complete word
although some contextual modification has to be accounted for
it is relatively straightforward and easy to predict
for example the plural morphs s and es can be realized phonetically
by
ul
uh z
ul
s
or
ul
z
depending on context
similarly the pasttense suffix ed may be rendered as
ul
uh d
ul
t
or
ul
d
the suffixes ion and ure sometimes cause modification of the previous
morph  for example
lb
ni
act  ion  em  c
ul
a k tc
   ion  em  c
ul
a k sh uh n
le
pp
the morph dictionary does not remove the need for a lexicon of exceptional
words
the irregular finale words mentioned earlier done anemone fete
need to be treated on an individual basis
as do words such as quadruped which have misleading endings
it should not be decomposed as quadruped
rh pronunciation of languages other than english
texttospeech systems for other languages have been reported in
the literature
for example french esperanto
italian russian spanish and german are covered
by lesmo
ul
et al
 oshaughnessy
ul
et al
 sherwood 
mangold and stall 

lesmo 


oshaughnessy lennig mermelstein divay 


sherwood 


mangold stall 

generally speaking these present fewer difficulties than does english
esperanto is particularly easy because each letter in its orthography
has only one sound making the pronunciation problem trivial
moreover stress in polysyllabic words always occurs on the penultimate
syllable
pp
it is tempting and often sensible when designing a synthesis system for
english to use an utterance representation somewhere between phonetics and
ordinary spelling
this may happen in practice even if it is not intended  a user finding
that a given word is pronounced incorrectly will alter the spelling to
make it work
the word english spelling alphabet dewey  amongst others haas 
is a simplified and apparently natural scheme which was developed by the
spelling reform movement

dewey 


haas 

it maps very simply on to a phonetic representation just like esperanto
however it can provide little help with the crucial problem of stress
assignment except perhaps by explicitly indicating reduced vowels
sh   discussion
pp
this chapter has really only touched the tip of a linguistic iceberg
i have given some examples of representations rules algorithms
and exceptions to make the concepts more tangible but a whole mass of
detail has been swept under the carpet
pp
there are two important messages that are worth reiterating once more
the first is that the representation of the input em that is
whether it be a concept
in some semantic domain a syntactic description of an utterance a
decomposition into morphs plain text or some contrived respelling of it em
is crucial to the quality of the output
almost any extra information about the utterance can be taken into account
and used to improve the speech
it is difficult to derive such information if it is not provided explicitly
for the process of climbing the tree from text to semantic representation is
at least as hard as descending it to a phonetic transcription
pp
secondly simple algorithms perform remarkably well em witness the
punctuationdriven intonation assignment scheme and word fragment rules
for pronunciation
however the combined degradation contributed by several imperfect
processes is likely to impair speech quality very seriously
and great complexity is introduced when these simple algorithms are
discarded in favour of more sophisticated ones
there is for example a world of difference between a pronunciation
program that copes with  of common words and one that deals correctly
with  of a random sample from a dictionary
pp
some of the options that face the system designer are recapitulated in
figure 
fc figure 
starting from text one can take the simple approach of lexicallybased
suffixsplitting lettertosound rules and prosodics derived
from punctuation to generate a phonetic transcription
this will provide a cheap system which is relatively easy to implement
but whose speech quality will probably not be acceptable to any but the
most dedicated listener
such as a blind person with no other access to reading material
pp
the biggest improvement in speech quality from such a system would
almost certainly come from more intelligent prosodic
control em particularly of intonation
this unfortunately is also by far the most difficult to make unless
intonation contours tonic stresses and tonegroup boundaries are handcoded
into the input
to generate the appropriate information from text one has to climb to the
upper levels in figure  em and even when these are reached the problems
are by no means over
still let us climb the tree
pp
for syntax analysis partofspeech information is needed and for this
the grammatical roles of individual words in the text must be ascertained
a morph dictionary is the most reliable way to do this
a linguist may prefer to go from morphs to syntax by way of morphemes
but this is not necessary for the present purpose
just the information that
the morph went is a verb can be stored in the dictionary instead
of its decomposition  went  go  ed
pp
now that we have the morphological structure of the text stress assignment rules
can be applied to produce more accurate speech rhythms
the morph decomposition will also allow improvements to be made to the
pronunciation particularly in the case of silent es in compound words
but the ability to assign intonation has hardly been improved at all
pp
let us proceed upwards
now the problems become really difficult
a semantic representation of the text is needed but what exactly does this
mean
we certainly must have
ul
morphemic
knowledge for now the fact that went is a derivative of go
rather than any other verb becomes crucial
very well let us augment the morph dictionary with morphemic information
but this does not attack the problem of semantic representation
we may wish to resolve pronoun references to help assign stress
parts of the problem are solved in principle
and reported in the artificial intelligence
literature but if such an ability is incorporated into the speech
synthesis system it will become enormously complicated
in addition we have seen that knowledge of antitheses in the text will greatly
assist intonation assignment but procedures for extracting this
information constitute a research topic in their own right
pp
now step back and take a topdown approach
what could we do with this semantic understanding and knowledge of the structure
of the discourse if we had it
suppose the input were a concept in some as yet undetermined representation
what are the
ul
acoustic
manifestations of such highlevel features as anaphoric references or
antithetical comparisons
of parenthetical or satirical remarks
of emotions  warmth sarcasm sadness and despair
can we program the art of elocution
these are good questions 
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
books on pronunciation give surprisingly little help in designing
a texttospeech procedure
the best aid is a good online dictionary and flexible software to
search it and record rules examples and exceptions
here are some papers that describe existing systems
lb nn
ainsworth

ds a ainsworth wa
ds d 
ds t a system for converting text into speech
ds j ieee trans audio and electroacoustics
ds v au
ds p 
nr p 
nr t 
nr a 
nr o 
  journalarticle
inn
inn
colby

ds a colby km
as a  christinaz d
as a  and graham s
ds d 
ds k 
ds t a computerdriven personal portable and intelligent speech prosthesis
ds j computers and biomedical research
ds v 
ds p 
nr p 
nr t 
nr a 
nr o 
  journalarticle
inn
inn
elovitz

ds a elovitz hs
as a  johnson rw
as a  mchugh a
as a  and shore je
ds d 
ds k 
ds t lettertosound rules for automatic translation of english text to phonetics
ds j ieee trans acoustics speech and signal processing
ds v assp
ds n 
ds p 
nr p 
ds o december
nr t 
nr a 
nr o 
  journalarticle
inn
inn
kooi

ds a kooi r
as a  and lim wc
ds d 
ds t an online minicomputerbased system for reading printed text aloud
ds j ieee trans systems man and cybernetics
ds v smc
ds p 
nr p 
ds o january
nr t 
nr a 
nr o 
  journalarticle
inn
inn
umeda

ds a umeda n
as a  and teranishi r
ds d 
ds k 
ds t the parsing program for automatic texttospeech synthesis developed at the electrotechnical laboratory in 
ds j ieee trans acoustics speech and signal processing
ds v assp
ds n 
ds p 
nr p 
ds o april
nr t 
nr a 
nr o 
  journalarticle
inn
inn
umeda

ds a umeda n
ds d 
ds k 
ds t linguistic rules for texttospeech synthesis
ds j proc ieee
ds v 
ds n 
ds p 
nr p 
ds o april
nr t 
nr a 
nr o 
  journalarticle
inn
inn
le nn
eq
delim 
en
ch   designing the mancomputer dialogue
ds rt the mancomputer dialogue
ds cx principles of computer speech
pp
interactive computers are being used more and more by nonspecialist people
without much previous computer experience
as processing costs continue to decline the overall expense of providing
highly interactive systems
becomes increasingly dominated by terminal and communications equipment
taken together these two factors highlight the need for easytouse
lowbandwidth interactive terminals that make maximum use of the existing
telephone network for remote access
pp
speech output can provide versatile feedback from a computer at very low
cost in distribution and terminal equipment  it is attractive from several
points of view
terminals em telephones em are invariably in place already
people without experience of computers are accustomed to their use
and are not intimidated by them
the telephone network is cheap to use and extends all over the world
the touchtone keypad or a portable tone generator
provides a complementary data input device which will do for many
purposes until the technology of speech recognition becomes better developed
and more widespread
indeed many applications em especially information retrieval ones em need
a much smaller bandwidth from user to computer than in the reverse direction
and voice output combined with restricted keypad entry provides a good match
to their requirements
pp
there are however severe problems in implementing natural and useful
interactive systems using speech output
the eye can absorb information at a far greater rate than can the ear
you can scan a page of text in a way which has no analogy in auditory terms
even so it is difficult to design a dialogue which allows you to search
computer output visually at high speed
in practice scanning a new report is often better done at your desk
with a printed copy than at a computer terminal with a viewing program
although this is likely to change in the near future
pp
with speech the problem of organizing output becomes even harder
most of the information we learn using our ears is presented in a
conversational way either in facetoface discussions or over the telephone
verbal but nonconversational presentations as in the
university lecture theatre are known to be a rather inefficient way
of transmitting information
the degree of interaction is extremely high even in a telephone conversation
and communication relies heavily on speech gestures such as hesitations
grunts and pauses on prosodic features such as intonation pitch range
tempo and voice quality and on conversational gambits such as interruption
and long silence
i emphasized in the last two chapters the rudimentary state of knowledge
about how to synthesize
prosodic features and the situation is even worse
for the other paralinguistic phenomena
pp
there is also a very special problem with voice output namely the transient
nature of the speech signal
if you miss an utterance its gone
with a visual display unit at least the last few interactions usually remain
available
even then it is not uncommon to look up beyond the top of the screen and
wish that more of the history was still visible
this obviously places a premium on a voice response systems
ability to repeat utterances
moreover the dialogue designer must do his utmost to ensure that the user
is always aware of the current state of the interaction
for there is no opportunity to refresh the memory by glancing at earlier
entries and responses
pp
there are two separate aspects to the mancomputer interface in a voice
response system
the first is the relationship between the system and the end user
that is the consumer of the synthesized dialogue
the second is the relationship between the system and the applications
programmer who creates the dialogue
these are treated separately in the next two sections
we will have more to say about the former aspect
for it is ultimately more important to more people
but the applications programmers view is important too for without him
no systems would exist
the technical difficulties in creating synthetic dialogues
for the majority of voice systems probably
explain why speech output technology is still greatly underused
finally we look at techniques for using small keypads such as those on
touchtone telephones
for they are an essential part of many voice response systems
sh   programming principles for natural interaction
pp
special attention must be paid to be details of the manmachine interface
in speechoutput systems
this section summarizes experience of human factors considerations
gained in developing the remote
telephone enquiry service described in chapter  witten and madams 
which employs an ordinary touchtone keypad for input in conjunction with
synthetic voice response

witten madams  telephone enquiry service

most of the principles which emerged were the result of natural evolution
of the system and were not clear at the outset
basically they stem from the fact that speech is both more intrusive
and more ephemeral than writing and so they are applicable in general to
speech output information retrieval systems with keyboard or even voice
input
be warned however that they are based upon casual observation and
speculation rather than empirical research
there is a desperate need for proper studies of user psychology in speech
systems
rh echoing
most alphanumeric input peripherals echo on a characterbycharacter basis
although one can expect quite a high proportion of mistakes with
unconventional keyboards especially when entering alphabetic data on a
basically numeric keypad audio character echoing is distracting and annoying
if you type  and the computer echoes
lb
ni
one  two  three
le
after the individual keypresses it is liable to divert your
attention for voice output is much more intrusive than a purely visual echo
pp
instead an immediate response to a completed input line is preferable
this response can take the form or a reply to a query or if successive
data items are being typed confirmation of the data entered
in the latter case it is helpful if the information can be generated in
the same way that the user himself would be likely to verbalize it
thus for example when entering numbers
lb
nr x wcomputer
nr x wuser
ni
userhnxunxu  is the endofline character
ni
computer one hundred and twentythree
le
for a query which requires lengthy processing the input should be
repeated in a neat meaningful format to give the user a chance to abort
the request
rh retracting actions
because commands are entered directly without explicit confirmation
it must always be easy for the user to revoke his actions
the utility of an undo command is now commonly recognized for
any interactive system and it becomes even more important in speech
systems because it is easier for the user to lose his place in the
dialogue and so make errors
rh interrupting
a command which interrupts output and returns to a known state
should be recognized at every level of the system
it is essential that voice output be terminated immediately
rather than at the end of the utterance
we do not want the user to live in fear of the system embarking on
a long boring monologue that is impossible to interrupt
again the same is true of interactive dialogues which do not use speech
but becomes particularly important with voice response because it takes
longer to transmit information
rh forestalling prompts
computergenerated prompts must be explicit and frequent enough
to allow new users to understand what they are expected to do
experienced users will type ahead quite naturally
and the system should suppress unnecessary prompts under these conditions
by inspecting the input buffer before prompting
this allows the user to concatenate frequentlyused commands into chunks whose
size is entirely at his own discretion
pp
with the abovementioned telephone enquiry service for example
it was found that people often took advantage of the promptsuppression
feature to enter their
user number password and required service number as a single keying
sequence
as you becomes familiar with a service you quickly and easily learn to
forestall expected prompts by typing ahead
this provides a very natural way for the system to adapt itself automatically
to the experience of the user
new users will naturally wait to be prompted and proceed through the dialogue
at a slower and more relaxed pace
pp
suppressing unnecessary prompts is a good idea in any interactive system
whether or not it uses the medium of speech em although it is hardly ever done
in conventional systems
it is particularly important with speech however because an unexpected
or unwanted
prompt is quite distracting and it is not so easy to ignore it as it is
with a visual display
furthermore speech messages usually take longer to present
than displayed ones so that the user is distracted for more time
rh information units
lengthy computer voice responses are inappropriate for conveying information
because attention wanders if one is not actively involved in the conversation
a sequential exchange of terse messages each designed to dispense one
small unit of information forces the user to take a meaningful part in the
dialogue
it has other advantages too allowing a higher degree of inputdependent
branching and permitting rapid recovery from errors
pp
the following example from the acidosis program an audio response system
designed to help physicians to diagnose acidosis is a good example
of what
ul
not
to do
lb
chime a value of sixpointzerozero has been entered for ph
this value is impossible
to continue the program enter a new value for ph in the range
between sixpointsix and eightpointzero
beep dah beepbeep  smith and goodwin 

smith goodwin 

le
the use of extraneous noises for example a chime heralds an error message
and a beep dah beepbeep requests data input in the form
digitpointdigitdigit
was thought necessary in the acidosis program to keep the user awake
and help him with the format of the interaction
rather than a long monologue like this
it seems much better to design a sequential interchange of terse messages
so that the caller can be guided into a state where he can rectify his error
for example
lb
nf
ne
nr x wcomputer
nr x wcaller
callerhnxunxu 
computer entry out of range
callerhnxunxu   persists
computer the minimum acceptable ph value is 
callerhnxunxu 
computer the maximum acceptable ph value is 
fi
le
this dialogue allows a rapid exit from the error situation in the likely
event that the entry has simply been mistyped
if the error persists the caller is given just one piece of information
at a time and forced to continue to play an active role in the interaction
rh input timeouts
in general input timeouts are dangerous because they introduce apparent
acausality in the system seen by the user
a case has been reported where a user became highly agitated and refused
to go near the terminal again after her first timedout prompt
she had been quietly thinking what to do and the terminal suddenly
interjecting and making its
own suggestions was just too much for her gaines and facey 

gaines facey 

pp
however voice response systems lack the satisfying visual feedback
of endofline on termination of an entry
hence a timedout reminder is appropriate if a delay occurs after some
characters have been entered
this requires the operating system to support a characterbycharacter mode
of input rather than the usual linebyline mode
rh repeat requests
any voice response system must support a universal repeat last utterance
command because old output does not remain visible
a fairly sophisticated facility is desirable as repeat requests are
very frequent in practice
they may be due to a simple inability to understand a response
to forgetting what was said or to distraction of attention em which is
especially common with office terminals
pp
in the telephone enquiry service two distinct commands were employed
one to repeat the last utterance in case of misrecognition
and the other to summarize the current state of the interaction
in case of distraction
for the former it is essential to avoid simply regenerating an utterance
identical with the last
some variation of intonation and rhythm is needed to prevent an annoying
stereotyped response
a second consecutive repeat request should trigger a paraphrased reply
an error recovery sequence could be used which presented the misunderstood
information in a different way with more interaction but experience
indicates that this is of minor importance especially if information units
are kept small anyway
to summarize the current state of the interaction in response to the second
type of repeat command necessitates the system maintaining a model of
the user
even a poor model like a record of his last few transactions and their
results is well worth having
rh varied speech
synthetic speech is usually rather dreary to listen to
successive utterances with identical intonations should be carefully avoided
small changes in speaking rate pitch range and mean pitch level
all serve to add variety
unfortunately little is known at present about the role of intonation in
interactive dialogue although this is an active research area and
new developments can be expected for a detailed report of a recent
research project relevant to this topic see brown
ul
et al


brown currie kenworthy 

however even random variations in certain parameters of the pitch contour
are useful to relieve the tedium of repetitive intonation patterns
sh   the applications programming environment
pp
the comments in the last section are aimed at the applications programmer
who is designing the dialogue and constructing the interactive system
but what kind of environment should
ul
he
be given to assist with this work
pp
the best help the applications programmer can have is a speech generation
method which makes it easy for him to enter new utterances and modify
them online in cutandtry attempts to render the manmachine dialogue
as natural as possible
this is perhaps the most important advantage of synthesizing speech by rule
from a textual representation
if encoded versions of natural utterances are stored it becomes quite
difficult to make minor modifications to the dialogue in the light of
experience with it for a recording session must be set up
to acquire new utterances
this is especially true if more than one voice is used or if the
voice belongs to a person who cannot be recalled quickly by the programmer
to augment the utterance library
even if it is his own voice there will still be delays for recording
speech is a realtime job which usually needs a standalone processor
and if data compression is used a substantial amount of computation will
be needed before the utterance is in a useable form
pp
the broad phonetic input required by segmental speech synthesisbyrule
systems is quite suitable for utterance representation
utterances can be entered quickly from a standard computer terminal
and edited as text files
programmers must acquire skill in phonetic transcription
but this is a small inconvenience
the art is easily learned in an interactive situation where the effect
of modifications to the transcription can be heard immediately
if allophones must be represented explicitly in the input then the
programmers task becomes considerably more complicated because of the
combinatorial explosion in trialanderror modifications
pp
plain text input is also quite suitable
a significant rate of error is tolerable if immediate audio feedback
of the result is available so that the operator can adjust his text
to suit the pronunciation idiosyncrasies of the program
but it is acceptable and indeed preferable if prosodic features are
represented explicitly in the input rather than being assigned automatically
by a computer program
pp
the application of voice response to interactive computer dialogue is
quite different to the problem of reading aloud from text
we have seen that a major concern with reading machines is how to glean
information about intonation rhythm emphasis tone of voice and so on
from an input of ordinary english text
the significant problems of semantic processing utilization of pragmatic
knowledge and syntactic analysis do not fortunately arise in interactive
information retrieval systems
in these the end user is communicating with a program which has been
created by a person who knows what he wants it to say
thus the major difficulty is in
ul
describing
the prosodic features rather than
ul
deriving
them from text
pp
speech synthesis by rule is a subsidiary process to the main interactive
procedure
it would be unwise to allow
the updating of resonance parameter tracks to be interrupted by
other calls on the system and so the synthesis process needs to be executed
in real time
if a standalone processor is used for the interactive dialogue it may
be able to handle the synthesis rules as well
in this case the speechbyrule program could be a library procedure
if the system is implemented in a compiled language
an interesting alternative with an interpretivelanguage implementation
such as basic is to alter the language interpreter to add a new
command speak which simply transfers a string representing an utterance
to an asynchronous process which synthesizes it
however there must be some way for an intepreted program to abort the
current synthesis in the event of an interrupt signal from the user
pp
if the main computer system is timeshared the synthesisbyrule
procedure is best executed by an independent processor
for example a bit microcomputer controlling a hardware
formant synthesizer has been used to run the
isp system in real time without too much difficulty witten and abbess 

witten abbess 

an important task is to define an interface between the two which
allows the main process to control relevant aspects of the prosody of
the speech in a way which is appropriate to the state of the interaction
without having to bother about such things as matching the intonation contour
to the utterance and the details of syllable rhythm
hallidays notation appears to be quite suitable for this purpose
pp
if there is only one synthesizer on the system there will be no
difficulty in addressing it
one way of dealing with multiple synthesizers is to treat them as
assignable devices in the same way that nonspooling peripherals
are in many operating systems
notice that the data rate to the synthesizer is quite low
if the utterance is represented as text with prosodic markers
and can easily be handled by a lowspeed asynchronous serial line
pp
the votrax mli synthesizer which is discussed in the next chapter has an
interface which interposes it between a visual display unit and the serial
port that connects it to the computer
the vdu terminal can be used quite normally except that a special sequence
of two control characters will cause votrax to intercept the following
message up to another control character and interpret it as speech
the fact that the characters which specify the spoken message do not appear
on the vdu screen means that the operation is invisible to the user
however this transparency can be inhibited by a switch on the synthesizer
to allow visual checking of the soundsegment character sequence
pp
votrax buffers up to  sound segments which is sufficient to generate
isolated spoken messages
for longer passages it can be synchronized with the constantrate
serial output using the modem control lines of the serial interface
together with appropriate devicedriving software
pp
this is a particularly convenient interfacing technique in cases when the
synthesizer should always be associated with a certain terminal 
as an example of how it can be used
one can arrange files each of whose lines contain a printed message
together with its votrax equivalent bracketed by the appropriate
control characters
when such a file is listed or examined with an editor program the lines
appear simultaneously in spoken and typed english
pp
if a phonetic representation is used for utterances with realtime
synthesis using a separate process or processor it is easy for
the programmer to fiddle about with the interactive dialogue to get
it feeling right
for him each utterance is just a textual string which
can be stored as a string constant within his program just as a vdu prompt
would be  he can edit it as part of his program and print it to
the speech synthesis device to hear it
there are no more technical problems to developing an interactive dialogue
with speech output than there are for a conventional interactive program
of course there are more human problems and the points discussed
in the last section should always be borne in mind
sh   using the keypad
pp
one of the greatest advantages of speech output from computers is the
ubiquity of the telephone network and the possibility of using it without
the need for special equipment at the terminal
the requirement for input as well as output obviously presents something of a problem
because of the restricted nature of the telephone keypad
pp
figure  shows the layout of the keypad
fc figure 
signalling is achieved by dualfrequency tones
for example if key  is pressed sinusoidal components at  hz and  hz
are transmitted down the line
during the process of dialling these are received by the telephone exchange
equipment which assembles the digits that form a number and attempts to route
the call appropriately
once a connection is made either party is free to press keys if desired
and the signals will be transmitted to the other end
where they can be decoded by simple electronic circuits
pp
dial telephones signal with closelyspaced dial pulses
one pulse is generated for a  two for a  and so on
obviously ten pulses are generated for a  rather than none  unfortunately
once the connection is made it is difficult to signal with dial pulses
they cannot be decoded reliably at the other end because the telephone
network is not designed to transmit such low frequencies
however handheld tone generators can be purchased for use with dial
telephones
although these are undeniably extra equipment and one purpose of using speech
output is to avoid this they are very cheap and portable compared with other
computer terminal equipment
pp
the small number of keys on the telephone pad makes it rather difficult
to use for communicating with computers
provision is made for  keys but only  are implemented em the others
may be used for some military purposes
of course if a separate tone generator is used then advantage can be taken
of the extra keys but this will introduce incompatibility with those
who use unmodified touchtone phones
more sophisticated terminals are available which extend the keypad em such
as the displayphone of northern telecommunications
however they are designed as a complete communications terminal and
contain their own visual display as well
rh keying alphabetic data
figure  shows the nearuniversal scheme for overlaying alphabetic letters
on to the telephone keypad
fc figure 
since more than one symbol occupies each key it is obviously necessary
to have multiple keystrokes per character if the input sequence is to be
decodable as a string of letters
one way of doing this is to depress the appropriate button the number of
times corresponding to the position of the letter on it
for example to enter the letter l the user would key the  button
three times in rapid succession
keying rhythm must be used to distinguish the four entries j j j
j k k j and l unless one of the bottom three buttons is used
as a separator
a different method is to use   and  as shift keys to indicate whether
the first second or third letter on a key is intended
then  would represent l
alternatively the shift could follow the key instead of preceding it
so that  represented l
pp
if numeric as well as alphabetic information may be entered a modeshift
operation is commonly used to switch between numeric and alphabetic modes
pp
the relative merits of these three methods multiple depressions shift
key prefix and shift key suffix have been investigated
experimentally kramer 

kramer 

the results were rather inconclusive
the first method seemed to be slightly inferior in terms of user accuracy
it seemed that preceding rather than following shifts gave higher accuracy
although this is perhaps rather counterintuitive and may have been
fortuitous
the most useful result from the experiments was that users exhibited
significant learning behaviour and a training period of at least two hours
was recommended
operators were found able to key at rates of at least three to four
characters per second and faster with practice
pp
if a greater range of characters must be represented then the coding problem
becomes more complex
figure  shows a keypad which can be used for entry of the full character
standard uppercase ascii alphabet shew 

shew 

fc figure 
the system is intended for remote vocabulary updating in a phoneticallybased
speech synthesis system
there are three modes of operation  numeric alphabetic and symbolic
these are entered by   and  respectively
two function modes signalled by  and  allow some
rudimentary lineediting and monitor facilities to be incorporated
lineediting commands include character and line delete and two kinds of
readback commands em one tries to pronounce the words in a line
and the other spells out the characters
the monitor commands allow the user to repeat the effect of the last input line
as though he had entered it again to order the system to read back the
last complete output line and to query time and system status
rh incomplete keying of alphanumeric data
it is obviously going to be rather difficult for the operator to key
alphanumeric information unambiguously on a key pad
in the description of the telephone enquiry service in chapter 
it was mentioned that singlekey entry can be useful for alphanumeric data
if the ambiguity can be resolved by the computer
if a multiplecharacter entry is known to refer to an item on a given
list the characters can be keyed directly according to the coding scheme
of figure 
pp
under most circumstances no ambiguity will arise
for example table  shows the keystrokes that would be entered for the
first  letter words in an english dictionary
only two clashes occur em between  adore and afore and
agate and agave
rf
nr x wabeam  u
nr x w    u
nr x nxunxunxunxunxunxunxunxunxuwu
nr x nlnx
in nxu
ta nxu nxu nxu nxu nxu nxu nxu nxu nxu
lnxuul
sp
abackabideadageadoreafter
abaftabodeadaptadornagain
abaseabortadderadultagape
abashaboutaddleadustagate
abateaboveadeptaegeragave
abbeyabuseadieuaegisagent
abbotabyssadmitaerieagile
abeamacornadmixaffixaglet
abeleacridadobeafootagony
abhoractoradoptaforeagree
lnxuul
in 
ta i i i i i i i i i i i i
fg table   keying equivalents of some words
as a more extensive example in a dictionary of  words just under 
ambiguities  of words were discovered
such ambiguities would have to be resolved interactively by the system explaining
its dilemma and asking the user for a choice
notice incidentally that although the keyed sequences do not have the same
lexicographic order as the words
no extra cost will be associated with the tablesearching
operation if the dictionary is stored in inverted form with each legal
number pointing to its english equivalent or equivalents
pp
a command language syntax is also a powerful way of disambiguating
keystrokes entered
figure  shows the keypad layout for a telephone voice calculator
newhouse and sibley 

newhouse sibley 

fc figure 
this calculator provides the standard arithmetic operators
ten numeric registers a range of predefined mathematical functions
and even the ability for a user to enter his own functions over the
telephone
the number representation is fixedpoint with user control through a system
function over the precision
input of numbers is free format
pp
despite the power of the calculator language the dialogue is defined
so that each keystroke is unique in context and never has to be disambiguated
explicitly by the user
table  summarizes the command language syntax in an informal and rather
heterogeneous notation
rf
nr x iiwsome functions do not need the value partu
nr x nlnx
in nxu
ta i i
lnxuul
constructdefinitionexplanation
lnxuul
sp
calculationa sequence of operations followed by a
call to the system function  fie  x  i  tfr
sp
operationadd or subtract or
multiply or divide or
function or clear or
erase or answer or
displaylast or display or
repeat or cancel
sp
add  value    or      function
sp
subtract
multiplysimilar to add
divide
sp
valuenumericvalue  or  firegisterfr singledigit
sp
numericvaluea sequence of keystrokes like
          or            or        
sp
functionfifunctionfr name    value  
some functions do not need the value part
sp
namea sequence of keystrokes like
fis  i  nfr  or  fie  x  i  tfr  or  fim  y  f  u  n  cfr
sp
clearficlear registerfr singledigit  
clears one of the  registers
sp
erasefierasefr  undoes the effect of the last operation
sp
answerfianswer registerfr singledigit  
reads the contents of a register
sp
displaylast
displaythese provide repeat facilities
repeat
sp
cancelaborts the current utterance
lnxuul
in 
ta i i i i i i i i i i i i
fg table   syntax for a telephone calculator
a calculation is a sequence of operations followed by an exit function call
there are twelve different operations one for each button on the keypad
actually two of them em
ul
cancel
and
ul
function
em share the same key so that  can be reserved for use as a
separator but the context ensures that they cannot be confused by the system
pp
six of the operations give control over the dialogue
there are three different repeat commands a command called
ul
erasec

which undoes the effect of the last operation
one which reads out the value of a register
and one which aborts the current utterance
four more commands provide the basic arithmetic operations of add
subtract multiply and divide
the operands of these may be keyed literal numbers or register values
or function calls
a further command clears a register
pp
it is through functions that the extensibility of the language is achieved
a function has a name like sin exit myfunc which is keyed with an
appropriate singlekeypercharacter sequence namely   
respectively
one function define allows new ones to be entered
another loop repeats sequences of operations
test incorporates arithmetic testing
the details of these are not important  what is interesting is the evident
power of the calculator
pp
for example the keying sequence
lb
ni
                                        
le
would be decoded as
lb
ni
ul
clearc
          c
ul
display  erasec
    
le
one of the difficulties with such a tight syntax is that almost any sequence
will be intepreted as a valid calculation em syntax errors are nearly
impossible
thus a small mistake by the user can have a catastrophic effect on the
calculation
here however speech output gives an advantage over conventional
characterbycharacter echoing
on visual displays
it is quite adequate to echo syntactic units as they are decoded instead
of echoing keys as they are entered
it was suggested earlier in this chapter that confirmation of entry
should be generated in the same way that the user would be likely to
verbalize it himself
thus the synthetic voice could respond to the above keying sequence as
shown in the second line except that the
ul
display
command would also state the result
and possibly summarize the calculation so far
numbers could be verbalized as one hundred and twentythree
instead of as one  two  three
note however that this will make it necessary to await the  terminator
after numbers and function names before they can be echoed
sh   references
lb nnnn

list

le nnnn
sh   further reading
pp
there are no books which relate techniques of mancomputer dialogue
to speech interaction
the best i can do is to guide you to some of the standard works on
interactive techniques
lb nn
gilb

ds a gilb t
as a  and weinberg gm
ds d 
ds t humanized input
ds i winthrop
ds c cambridge massachusetts
nr t 
nr a 
nr o 
  book
inn
this book is subtitled techniques for reliable keyed input
and considers most aspects of the problem of data entry by
professional key operators
inn
martin

ds a martin j
ds d 
ds t design of mancomputer dialogues
ds i prenticehall
ds c englewood cliffs new jersey
nr t 
nr a 
nr o 
  book
inn
martin concerns himself with all aspects of mancomputer dialogue
and the book even contains a short chapter on  the use of
voice response systems
inn
smith

ds a smith ht
as a  and green trgeditors
ds d 
ds t human interaction with computers
ds i academic press
ds c london
nr t 
nr a 
nr o 
  book
inn
a recent collection of contributions on mancomputer systems and programming
research
inn
le nn
eq
delim 
en
ch   commercial speech output devices
ds rt commercial speech output devices
ds cx principles of computer speech
pp
this chapter takes a look at four speech output peripherals that are
available today
it is risky in a book of this nature to descend so close to the technology
as to discuss particular examples of commercial products
for such information becomes dated very quickly
nevertheless having covered the principles of various types of speech
synthesizer and the methods of driving them from widely differing utterance
representations it seems worthwhile to see how these principles are
embodied in a few products actually on the market
pp
developments in electronic speech devices are moving so fast that it is
hard to keep up with them and the newest technology today will undoubtedly
be superseded next year
hence i have not tried to choose examples from the very latest technology
instead this chapter discusses synthesizers which exemplify rather different
principles and architectures in order to give an idea of the range of options
which face the system designer
pp
three of the devices are landmarks in the commercial adoption of speech
technology and have stood the test of time
votrax was introduced in the early s and has been reimplemented
several times since in an attempt to cover different market sectors
the computalker appeared in 
it was aimed primarily at the burgeoning computer hobbies market
one of its most farreaching effects was to stimulate the interest of
hobbyists always eager for new lowcost peripherals in speech synthesis
and so provide a useful new source of experimentation and expertise
which will undoubtedly help this heretofore rather esoteric discipline to
mature
computalker is certainly the longestlived and probably still the most
popular hobbyists speech synthesizer
the texas instruments speech synthesis chip brought speech output technology to the
consumer
it was the first singlechip speech synthesizer and is still the biggest
seller
it forms the heart of the speak n spell talking toy which appeared in
toyshops in the summer of 
although talking calculators had existed several years before they were
exotic gadgets rather than household toys
sh   formant synthesizer
pp
the computalker is a straightforward implementation of a serial formant
synthesizer
a block diagram of it is shown in figure 
fc figure 
in the centre is the main vocal tract path with three formant filters
whose resonant frequencies can be controlled individually
a separate nasal branch in parallel with the oral one is provided
with a nasal formant of fixed frequency
it is less important to allow for variation of the nasal formant
frequency than it is for the oral ones because the size and
shape of the nasal tract is relatively fixed
however it is essential to control the nasal amplitude in particular to turn
it off during nonnasal sounds
computalker provides independent oral and nasal amplitude parameters
pp
unvoiced excitation can be passed through the main vocal tract
through the aspiration amplitude control ah
in practice the voicing amplitudes av and an will probably always be zero when ah
is nonzero for physiological constraints prohibit simultaneous voicing
and aspiration
a second unvoiced excitation path passes through a fricative formant filter
whose resonant frequency can be varied and has its amplitude independently
controlled by af
rh control parameters
table  summarizes the nine parameters which drive computalker
rf
nr x waddresswfundamental frequency of voicingw bitswlogarithmicw hz
nr x nlnx
in nxu
ta wu waddressu wfundamental frequency of voicingu w bitsu wlogarithmicu
addressmeaningwidthrange
lnxuul
sp
avamplitude of voicing bits
annasal amplitude bits
ahamplitude of aspiration bits
afamplitude of frication bits
fvfundamental frequency of voicing bitslogarithmic hz
fformant  resonant frequency bitslogarithmic hz
fformant  resonant frequency bitslogarithmic hz
fformant  resonant frequency bitslogarithmic hz
fffricative resonant frequency bitslogarithmic hz
not used
not used
not used
not used
not used
not used
swaudio onoff switch bit
lnxuul
in 
ta i i i i i i i i i i i i
fg table   computalker control parameters
four of them control amplitudes while the others control frequencies
in the latter case the parameter value is logarithmically related to
the actual frequency of the excitation fv or resonance f f f ff
the ranges over which each frequency can be controlled is shown in the table
an independent calibration of one particular computalker has shown that
the logarithmic specifications are met remarkably well
pp
each parameter is specified to computalker as an bit number
parameters are addressed by a bit code and so a total of  bits
is transferred in parallel to computalker from the computer
for each parameter update
parameters  to  are unassigned reserved for future expansion is
the official phrase and the last parameter sw governs the position of
an audio onoff switch
pp
computalker does not contain a clock that is accessible to the user
and so the timing of parameter updates is entirely up to the host computer
typically a  msec interval between frames is used
with interrupts generated by a separate timer
in fact the frame interval can be anywhere between  msec and  msec
and can be changed to alter the rate of speaking
however it is rather naive to view fast speech as slow
speech speeded up by a linear time compression for in human
speech production the rhythm changes and elisions occur in a rather
more subtle way
thus it is not particularly useful to be able to alter the frame rate
pp
at each interrupt the host computer transfers values for all of the nine
parameters to computalker a total of  data bits
in theory perhaps it is only necessary to transmit those parameters
whose values have changed but in practice all of them should be updated
regardless
this is because the parameters are stored for the duration of the frame
in analogue sampleandhold devices  essentially the parameter value
is represented as the charge on a capacitor
in time em and it takes only a short time em the values drift
although the drift over  msec is insignificant it becomes very
noticeable over longer time periods
if parameters are not updated at all the result is a
whooosh sound up to maximum amplitude in a period of a second or two
hence it is essential that computalker be serviced by the computer regularly
to update all its parameters
the audio onoff switch is provided so that the computer can turn off
the sound directly if another program which does not use the device
is to be run
rh filter implementation
it is hard to get definite information on the implementation
of computalker
because it is a commercial device circuit diagrams are not published
it is certainly an analogue rather than a digital implementation
the designer suggests that a configuration like that of figure  is used
for the formant filters rice 

rice  byte

fc figure 
control is obtained over the resonant frequency by varying the resistance
at the bottom in sympathy with the parameter value
the middle two operational amplifiers can be modelled by a resistance
rk in the forward path where k is the digital control value
this gives the circuit in figure  which can be analysed to obtain
the transfer function
lb
eq
  k over rr sub  c sub  c sub     r sub  c sub  s  over
 s sup  
  over r sub  c sub   k r sub  over rr sub  c sub s 
k over rr sub  c sub  c sub   
en
le
fc figure 
pp
this expression has a dc gain of  and the denominator is similar to those
of the analogue formant resonators discussed in chapter 
however unlike them the transfer function has a numerator which creates
a zero at
lb
eq
s  over r sub  c sub   
en
le
if  r sub  c sub   is sufficiently small this zero will have
negligible effect at audio frequencies and the filter has
the following parameters
lb
centre frequency     mark
 over  pi k over rr sub  c sub  c sub    sup   hz
sp
bandwidthlineup
 over  pi  over r sub  c sub 
k r sub  over rr sub  c sub     hz
le
pp
note first that the centre frequency is proportional to the square root of
the control value k
hence a nonlinear transformation must be implemented on the control
signal after da conversion to achieve the required logarithmic relationship
between parameter value and resonant frequency
the formant bandwidth is not constant as it should be see chapter 
but depends upon the control value k
this dependency can be minimized by selecting component values such that
lb
eq
k r sub  over rr sub  c sub  over r sub  c sub 
en
le
for the largest value of k which can occur
then the bandwidth is solely determined by the time constant  r sub  c sub 
pp
the existence of the zero can be exploited for the fricative resonance
this should have zero dc gain and so the component values for the fricative
filter should make the timeconstant  r sub  c sub   large enough to place
the zero sufficiently near the frequency origin
rh market orientation
as mentioned above computalker is designed for the computer hobbies market
figure  shows a photograph of the device
fc figure 
it plugs into the s bus which has been a
ul
de facto
standard for hobbyists for several years and has recently been adopted
as a standard by the institute of electrical and electronic engineers
this makes it immediately accessible to many microcomputer systems
pp
an inexpensive synthesisbyrule program which runs on
the popular  microprocessor is available to drive computalker
the input is coded in a machinereadable version of the standard phonetic
alphabet similar to that which was introduced in chapter  table 
stress digits may appear in the transcription and the program caters for
five levels of stress
the punctuation mark at the end of an utterance has some effect on pitch
the program is perhaps remarkable in that it occupies only  kbyte of storage
including phoneme tables and runs on an bit microprocessor
but not in real time
it is however
ul
unc
remarkable in that it produces rather poor speech
according to a demonstration cassette
most people find the speech to be readily intelligible
especially after a little practice listening to it
but this seems extremely optimistic
it also cunningly insinuates that if you dont understand it you yourself
may share the blame with the synthesizer em after all
ul
most
people do
nevertheless computalker has made synthetic speech accessible to a large
number of home computer users
sh   soundsegment synthesizer
pp
votrax was the first fully commercial speech synthesizer and at the time of
writing is still the only offtheshelf speech output
peripheral as distinct from reading machine which is aimed
specifically at synthesisbyrule rather than storage of parameter tracks
extracted from natural utterances
figure  shows a photograph of the votrax mli
fc figure 
pp
votrax accepts as input a string of codes representing sound segments
each with additional bits to control the duration and pitch of the segment
in the earlier versions eg model vs there are  sound segments specified
by a bit code and two further bits accompany each segment to provide a
level control over pitch
four pitch levels are quite inadequate to generate acceptable intonation
contours for anything but isolated words spoken in citation form
however a later model mli uses an level pitch specification
as well as a level duration qualifier
associated with each sound segment
it provides a vocabulary of  sound segments together with an additional
code which allows local amplitude modifications and extra duration alterations
to following segments
a further lowcost model vsk is now available which plugs in to the s
bus and
is aimed primarily at
computer hobbyists
it provides no pitch control at all and is therefore
quite unsuited to serious voice response applications
the device has recently been packaged as an lsi circuit model sc
using analogue switchedcapacitor filter technology
pp
one point where the mli scores favourably over other speech synthesis
peripherals is the remarkably convenient engineering of its
computer interface which was outlined in the previous chapter
pp
the internal workings of votrax are not divulged by the manufacturer
figure  shows a block diagram at the level of detail that they supply
fc figure 
it seems to be essentially a formant synthesizer with analogue function
generators and parameter smoothing circuits that provide transitions between
sound segments
rh sound segments
the  segments of the highrange mli model
are summarized in table 
fc table 
they are divided into phoneme classes according to the
classification discussed in chapter 
the segments break down into the following categories
numbers in parentheses are the corresponding figures for vs
lb   
ni   
  vowel sounds which are representative of the phonological
vowel classes for english
ni   
  vowel allophones with slightly different sound qualities from the
above
ni   
  segments whose sound qualities are identical to the segments above but with
different durations
ni   
  consonant sounds which are representative of the phonological
consonant classes for english
ni   
  consonant allophones
ni   
  segments to be used in conjunction with unvoiced plosives to increase
their aspiration
ni   
  silent segments with different pause durations
ni   
  very short silent segment about  msec
le   
somewhat under half of the  elements
can be put into onetoone correspondence with the phonemes of english
the rest are either allophonic variations or additional sounds which can
sensibly be combined with certain phonemes in certain contexts
the votrax literature and consequently votrax users persists in calling
all elements phonemes and this can cause considerable confusion
i prefer to use the term sound segment instead reserving phoneme for its
proper linguistic use
pp
the rules which votrax uses for transitions between sound segments are not
made public by the manufacturer and are embedded in encapsulated circuits
in the hardware
they are clearly very crude
the key to successful encoding of utterances is to use the many
nonphonemic segments in an appropriate way as transitions between the main
segments which represent phonetic classes  this is a tricky process and
i have heard of one commercial establishment giving up in despair at the
extreme difficulty of generating the utterances it wanted
it probably explains the proliferation of lettertosound rules for
votrax which have been developed in research laboratories
colby
ul
et al
 elovitz
ul
et al
 mcilroy  sherwood 

colby christinaz graham 


elovitz  ieee trans acoustics speech and signal processing


mcilroy 


sherwood 

nevertheless with luck skill and especially persistence
excellent results can be
obtained  the mli manual votrax  contains a list of about  words and short phrases
and they are usually clearly recognizable

votrax 

rh duration and pitch qualifiers
each sound segment has a different duration
table  shows the measured duration of the segments although no
calibration data is given by votrax
as mentioned earlier a bit number accompanies each segment to modify
its duration and
this was set to  least duration for the measurements
the qualifier has a multiplicative effect shown in table 
rf
nr x wrate qualifier
nr x win table  by
nr x nxiwnx
nr x nlnx
in nxu
ta nxu i
lnxuul
sp
nr x wmultiply duration
rate qualifierhnxumultiply duration
nr x win table  by
hnxuin table  by
lnxuul
sp




lnxuul
ta i i i i i i i i i i i i
in 
fg table   effect of the bit persegment rate qualifier
pp
as well as the bit rate qualifier each sound segment is accompanied by
a bit pitch specification  this provides a linear control over fundamental
frequency and table  shows the measured values
rf
nr x wpitch specifier
nr x wpitch hz
nr x nxinx
nr x nlnx
in nxu
ta nxu i
lnxuul
sp
pitch specifierhnxupitch hz
lnxuul
sp








lnxuul
ta i i i i i i i i i i i i
in 
fg table   effect of the bit persegment pitch specifier
the quantization interval varies from
one to two semitones
votrax interpolates pitch from phoneme to phoneme in a highly satisfactory
manner and this permits surprisingly sophisticated intonation contours
to be generated considering the crude level quantization
pp
the notation in which the votrax manual defines utterances
gives duration qualifiers and pitch specifications as digits
preceding the sound segment and separated from it by a slash 
thus for example
lb
thv
le
defines the sound segment thv with duration qualifier  multiplies the
 msec duration of table  by  em from table  em to give  msec
and pitch specification   hz
this representation of a segment is transformed into two ascii characters before transmission
to the synthesizer
rh converting a phonetic transcription to sound segments
it would be useful to have a computer procedure to produce a specification for
an utterance in terms of votrax sound segments from a standard phonetic
transcription
this could remove much of the tedium from utterance preparation
by incorporating the contextual rules given in the votrax manual
starting with a phonetic transcription each phoneme should be converted
to its default votrax representative
the resulting wide votrax transcription must be
transformed into a narrow one by application of contextual rules
separate rules are needed for
lb
np
vowel clusters diphthongs
np
vowel transitions ie consonantvowel and vowelconsonant
where the vowel segment is altered
np
intervocalic consonants
np
consonant transitions ie consonantvowel and vowelconsonant
where the consonant segment is altered
np
consonant clusters
np
stressedsyllable effects
np
utterancefinal effects
le
stressedsyllable effects which include
extra aspiration for unvoiced stops beginning stressed syllables
can be applied only if stress markers are included in the phonetic
transcription
pp
to specify a rule it is necessary to give a
ul
matching part
and a
ul
context
which define at what points in an utterance it is applicable and a
ul
replacement part
which is used to replace the matching part
the context can be specified in mathematical set notation using curly brackets
for example
lb
g sh w k ooiu oo
le
states that the matching part oo is replaced by iu oo after a g sh w or k
in fact allophonic variations of each sound segment
should also be accepted as valid context so this rule will also replace oo
after g ch w k or x table  gives allophones of each segment
pp
table  gives some rules that have been used for this purpose
fc table 
they were derived from careful study of the hints given in the
mli manual votrax 

votrax 

classes such as voiced and stopconsonant in the context specify sets
of sound segments in the obvious way
the beginning of a stressed syllable is marked in the input by syll
parentheses in the replacement part have a significance which is explained in
the next section
rh handling prosodic features
we know from chapter  the vital importance of prosodic features
in synthesizing lifelike speech
to allow them to be assigned to votrax utterances an intermediate
output from a prosodic analysis program like isp can be used
for example
lb
  c
ul
dh i s  i z  d zh aa k s  h aa u s
le
which specifies this is jacks house in a declarative intonation with
emphasis on the jacks can be intercepted in the following form
lb
syll
ul
dhc
   
ul
ic
 
ul
sc
   
ul
ic
 
ul
zc
   
syll
ul
dc
   
ul
zhc
 
ul
aac
 
ul
kc
   
ul
sc
 
syll
ul
hc
 
ul
aac
 
ul
uc
 
ul
sc
 
    
le
syllable boundaries pitches and durations have been assigned by the
procedures given earlier chapter 
a number always follows each phoneme to specify its duration
in msec
pairs of numbers in parentheses define a pitch specification at some
point during the preceding phoneme  the first number of the pair defines
the time offset of the specification from the beginning
of the phoneme while the second gives the pitch itself in hz
this form of utterance specification can then be passed to a votrax
conversion procedure
pp
the phonetic transcription is converted
to votrax sound segments using the method described above  the wide votrax
transcription is
lb
syll thv i s i z syll d zh ae k s syll h ae oo s pa 
le
which is transformed to the following narrow one according to the rules
of table 
lb
syll thv i s i z syll d j ae eh k s syll h ah uh o u
s pa 
le
the duration and pitch specifications are preserved by the transformation
in their original positions in the string although they are not shown above
the next stage uses them to expand the transcription by adjusting
the segments to have durations as close as possible to the specifications and
computing pitch numbers to be associated with each phoneme
pp
correct durationexpansion can in general require a great amount of
computation
associated with each sound segment is a set of elements with the same sound quality
but different durations formed by attaching each of the four duration
qualifiers of table  to the segment and any others which are
soundequivalents to it  for example the segment z has the durationset
lb
z   z   z   z
le
with durations
lb
          
le
msec respectively where the initial numerals denote the duration qualifier
the segment i has the much larger durationset
lb
i   i   i   i   i   i   i   i   i   i   i   i
le
with durations
lb
                                  
le
because segments i and i are soundequivalents to it
duration assignment is a matter of selecting elements from the
durationset whose total duration is as close as possible to that desired
for the segment
it happens that votrax deals sensibly with concatenations of more than one
identical plosive suppressing the stop burst on all but the last
although the general problem of approximating durations in
this way is computationally demanding a simple recursive exhaustive search
works in a reasonable amount of time because the desired duration is usually
not very much greater than the longest member of the durationset and so
the search terminates quite quickly
pp
at this point the role of the parentheses which appear on the righthand side
of table  becomes apparent  because durations are only associated with
the input phonemes which may each be expanded into several votrax
segments it is necessary to keep track of the segments which have descended
from a single phoneme
target durations are simply spread equally across any parenthesized groups
to which they apply
pp
having expanded durations mapping pitches on to the sound segments is
a simple matter  the isp system for formant synthesizers chapters  and 
uses linear interpolation between pitch specifications and the frequency which
results for each sound segment needs to be converted to a votrax specification
using the information in table 
pp
after applying these procedures to the example utterance it becomes
lb
thv  i  s  i  z  d  j  ae  eh  c
k  k  s  h  ah  uh  o  u  s  c
s  pa  pa  
le
in several places shorter soundequivalents have been substituted
i for i ah for ah o for o and u for u while doublingup also occurs
in the k s and pa segments
pp
the speech which results from the use of these procedures with the
votrax synthesizer sounds remarkably similar to that generated by the
isp system which uses
parametricallycontrolled synthesizers  formal evaluation experiments have
not been undertaken but it seems clear from careful listening that it would
be rather difficult and probably pointless to evaluate the votrax conversion
algorithm for the outcome would be completely dominated by the success of the
original pitch and rhythm assignment procedures
sh   linear predictive synthesizer
pp
the first singlechip speech synthesizer was introduced by
texas instruments ti in the summer of  wiggins and brantingham 

wiggins brantingham 

it was a remarkable development combining recent advances in signal processing
with the very latest in vlsi technology
packaged in the speak n spell toy figure  it was a striking demonstration
of imagination and prowess in integrated electronics
fc figure 
it gave ti a long lead over its competitors and surprised many experts
in the speech field
eq
delim 
en
overnight it seemed digital speech technology had descended from
research laboratories with their expensive and specialized equipment into
a  consumer item
eq
delim 
en
naturally ti did not sell the chip separately but only as part of their
massmarket product nor would they make available information on how to
drive it directly
only recently when other similar devices appeared on the market did they
unbundle the package and sell the chip
rh the speak n spell toy
the ti chip tmc uses the linear predictive method of synthesis
primarily because of the ease of the speech analysis procedure and the known
high quality at low data rates
speech researchers incidentally sometimes scoff at what they perceive to be
the poor quality of the toys speech but considering the data rate
used which averages  bits per second of speech it is remarkably good
anyway i have never heard a child complain em although it is not uncommon
to misunderstand a word
two  kbit readonly memories are used in the toy to hold data for about
 words and phrases em lasting between  and  minutes em of speech
at the time mid these memories were the largest that were available
in the industry
the data flow and user dialogue are handled by a microprocessor
which is the fourth lsi circuit in the photograph of figure 
fc figure 
pp
a schematic diagram of the toy is given in figure 
fc figure 
it has a small display which shows uppercase letters
some teachers of spelling hold that the lack of lower case destroys
any educational value that the toy may have  it
has a full key alphanumeric keyboard with  additional control keys
this is the toys achilles heel for the keys fall out after extended use
more recent toys from ti use an improved keyboard  the
keyboard is laid out alphabetically instead of in qwerty order possibly
missing an opportunity to teach kids to type as well as spell
an internal connector permits vocabulary expansion with up to  more
readonly memory chips
controlling the toy is a bit microprocessor a modified tms
however the synthesizer chip does not receive data from the processor
during speech it accesses the memory directly and only returns control
to the processor when an endofphrase marker is found in the data stream
meanwhile the processor is idle and cannot even be interrupted from the
keyboard
moreover in one operational mode sayit the toy embarks upon a long
monologue and remains deaf to the keyboard em it cannot even be turned off
any threeyearold will quickly discover that a sharp slap solves the problem
a useful feature is that the device switches itself off if unused for more
than a few minutes
a fascinating account of the development of the toy from the point of view
of product design and market assessment has been published
frantz and wiggins 

frantz wiggins 

rh control parameters
the lattice filtering method of linear predictive synthesis see chapter 
was selected because of its good stability properties and guaranteed
performance with small word sizes
the lattice has  stages
all the control parameters are represented as bit fixedpoint numbers
and the lattice operates with an internal precision of  bits including
sign
pp
there are twelve parameters for the device  ten reflection coefficients
energy and pitch
these are updated every  msec
however if bit values were stored for each a data rate of  bits
every  msec or  kbits would be needed
this would reduce the capacity of the two readonly memory chips to well
under a minute of speech em perhaps  words and phrases
but one of the desirable properties of the reflection coefficients
which drive the lattice filter is that they are amenable to quantization
a nonlinear quantization scheme is used with the parameter data addressing
an onchip quantization table to yield a bit coefficient
pp
table  shows the number of bits devoted to each parameter
rf
ini
ta wrepeat flagu i i
nr x wrepeat flagiwwsize bit words
lnxuul
nr x wbits
nr x wquantization table
nr x m
parameterhnxubitshnxuquantization table
nr x wsize bit words
hnxusize bit words
lnxuul
sp
energyvnxuvnxuzvnxuvnxu  energy means bit frame
pitch
repeat flagemzvnxuvnxuzvnxuvnxu  repeat flag  means bit frame
k
k
k
kzvnxuvnxuzvnxuvnxu  pitch unvoiced means bit frame
k
k
k
k
k
kzvnxuvnxuzvnxuvnxu  otherwise bit frame

sp
 bits words
lnxuul
ta i i i i i i i i i i i i
ini
fg table   bit allocation for speak n spell chip
there are  bits for energy and  bits for pitch and the first two
reflection coefficients
thereafter the number of bits allocated to reflection coefficients decreases
steadily for higher coefficients are less important for intelligibility
than lower ones
note that using a stage filter is tantamount to allocating
ul
no
bits to coefficients higher than the tenth  with a
bit repeat flag whose role is discussed shortly the frame size
becomes  bits
updated every  msec this gives a data rate of just under  kbits
pp
the parameters are expanded into bit numbers by a separate quantization
table for each one
for example the five pitch bits address a word lookup table which
returns a bit value
the transformation is logarithmic in this case the lowest pitch being
around  hz and the highest  hz
as shown in table  a total of  bit words suffices to hold all
twelve quantization tables and they are implemented on the synthesizer
chip
to provide further smoothing of the control parameters
they are interpolated linearly from one frame to the next at eight points
within the frame
pp
the raw data rate of  kbits is reduced to an average of  bits
by further coding techniques
firstly if the energy parameter is zero the frame is silent
and no more parameters are transmitted bit frame
secondly if the repeat flag is  all reflection coefficients are held
over from the previous frame giving a constant filter but with the ability
to vary amplitude and pitch bit frame
finally if the frame is unvoiced signalled by the pitch value being zero
only four reflection coefficients are transmitted because the ear is
relatively insensitive to spectral detail in unvoiced speech bit frame
the end of the utterance is signalled by the energy bits all being 
rh chip organization
the configuration of the lattice filter is shown in figure 
fc figure 
the twomultiplier structure chapter  is used so the stage filter
requires  multiplications and  additions
per speech sample
the last operation in the reverse path at the bottom is not needed  since
a  khz sample rate is used just  musec are available for each
speech sample
a single  musec adder and a pipelined multiplier are implemented on
the chip and multiplexed among the  operations
the latter begins a new multiplication every  musec and finishes it
 musec later
these times are within the capability of pchannel mos technology
allowing the chip to be produced at low cost
the time slot for the th unnecessary filter multiplication is used
for an overall gain adjustment
pp
the final analogue signal is produced by an bit onchip da converter
which drives a  milliwatt speaker through an impedancematching
transformer
these constitute the necessary analogue lowpass desampling filter
pp
figure  summarizes the organization of the synthesis chip
fc figure 
serial data enters directly from the readonly memories although a control
signal from the processor begins synthesis and another signal is returned
to it upon termination
the data is decoded into individual parameters which are used to address
the quantization tables to generate the full bit parameter
values
these are interpolated from one frame to the next
the lower part of the figure shows the speech generation subsystem
an excitation waveform for voiced speech is stored in readonly
memory and read out repeatedly at a rate determined by the pitch
the source for unvoiced sounds is hardlimited noise provided by a digital
pseudorandom bit generator
the sound source that is used depends on whether the pitch value is zero
or not  notice that this precludes mixed excitation for voiced fricatives
and the sound is noticeably poor in words like zee
a gain multiplication is performed before the signal is passed through the
lattice synthesis filter described earlier
sh   programmable signal processors
pp
the ti chip has a fixed architecture and is destined forever
to implement the same vocal tract model em a th order lattice filter
a more recent device the programmable digital signal processor
caldwell  from telesensory systems allows more flexibility
in the type of model

caldwell 

it can serve as a digital formant synthesizer or a linear predictive
synthesizer and the order of model number of formants in the former case
can be changed
pp
before describing the pdsp it is worth looking at an earlier microprocessor
which was designed for digital signal processing
some industry observers have said that this processor the intel 
is to the analogue design engineer what the first microprocessor was to
the random logic engineer way back in the mists of time early s
rh the analogue microprocessor
the  is a digital microprocessor
however it contains an onchip da converter which can be used in
successive approximation fashion for ad conversion under program control
and its architecture is designed to aid digital signal processing calculations
although the precision of conversion is  bits internal arithmetic is
done with  bits to accomodate the accumulation of roundoff errors in
arithmetic operations
an onchip programmable readonly memory holds a instruction program
which is executed in sequence with no program jumps allowed
this ensures that each pass through the program takes the same time
so that the analogue waveform is regularly sampled and processed
pp
the device is implemented in nchannel mos technology which makes it
slightly faster than the pmos speak n spell chip
at its fastest operating speed each instruction takes  nsec
the instruction program therefore executes in  musec corresponding
to a sampling rate of almost  khz
thus the processor can handle signals with a bandwidth of  khz em ample
for highquality speech
however a special eop end of program instruction is provided which
causes an immediate jump back to the beginning
hence if the program occupies less than  instructions faster sampling
rates can be used
for example a single secondorder formant resonance
requires only  instructions and so can
be executed at over  khz
pp
despite this speed the  is only marginally capable of synthesizing
speech
table  gives approximate numbers of instructions needed to do some
subtasks for speech generation hoff and li 

hoff li  software makes a big talker

rf
nr x wparameter entry and data distributionw
nr x winstructions
nr x nlnx
in nxu
ta wparameter entry and data distributionu
lnxuul
sp
taskhnxuinstructions
lnxuul
sp
parameter entry and data distribution
glottal pulse generation
noise generation
lattice section
formant filter
lnxuul
ta i i i i i i i i i i i i
in 
fg table    instruction counts for typical speech subsystems
the parameter entry and data distribution procedure
collects  bit parameters from a serial input stream at a frame rate of
 framess
the parameter data rate is  kbits and the routine assumes that the
 performs each complete cycle in  musec to generate sampled speech
at  khz
therefore one bit of parameter data is accepted on every cycle
the glottal pulse program generates an asymmetrical triangular waveform
chapter  while the noise generator uses a bit pseudorandom feedback
shift register
about  of the instruction program memory is consumed by these
essential tasks
a twomultiplier lattice section takes  instructions
and so only six sections can fit into the remaining program space
it may be possible to use two s to implement a complete  or th
order lattice but the results of the first stage must be passed to the
second by transmitting analogue or digital data between each of the
s analogue ports em not a terribly satisfactory method
pp
since a formant filter occupies only  instructions up to nine of them
would fit in the program space left after the abovementioned essential
subsystems
although other necessary housekeeping tasks may reduce this number
substantially
it does seem possible to implement a formant synthesizer on a single 
rh the programmable digital signal processor
whereas the  is intended for general signalprocessing jobs
telesensory systems pdsp
programmable digital signal processor is aimed specifically at speech
synthesis
it comprises two separate chips a control unit and an arithmetic unit
to build a synthesizer these must be augmented with external memory
and a da converter arranged in a configuration like that of figure 
fc figure 
pp
the control unit accepts parameter data from a host computer one byte at a time
the data is temporarily held in buffer memory before being serialized and passed
to the arithmetic unit
notice that for the  we assumed that parameters were presented
to the chip already serialized and precisely timed  the pdsp control unit
effectively releases the host from this highspeed realtime operation
but it does more
it generates both a voiced and an unvoiced excitation source and passes them
to the arithmetic unit to relieve the latter of the generalpurpose
programming required for both these tasks and allow its instruction set
to be highly specialized for digital filtering
pp
the arithmetic unit has rather a peculiar structure
it accomodates only  program steps and can execute the full instruction
program at a rate of  khz
the internal wordlength is  bits but coefficients and the digital output
are only  bits
each instruction can accomplish quite a lot of work
figure  shows that there are four separate blocks of store in addition
to the program memory
fc figure 
one location of each block is automatically associated with each program step
thus on instruction  for example two bit scratchpad registers ma
and mb and two bit coefficient registers a and a are
accessible
in addition five general registers curiously numbered r r r r r
are available to every program step
pp
each instruction has five fields
a single instruction loads all the general registers and simultaneously
performs two multiplications and up to three additions
the fields specify exactly which operands are involved in these operations
pp
the instructions of the pdsp arithmetic unit are really very powerful
for example a secondorder digital formant resonator requires only
two program steps
a twomultiplier lattice stage needs only one step and
a complete stage lattice filter can be implemented in the  steps available
an important feature of the architecture is that it
is quite easy to incorporate more than one
arithmetic unit into a system with a single control unit
intermediate data can be transferred digitally between arithmetic units
since the da converter is offchip
a fourmultiplier normalized lattice chapter  with  stages can be implemented
on two arithmetic units as can a lattice filter which incorporates zeros
as well as poles and a complex seriesparallel formant synthesizer
with a total of  resonators whose centre frequencies and bandwidths
can be controlled independently klatt 

klatt 

pp
how this device will fare in actual commercial products is yet to be seen
it is certainly much more sophisticated than the ti speak n spell chip
and a complete system will necessitate a much higher chip count and consequently
more expense
telesensory systems are committed to producing a texttospeech
system based upon it
for use both in a reading machine for the blind and as a textinput
speechoutput computer peripheral
sh   references
lb nnnn

list

le nnnn
bp
ev
ta wfisilencefr u wehu wused to change amplitude and durationu wtest wordu
nr x wfisilencefr wehwused to change amplitude and durationwtest word
lnxuul
sp
nr x wvotrax
nr x wduration msec
nr x wtest word
hnxuvotraxhnxuduration msechnxutest word
lnxuul
sp
nr x whid
fiifrihnxuhid
isound equivalent of i
isound equivalent of i
iallophone of i
isound equivalent of i
ayallophone of i
nr x whead
fiefrehhnxuhead
ehsound equivalent of eh
ehsound equivalent of eh
ehallophone of eh
ehsound equivalent of eh
aallophone of eh
asound equivalent of a
nr x whad
fiaafraehnxuhad
aesound equivalent of ae
nr x whod
fiofrawhnxuhod
awsound equivalent of aw
awallophone of aw
nr x whood
fiufroohnxuhood
oosound equivalent of oo
iuallophone of oo
nr x whud
fiafruhhnxuhud
uhsound equivalent of uh
uhsound equivalent of uh
uhallophone of uh
uhsound equivalent of uh
uhallophone of uh
nr x whard
fiarfrahhnxuhard
ahsound equivalent of ah
nr x whawed
fiawfrohnxuhawed
osound equivalent of o
osound equivalent of o
oallophone of o
osound equivalent of o
osound equivalent of o
nr x wwho d
fiuufruhnxuwhod
usound equivalent of u
nr x wheard
fierfrerhnxuheard
nr x wheed
fieefrehnxuheed
esound equivalent of e
firfrr
rallophone of r
fiwfrw
wallophone of w
lnxuul
sp
ce
table   votrax sound segments and their durations
bp
lnxuul
sp
nr x wvotrax
nr x wduration msec
nr x wtest word
hnuvotraxhnxuduration msechnxutest word
lnxuul
sp
filfrl
lallophone of l
fiyfry
yallophone of y
fimfrm
fibfrb
fipfrp
phaspiration burst for use with p
finfrn
fidfrd
dallophone of d
fitfrt
dtallophone of t
saspiration burst for use with t
fingfrng
figfrg
gallophone of g
fikfrk
kallophone of k
xaspiration burst for use with k
fisfrs
fizfrz
fishfrsh
challophone of sh
fizhfrzh
jallophone of zh
fiffrf
fivfrv
fithfrth
fidhfrthv
fihfrh
hallophone of h
hallophone of h
fisilencefrpa
pa
pa

pa used to change amplitude and duration
lnxuul
ta i i i i i i i i i i i i
sp
ce
table   continued
bp
ta i i wah uh  o uu
nr x iiwah uh  o uw i uh   here
lnxuul
sp
vowel clusters
eh ia ay e i   hey
uh ooo u uh u   ho
ae iah eh i aa i   hi
ae ooah uh o u aa u   how
aw io uh e o i   hoi
i uhe i i uh   here
eh uheh a eh e uh   hair
oo uhoo uh u uh   poor
y uy iu u
sp
vowel transitions
f m b p oo o
l r eheh eh
b k t d r uhuh uh
t d aeh a
t d awah aw
w ii i
g sh w k ooiu oo
ay k g t day y
e m te y
i m ti y
e li uh
eh r n s d teh eh
i r ti i
ae s nae eh
ae kae eh
a ra eh
ah r p kah uh
ah zhah eh
sp
intervocalics
voiced t voiceddt
sp
consonant transitions
l ehl
h u oo iuh
lnxuul
sp
ce
table   contextual rules for votrax sound segments
bp
lnxuul
sp
consonant clusters
b stopconsonantb pa
p stopconsonantp pa
d stopconsonantd pa
t stopconsonantt pa
dt stopconsonantt pa
g stopconsonantg pa
k stopconsonantk pa
d t rx r
k rk x r
consonant rr
consonant ll
k wk w
d zhd j
t sht ch
sp
initial effects
syll p vowelp ph
syll k vowelk h
syll t vowelt s
syll ll
syll h u oo o aw ahh
sp
terminal effects
e pae y
lnxuul
ta i i i i i i i i i i i i
sp
ce
table   continued
ev
